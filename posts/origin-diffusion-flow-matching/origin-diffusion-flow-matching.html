<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nicolas Brosse">
<meta name="dcterms.date" content="2024-12-15">
<meta name="description" content="A detailed exposition of the foundations of diffusion models, covering Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Models (DDPM), and their unification through Stochastic Differential Equations using the EDM framework.">

<title>Origins of Diffusion Models: From SMLD and DDPM to SDE Unification – Nicolas’ Notebook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-75c24fa0c77a874b0ab0aff8b6422dd8.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Nicolas’ Notebook</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/nbrosse"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/nicolas-brosse-984685a0/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Origins of Diffusion Models: From SMLD and DDPM to SDE Unification</h1>
                  <div>
        <div class="description">
          A detailed exposition of the foundations of diffusion models, covering Score Matching with Langevin Dynamics (SMLD), Denoising Diffusion Probabilistic Models (DDPM), and their unification through Stochastic Differential Equations using the EDM framework.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">deep learning</div>
                <div class="quarto-category">generative models</div>
                <div class="quarto-category">diffusion</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Nicolas Brosse </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 15, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-introduction" id="toc-sec-introduction" class="nav-link active" data-scroll-target="#sec-introduction">Introduction</a></li>
  <li><a href="#sec-score-matching" id="toc-sec-score-matching" class="nav-link" data-scroll-target="#sec-score-matching">Score Matching and Denoising Foundations</a>
  <ul class="collapse">
  <li><a href="#the-score-function" id="toc-the-score-function" class="nav-link" data-scroll-target="#the-score-function">The Score Function</a></li>
  <li><a href="#smoothing-with-gaussian-noise" id="toc-smoothing-with-gaussian-noise" class="nav-link" data-scroll-target="#smoothing-with-gaussian-noise">Smoothing with Gaussian Noise</a></li>
  <li><a href="#denoising-and-the-optimal-denoiser" id="toc-denoising-and-the-optimal-denoiser" class="nav-link" data-scroll-target="#denoising-and-the-optimal-denoiser">Denoising and the Optimal Denoiser</a></li>
  <li><a href="#tweedies-formula" id="toc-tweedies-formula" class="nav-link" data-scroll-target="#tweedies-formula">Tweedie’s Formula</a></li>
  <li><a href="#connection-denoiser-to-score" id="toc-connection-denoiser-to-score" class="nav-link" data-scroll-target="#connection-denoiser-to-score">Connection: Denoiser to Score</a></li>
  <li><a href="#denoising-score-matching" id="toc-denoising-score-matching" class="nav-link" data-scroll-target="#denoising-score-matching">Denoising Score Matching</a></li>
  </ul></li>
  <li><a href="#sec-smld" id="toc-sec-smld" class="nav-link" data-scroll-target="#sec-smld">Score Matching with Langevin Dynamics (SMLD)</a>
  <ul class="collapse">
  <li><a href="#multi-scale-noise-perturbation" id="toc-multi-scale-noise-perturbation" class="nav-link" data-scroll-target="#multi-scale-noise-perturbation">Multi-Scale Noise Perturbation</a></li>
  <li><a href="#why-multiple-noise-scales" id="toc-why-multiple-noise-scales" class="nav-link" data-scroll-target="#why-multiple-noise-scales">Why Multiple Noise Scales?</a></li>
  <li><a href="#training-objective-noise-conditional-score-network-ncsn" id="toc-training-objective-noise-conditional-score-network-ncsn" class="nav-link" data-scroll-target="#training-objective-noise-conditional-score-network-ncsn">Training Objective: Noise Conditional Score Network (NCSN)</a></li>
  <li><a href="#sampling-annealed-langevin-dynamics" id="toc-sampling-annealed-langevin-dynamics" class="nav-link" data-scroll-target="#sampling-annealed-langevin-dynamics">Sampling: Annealed Langevin Dynamics</a></li>
  </ul></li>
  <li><a href="#sec-ddpm" id="toc-sec-ddpm" class="nav-link" data-scroll-target="#sec-ddpm">Denoising Diffusion Probabilistic Models (DDPM)</a>
  <ul class="collapse">
  <li><a href="#forward-diffusion-process" id="toc-forward-diffusion-process" class="nav-link" data-scroll-target="#forward-diffusion-process">Forward Diffusion Process</a>
  <ul class="collapse">
  <li><a href="#reparameterization" id="toc-reparameterization" class="nav-link" data-scroll-target="#reparameterization">Reparameterization</a></li>
  <li><a href="#closed-form-marginal" id="toc-closed-form-marginal" class="nav-link" data-scroll-target="#closed-form-marginal">Closed-Form Marginal</a></li>
  </ul></li>
  <li><a href="#reverse-diffusion-process" id="toc-reverse-diffusion-process" class="nav-link" data-scroll-target="#reverse-diffusion-process">Reverse Diffusion Process</a>
  <ul class="collapse">
  <li><a href="#true-posterior-conditioned-on-mathbfx_0" id="toc-true-posterior-conditioned-on-mathbfx_0" class="nav-link" data-scroll-target="#true-posterior-conditioned-on-mathbfx_0">True Posterior (Conditioned on <span class="math inline">\(\mathbf{x}_0\)</span>)</a></li>
  <li><a href="#deriving-the-reverse-mean-via-tweedie" id="toc-deriving-the-reverse-mean-via-tweedie" class="nav-link" data-scroll-target="#deriving-the-reverse-mean-via-tweedie">Deriving the Reverse Mean via Tweedie</a></li>
  <li><a href="#expressing-the-mean-in-terms-of-boldsymbolepsilon" id="toc-expressing-the-mean-in-terms-of-boldsymbolepsilon" class="nav-link" data-scroll-target="#expressing-the-mean-in-terms-of-boldsymbolepsilon">Expressing the Mean in Terms of <span class="math inline">\(\boldsymbol{\epsilon}\)</span></a></li>
  </ul></li>
  <li><a href="#variational-lower-bound-elbo" id="toc-variational-lower-bound-elbo" class="nav-link" data-scroll-target="#variational-lower-bound-elbo">Variational Lower Bound (ELBO)</a>
  <ul class="collapse">
  <li><a href="#jensens-inequality-derivation" id="toc-jensens-inequality-derivation" class="nav-link" data-scroll-target="#jensens-inequality-derivation">Jensen’s Inequality Derivation</a></li>
  <li><a href="#decomposition-into-kl-divergences" id="toc-decomposition-into-kl-divergences" class="nav-link" data-scroll-target="#decomposition-into-kl-divergences">Decomposition into KL Divergences</a></li>
  </ul></li>
  <li><a href="#parameterization-and-training-loss" id="toc-parameterization-and-training-loss" class="nav-link" data-scroll-target="#parameterization-and-training-loss">Parameterization and Training Loss</a>
  <ul class="collapse">
  <li><a href="#noise-prediction-parameterization" id="toc-noise-prediction-parameterization" class="nav-link" data-scroll-target="#noise-prediction-parameterization">Noise Prediction Parameterization</a></li>
  <li><a href="#simplified-loss" id="toc-simplified-loss" class="nav-link" data-scroll-target="#simplified-loss">Simplified Loss</a></li>
  </ul></li>
  <li><a href="#connection-to-score-matching" id="toc-connection-to-score-matching" class="nav-link" data-scroll-target="#connection-to-score-matching">Connection to Score Matching</a></li>
  <li><a href="#sampling-ancestral-sampling" id="toc-sampling-ancestral-sampling" class="nav-link" data-scroll-target="#sampling-ancestral-sampling">Sampling: Ancestral Sampling</a></li>
  </ul></li>
  <li><a href="#sec-sde-unification" id="toc-sec-sde-unification" class="nav-link" data-scroll-target="#sec-sde-unification">SDE Unification with EDM Framework</a>
  <ul class="collapse">
  <li><a href="#general-sde-framework" id="toc-general-sde-framework" class="nav-link" data-scroll-target="#general-sde-framework">General SDE Framework</a>
  <ul class="collapse">
  <li><a href="#forward-sde" id="toc-forward-sde" class="nav-link" data-scroll-target="#forward-sde">Forward SDE</a></li>
  <li><a href="#reverse-sde" id="toc-reverse-sde" class="nav-link" data-scroll-target="#reverse-sde">Reverse SDE</a></li>
  </ul></li>
  <li><a href="#edm-parameterization" id="toc-edm-parameterization" class="nav-link" data-scroll-target="#edm-parameterization">EDM Parameterization</a>
  <ul class="collapse">
  <li><a href="#denoiser-and-score-relationship" id="toc-denoiser-and-score-relationship" class="nav-link" data-scroll-target="#denoiser-and-score-relationship">Denoiser and Score Relationship</a></li>
  <li><a href="#unified-training-objective" id="toc-unified-training-objective" class="nav-link" data-scroll-target="#unified-training-objective">Unified Training Objective</a></li>
  </ul></li>
  <li><a href="#from-smld-to-variance-exploding-ve-sde" id="toc-from-smld-to-variance-exploding-ve-sde" class="nav-link" data-scroll-target="#from-smld-to-variance-exploding-ve-sde">From SMLD to Variance Exploding (VE) SDE</a>
  <ul class="collapse">
  <li><a href="#discrete-chain" id="toc-discrete-chain" class="nav-link" data-scroll-target="#discrete-chain">Discrete Chain</a></li>
  <li><a href="#continuous-limit" id="toc-continuous-limit" class="nav-link" data-scroll-target="#continuous-limit">Continuous Limit</a></li>
  </ul></li>
  <li><a href="#from-ddpm-to-variance-preserving-vp-sde" id="toc-from-ddpm-to-variance-preserving-vp-sde" class="nav-link" data-scroll-target="#from-ddpm-to-variance-preserving-vp-sde">From DDPM to Variance Preserving (VP) SDE</a>
  <ul class="collapse">
  <li><a href="#discrete-chain-1" id="toc-discrete-chain-1" class="nav-link" data-scroll-target="#discrete-chain-1">Discrete Chain</a></li>
  <li><a href="#continuous-limit-1" id="toc-continuous-limit-1" class="nav-link" data-scroll-target="#continuous-limit-1">Continuous Limit</a></li>
  </ul></li>
  <li><a href="#variance-dynamics" id="toc-variance-dynamics" class="nav-link" data-scroll-target="#variance-dynamics">Variance Dynamics</a>
  <ul class="collapse">
  <li><a href="#vp-sde-covariance" id="toc-vp-sde-covariance" class="nav-link" data-scroll-target="#vp-sde-covariance">VP SDE Covariance</a></li>
  <li><a href="#sub-vp-sde" id="toc-sub-vp-sde" class="nav-link" data-scroll-target="#sub-vp-sde">Sub-VP SDE</a></li>
  </ul></li>
  <li><a href="#perturbation-kernels-summary" id="toc-perturbation-kernels-summary" class="nav-link" data-scroll-target="#perturbation-kernels-summary">Perturbation Kernels Summary</a></li>
  <li><a href="#reverse-sdes" id="toc-reverse-sdes" class="nav-link" data-scroll-target="#reverse-sdes">Reverse SDEs</a>
  <ul class="collapse">
  <li><a href="#ve-reverse-sde" id="toc-ve-reverse-sde" class="nav-link" data-scroll-target="#ve-reverse-sde">VE Reverse SDE</a></li>
  <li><a href="#vp-reverse-sde" id="toc-vp-reverse-sde" class="nav-link" data-scroll-target="#vp-reverse-sde">VP Reverse SDE</a></li>
  </ul></li>
  <li><a href="#unified-view-score-estimation-for-any-sde" id="toc-unified-view-score-estimation-for-any-sde" class="nav-link" data-scroll-target="#unified-view-score-estimation-for-any-sde">Unified View: Score Estimation for Any SDE</a></li>
  </ul></li>
  <li><a href="#sec-summary" id="toc-sec-summary" class="nav-link" data-scroll-target="#sec-summary">Summary and Comparison</a>
  <ul class="collapse">
  <li><a href="#comparison-table" id="toc-comparison-table" class="nav-link" data-scroll-target="#comparison-table">Comparison Table</a></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="sec-introduction" class="level1">
<h1>Introduction</h1>
<p>Diffusion models have emerged as one of the most powerful approaches for generative modeling, achieving state-of-the-art results in image synthesis, audio generation, and many other domains. The core idea is elegantly simple: gradually destroy data by adding noise, then learn to reverse this corruption process to generate new samples from noise.</p>
<p>This article traces the origins of modern diffusion models through two foundational approaches:</p>
<ol type="1">
<li><p><strong>Score Matching with Langevin Dynamics (SMLD)</strong> <span class="citation" data-cites="song_score-based_2021">(<a href="#ref-song_score-based_2021" role="doc-biblioref">Song et al. 2021</a>)</span>: Learns the score function (gradient of log-density) at multiple noise scales and samples via annealed Langevin dynamics.</p></li>
<li><p><strong>Denoising Diffusion Probabilistic Models (DDPM)</strong>: Defines a Markov chain that progressively adds Gaussian noise and learns to reverse it through variational inference.</p></li>
</ol>
<p>We then show how both approaches are unified through the lens of <strong>Stochastic Differential Equations (SDEs)</strong>, using the notation and framework from the EDM paper <span class="citation" data-cites="karras_elucidating_2022">(<a href="#ref-karras_elucidating_2022" role="doc-biblioref">Karras et al. 2022</a>)</span>. This unification reveals that SMLD and DDPM are discretizations of continuous diffusion processes, opening the door to more flexible and principled designs.</p>
<p><strong>Roadmap:</strong></p>
<ul>
<li><a href="#sec-score-matching" class="quarto-xref">Section&nbsp;2</a>: Score matching and denoising foundations, including Tweedie’s formula</li>
<li><a href="#sec-smld" class="quarto-xref">Section&nbsp;3</a>: Score Matching with Langevin Dynamics (SMLD)</li>
<li><a href="#sec-ddpm" class="quarto-xref">Section&nbsp;4</a>: Denoising Diffusion Probabilistic Models (DDPM)</li>
<li><a href="#sec-sde-unification" class="quarto-xref">Section&nbsp;5</a>: SDE unification using the EDM framework</li>
<li><a href="#sec-summary" class="quarto-xref">Section&nbsp;6</a>: Summary and comparison of all formulations</li>
</ul>
</section>
<section id="sec-score-matching" class="level1">
<h1>Score Matching and Denoising Foundations</h1>
<section id="the-score-function" class="level2">
<h2 class="anchored" data-anchor-id="the-score-function">The Score Function</h2>
<p>Let <span class="math inline">\(p_{\mathrm{data}}(\mathbf{x})\)</span> be an unknown data distribution on <span class="math inline">\(\mathbb{R}^d\)</span>. The <strong>score function</strong> is defined as the gradient of the log-density: <span class="math display">\[
\nabla_\mathbf{x} \log p_{\mathrm{data}}(\mathbf{x}).
\]</span></p>
<p>The score function is fundamental because it enables sampling algorithms such as Langevin dynamics, diffusion models, and MCMC procedures. Unlike the density itself, the score does not require computing the intractable normalization constant.</p>
<p>Since we do not know <span class="math inline">\(p_{\mathrm{data}}\)</span> analytically and only have samples <span class="math inline">\(\mathbf{x} \sim p_{\mathrm{data}}\)</span>, we must approximate the score with a neural network <span class="math inline">\(s_\theta(\mathbf{x}) \approx \nabla_\mathbf{x} \log p_{\mathrm{data}}(\mathbf{x})\)</span>.</p>
</section>
<section id="smoothing-with-gaussian-noise" class="level2">
<h2 class="anchored" data-anchor-id="smoothing-with-gaussian-noise">Smoothing with Gaussian Noise</h2>
<p>Directly regressing the score of the data distribution is problematic because:</p>
<ol type="1">
<li>The score is undefined in regions with zero density</li>
<li>Estimation is unstable in low-density regions</li>
<li>The data manifold may have complex geometry</li>
</ol>
<p>The solution is to <strong>smooth</strong> the data distribution by injecting Gaussian noise. Given <span class="math inline">\(\mathbf{x} \sim p_{\mathrm{data}}\)</span>, define: <span class="math display">\[
\mathbf{y} = \mathbf{x} + \sigma \mathbf{w}, \qquad \mathbf{w} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}_d).
\]</span></p>
<p>The density of <span class="math inline">\(\mathbf{y}\)</span> is the Gaussian convolution: <span class="math display">\[
p_\sigma(\mathbf{y}) = (p_{\mathrm{data}} * \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_d))(\mathbf{y}) = \int p_{\mathrm{data}}(\mathbf{x}) \, \mathcal{N}(\mathbf{y}; \mathbf{x}, \sigma^2 \mathbf{I}_d) \, d\mathbf{x},
\]</span> a smoothed version of the true data distribution that has support everywhere and is easier to estimate.</p>
</section>
<section id="denoising-and-the-optimal-denoiser" class="level2">
<h2 class="anchored" data-anchor-id="denoising-and-the-optimal-denoiser">Denoising and the Optimal Denoiser</h2>
<p>We introduce a <strong>denoising network</strong> <span class="math inline">\(D_\sigma(\mathbf{y}; \theta) : \mathbb{R}^d \to \mathbb{R}^d\)</span> trained to reconstruct <span class="math inline">\(\mathbf{x}\)</span> from the noisy observation <span class="math inline">\(\mathbf{y}\)</span>. Using the mean squared error loss: <span class="math display">\[
\mathcal{L}_{\mathrm{denoise}}(\theta) = \frac{1}{2} \mathbb{E}_{\mathbf{x}, \mathbf{w}} \left[ \|\mathbf{x} - D_\sigma(\mathbf{x} + \sigma \mathbf{w}; \theta)\|^2 \right].
\]</span></p>
<p>It is well known that the minimizer of the <span class="math inline">\(L^2\)</span> denoising loss over all measurable functions is the <strong>conditional expectation</strong>: <span class="math display">\[
D_\sigma^*(\mathbf{y}) = \mathbb{E}[\mathbf{x} \mid \mathbf{y}].
\]</span></p>
</section>
<section id="tweedies-formula" class="level2">
<h2 class="anchored" data-anchor-id="tweedies-formula">Tweedie’s Formula</h2>
<p>The key insight connecting denoising to score estimation is <strong>Tweedie’s formula</strong>. For additive Gaussian noise, it states:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Tweedie’s Formula
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\mathbb{E}[\mathbf{x} \mid \mathbf{y}] = \mathbf{y} + \sigma^2 \nabla_\mathbf{y} \log p_\sigma(\mathbf{y})
\]</span> where <span class="math inline">\(p_\sigma\)</span> is the density of the noisy variable <span class="math inline">\(\mathbf{y} = \mathbf{x} + \sigma \mathbf{w}\)</span>.</p>
</div>
</div>
<p><strong>General form:</strong> For the linear Gaussian channel <span class="math inline">\(\mathbf{y} = a\mathbf{x} + \sigma \mathbf{w}\)</span> where <span class="math inline">\(a &gt; 0\)</span>: <span class="math display">\[
\mathbb{E}[\mathbf{x} \mid \mathbf{y}] = \frac{1}{a} \left( \mathbf{y} + \sigma^2 \nabla_\mathbf{y} \log p_{a,\sigma}(\mathbf{y}) \right).
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center collapsed" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>Proof of Tweedie’s Formula
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\phi_\sigma(\mathbf{z}) = (2\pi\sigma^2)^{-d/2} \exp(-\|\mathbf{z}\|^2 / 2\sigma^2)\)</span> denote the Gaussian density with variance <span class="math inline">\(\sigma^2\)</span>. Since <span class="math inline">\(\mathbf{y} \mid \mathbf{x} \sim \mathcal{N}(a\mathbf{x}, \sigma^2 \mathbf{I})\)</span>, the marginal density of <span class="math inline">\(\mathbf{y}\)</span> is: <span class="math display">\[
p_{a,\sigma}(\mathbf{y}) = \int p_{\mathrm{data}}(\mathbf{x}) \, \phi_\sigma(\mathbf{y} - a\mathbf{x}) \, d\mathbf{x}.
\]</span></p>
<p>Using <span class="math inline">\(\nabla_\mathbf{y} \phi_\sigma(\mathbf{y} - a\mathbf{x}) = -\frac{\mathbf{y} - a\mathbf{x}}{\sigma^2} \phi_\sigma(\mathbf{y} - a\mathbf{x})\)</span>, we differentiate under the integral: <span class="math display">\[
\nabla_\mathbf{y} p_{a,\sigma}(\mathbf{y}) = \frac{1}{\sigma^2} \int (a\mathbf{x} - \mathbf{y}) \, p_{\mathrm{data}}(\mathbf{x}) \, \phi_\sigma(\mathbf{y} - a\mathbf{x}) \, d\mathbf{x}.
\]</span></p>
<p>Dividing by <span class="math inline">\(p_{a,\sigma}(\mathbf{y})\)</span> and recognizing the conditional density <span class="math inline">\(p(\mathbf{x} \mid \mathbf{y}) = p_{\mathrm{data}}(\mathbf{x}) \phi_\sigma(\mathbf{y} - a\mathbf{x}) / p_{a,\sigma}(\mathbf{y})\)</span>: <span class="math display">\[
\nabla_\mathbf{y} \log p_{a,\sigma}(\mathbf{y}) = \frac{1}{\sigma^2} \left( a \, \mathbb{E}[\mathbf{x} \mid \mathbf{y}] - \mathbf{y} \right).
\]</span></p>
<p>Rearranging yields Tweedie’s formula.</p>
</div>
</div>
</div>
</section>
<section id="connection-denoiser-to-score" class="level2">
<h2 class="anchored" data-anchor-id="connection-denoiser-to-score">Connection: Denoiser to Score</h2>
<p>Rearranging Tweedie’s formula: <span class="math display">\[
\nabla_\mathbf{y} \log p_\sigma(\mathbf{y}) = \frac{1}{\sigma^2} \left( D_\sigma^*(\mathbf{y}) - \mathbf{y} \right) = \frac{D_\sigma^*(\mathbf{y}) - \mathbf{y}}{\sigma^2}.
\]</span></p>
<p>Thus, the <strong>optimal denoiser implicitly contains the score</strong> of the smoothed distribution <span class="math inline">\(p_\sigma\)</span>. We can define a score network via: <span class="math display">\[
s_\theta(\mathbf{y}, \sigma) := \frac{D_\theta(\mathbf{y}, \sigma) - \mathbf{y}}{\sigma^2}.
\]</span></p>
<p>If <span class="math inline">\(D_\theta \approx D_\sigma^*\)</span>, then <span class="math inline">\(s_\theta\)</span> approximates the true score <span class="math inline">\(\nabla_\mathbf{y} \log p_\sigma(\mathbf{y})\)</span>.</p>
</section>
<section id="denoising-score-matching" class="level2">
<h2 class="anchored" data-anchor-id="denoising-score-matching">Denoising Score Matching</h2>
<p><strong>Denoising Score Matching (DSM)</strong> directly trains the score network. The DSM loss is equivalent to the denoising loss (up to constants): <span class="math display">\[
\mathcal{L}_{\mathrm{DSM}}(\theta) = \frac{1}{2} \mathbb{E}_{\mathbf{x}, \mathbf{w}} \left[ \left\| s_\theta(\mathbf{x} + \sigma \mathbf{w}, \sigma) + \frac{\mathbf{w}}{\sigma} \right\|^2 \right].
\]</span></p>
<p>The target <span class="math inline">\(-\mathbf{w}/\sigma\)</span> comes from the fact that for the perturbation kernel <span class="math inline">\(p(\mathbf{y} \mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathbf{x}, \sigma^2 \mathbf{I})\)</span>: <span class="math display">\[
\nabla_\mathbf{y} \log p(\mathbf{y} \mid \mathbf{x}) = -\frac{\mathbf{y} - \mathbf{x}}{\sigma^2} = -\frac{\sigma \mathbf{w}}{\sigma^2} = -\frac{\mathbf{w}}{\sigma}.
\]</span></p>
</section>
</section>
<section id="sec-smld" class="level1">
<h1>Score Matching with Langevin Dynamics (SMLD)</h1>
<p>The key insight of SMLD is that perturbing data with <strong>multiple noise scales</strong> is essential for successful score-based generative modeling.</p>
<section id="multi-scale-noise-perturbation" class="level2">
<h2 class="anchored" data-anchor-id="multi-scale-noise-perturbation">Multi-Scale Noise Perturbation</h2>
<p>Let <span class="math inline">\(p_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) := \mathcal{N}(\tilde{\mathbf{x}}; \mathbf{x}, \sigma^2 \mathbf{I})\)</span> be a perturbation kernel, and define the smoothed distribution: <span class="math display">\[
p_\sigma(\tilde{\mathbf{x}}) := \int p_{\mathrm{data}}(\mathbf{x}) \, p_\sigma(\tilde{\mathbf{x}} \mid \mathbf{x}) \, d\mathbf{x}.
\]</span></p>
<p>Consider a sequence of positive noise scales: <span class="math display">\[
\sigma_{\min} = \sigma_1 &lt; \sigma_2 &lt; \cdots &lt; \sigma_N = \sigma_{\max}.
\]</span></p>
<p>The noise scales are chosen such that:</p>
<ul>
<li><span class="math inline">\(\sigma_{\min}\)</span> is small enough that <span class="math inline">\(p_{\sigma_{\min}}(\mathbf{x}) \approx p_{\mathrm{data}}(\mathbf{x})\)</span></li>
<li><span class="math inline">\(\sigma_{\max}\)</span> is large enough that <span class="math inline">\(p_{\sigma_{\max}}(\mathbf{x}) \approx \mathcal{N}(\mathbf{x}; \mathbf{0}, \sigma_{\max}^2 \mathbf{I})\)</span></li>
</ul>
</section>
<section id="why-multiple-noise-scales" class="level2">
<h2 class="anchored" data-anchor-id="why-multiple-noise-scales">Why Multiple Noise Scales?</h2>
<p>Using a single noise level has fundamental limitations:</p>
<ol type="1">
<li><p><strong>Small noise</strong>: The score is accurate near data but undefined/inaccurate in most of the space, making it hard to guide samples from far away.</p></li>
<li><p><strong>Large noise</strong>: The smoothed distribution covers more space but loses information about the data structure.</p></li>
</ol>
<p>The solution is to use multiple scales: start with large noise (easy to sample, covers the space) and progressively reduce it (refining toward the data distribution).</p>
</section>
<section id="training-objective-noise-conditional-score-network-ncsn" class="level2">
<h2 class="anchored" data-anchor-id="training-objective-noise-conditional-score-network-ncsn">Training Objective: Noise Conditional Score Network (NCSN)</h2>
<p>SMLD trains a <strong>Noise Conditional Score Network</strong> <span class="math inline">\(s_\theta(\mathbf{x}, \sigma)\)</span> with a weighted sum of denoising score matching objectives:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>SMLD Training Objective
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
\theta^* = \arg\min_\theta \sum_{i=1}^N \sigma_i^2 \, \mathbb{E}_{p_{\mathrm{data}}(\mathbf{x})} \mathbb{E}_{p_{\sigma_i}(\tilde{\mathbf{x}} \mid \mathbf{x})} \left\| s_\theta(\tilde{\mathbf{x}}, \sigma_i) - \nabla_{\tilde{\mathbf{x}}} \log p_{\sigma_i}(\tilde{\mathbf{x}} \mid \mathbf{x}) \right\|_2^2
\]</span></p>
</div>
</div>
<p>Since <span class="math inline">\(\nabla_{\tilde{\mathbf{x}}} \log p_{\sigma_i}(\tilde{\mathbf{x}} \mid \mathbf{x}) = -(\tilde{\mathbf{x}} - \mathbf{x})/\sigma_i^2\)</span>, the objective simplifies to: <span class="math display">\[
\theta^* = \arg\min_\theta \sum_{i=1}^N \sigma_i^2 \, \mathbb{E}_{\mathbf{x}, \boldsymbol{\epsilon}} \left\| s_\theta(\mathbf{x} + \sigma_i \boldsymbol{\epsilon}, \sigma_i) + \frac{\boldsymbol{\epsilon}}{\sigma_i} \right\|_2^2
\]</span> where <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>.</p>
<p><strong>Weight interpretation:</strong> The weights <span class="math inline">\(\sigma_i^2\)</span> are proportional to the inverse Fisher information of the perturbation kernel: <span class="math display">\[
\sigma_i^2 \propto \frac{1}{\mathbb{E}\| \nabla_{\tilde{\mathbf{x}}} \log p_{\sigma_i}(\tilde{\mathbf{x}} \mid \mathbf{x}) \|^2}.
\]</span> This ensures all noise levels contribute comparably to the loss.</p>
</section>
<section id="sampling-annealed-langevin-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="sampling-annealed-langevin-dynamics">Sampling: Annealed Langevin Dynamics</h2>
<p>Given a trained score model <span class="math inline">\(s_{\theta^*}\)</span>, sampling proceeds by running Langevin MCMC at each noise scale sequentially, from largest to smallest.</p>
<p>For each noise scale <span class="math inline">\(\sigma_i\)</span> (from <span class="math inline">\(i = N\)</span> down to <span class="math inline">\(i = 1\)</span>), run <span class="math inline">\(M\)</span> steps of Langevin dynamics: <span class="math display">\[
\mathbf{x}_i^{(m)} = \mathbf{x}_i^{(m-1)} + \epsilon_i \, s_{\theta^*}(\mathbf{x}_i^{(m-1)}, \sigma_i) + \sqrt{2\epsilon_i} \, \mathbf{z}_i^{(m)}, \qquad m = 1, 2, \ldots, M,
\]</span> where:</p>
<ul>
<li><span class="math inline">\(\epsilon_i &gt; 0\)</span> is the step size</li>
<li><span class="math inline">\(\mathbf{z}_i^{(m)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span></li>
<li><span class="math inline">\(\mathbf{x}_N^{(0)} \sim \mathcal{N}(\mathbf{0}, \sigma_{\max}^2 \mathbf{I})\)</span> (initialization)</li>
<li><span class="math inline">\(\mathbf{x}_i^{(0)} = \mathbf{x}_{i+1}^{(M)}\)</span> for <span class="math inline">\(i &lt; N\)</span> (chain across scales)</li>
</ul>
<p>As <span class="math inline">\(M \to \infty\)</span> and <span class="math inline">\(\epsilon_i \to 0\)</span> for all <span class="math inline">\(i\)</span>, the final sample <span class="math inline">\(\mathbf{x}_1^{(M)}\)</span> becomes an exact sample from <span class="math inline">\(p_{\sigma_{\min}}(\mathbf{x}) \approx p_{\mathrm{data}}(\mathbf{x})\)</span> under regularity conditions.</p>
</section>
</section>
<section id="sec-ddpm" class="level1">
<h1>Denoising Diffusion Probabilistic Models (DDPM)</h1>
<p>DDPM takes a different perspective: define a forward Markov chain that progressively corrupts data, then learn the reverse chain through variational inference.</p>
<section id="forward-diffusion-process" class="level2">
<h2 class="anchored" data-anchor-id="forward-diffusion-process">Forward Diffusion Process</h2>
<p>Given data <span class="math inline">\(\mathbf{x}_0 \sim q(\mathbf{x}) = p_{\mathrm{data}}(\mathbf{x})\)</span>, define a forward noising process as a Markov chain: <span class="math display">\[
q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}\left( \mathbf{x}_t; \sqrt{1 - \beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I} \right),
\]</span> with variance schedule <span class="math inline">\(\{\beta_t\}_{t=1}^T\)</span>, typically with <span class="math inline">\(0 &lt; \beta_1 &lt; \beta_2 &lt; \cdots &lt; \beta_T &lt; 1\)</span>.</p>
<section id="reparameterization" class="level3">
<h3 class="anchored" data-anchor-id="reparameterization">Reparameterization</h3>
<p>Define <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span> and <span class="math inline">\(\bar{\alpha}_t = \prod_{i=1}^t \alpha_i\)</span>. By reparameterization: <span class="math display">\[
\mathbf{x}_t = \sqrt{\alpha_t} \, \mathbf{x}_{t-1} + \sqrt{1 - \alpha_t} \, \boldsymbol{\epsilon}_{t-1}, \qquad \boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
\]</span></p>
</section>
<section id="closed-form-marginal" class="level3">
<h3 class="anchored" data-anchor-id="closed-form-marginal">Closed-Form Marginal</h3>
<p>A key property is that we can sample <span class="math inline">\(\mathbf{x}_t\)</span> directly from <span class="math inline">\(\mathbf{x}_0\)</span> without iterating through intermediate steps:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>DDPM Marginal Distribution
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
q(\mathbf{x}_t \mid \mathbf{x}_0) = \mathcal{N}\left( \mathbf{x}_t; \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I} \right)
\]</span></p>
<p>Equivalently: <span class="math inline">\(\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \, \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon}\)</span>, where <span class="math inline">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>.</p>
</div>
</div>
<p>The schedule is chosen so that <span class="math inline">\(\bar{\alpha}_T \approx 0\)</span>, meaning <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> approximately.</p>
</section>
</section>
<section id="reverse-diffusion-process" class="level2">
<h2 class="anchored" data-anchor-id="reverse-diffusion-process">Reverse Diffusion Process</h2>
<p>To generate samples, we need to reverse the forward process. We model: <span class="math display">\[
p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) = \mathcal{N}\left( \mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \boldsymbol{\Sigma}_\theta(\mathbf{x}_t, t) \right).
\]</span></p>
<section id="true-posterior-conditioned-on-mathbfx_0" class="level3">
<h3 class="anchored" data-anchor-id="true-posterior-conditioned-on-mathbfx_0">True Posterior (Conditioned on <span class="math inline">\(\mathbf{x}_0\)</span>)</h3>
<p>When conditioned on <span class="math inline">\(\mathbf{x}_0\)</span>, the reverse transition has a closed form: <span class="math display">\[
q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}\left( \mathbf{x}_{t-1}; \tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0), \tilde{\beta}_t \mathbf{I} \right),
\]</span> where: <span class="math display">\[
\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t,
\]</span> <span class="math display">\[
\tilde{\boldsymbol{\mu}}_t(\mathbf{x}_t, \mathbf{x}_0) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0.
\]</span></p>
</section>
<section id="deriving-the-reverse-mean-via-tweedie" class="level3">
<h3 class="anchored" data-anchor-id="deriving-the-reverse-mean-via-tweedie">Deriving the Reverse Mean via Tweedie</h3>
<p>Applying Tweedie’s formula to the DDPM forward transition <span class="math inline">\(p(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t} \, \mathbf{x}_{t-1}, \beta_t \mathbf{I})\)</span>, we identify <span class="math inline">\(a = \sqrt{1-\beta_t}\)</span> and <span class="math inline">\(\sigma^2 = \beta_t\)</span>. This gives: <span class="math display">\[
\mathbb{E}[\mathbf{x}_{t-1} \mid \mathbf{x}_t] = \frac{1}{\sqrt{1-\beta_t}} \left( \mathbf{x}_t + \beta_t \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t) \right).
\]</span></p>
<p>This motivates the parameterization: <span class="math display">\[
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{1-\beta_t}} \left( \mathbf{x}_t + \beta_t \, s_\theta(\mathbf{x}_t, t) \right),
\]</span> where <span class="math inline">\(s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t)\)</span> is a learned score network.</p>
</section>
<section id="expressing-the-mean-in-terms-of-boldsymbolepsilon" class="level3">
<h3 class="anchored" data-anchor-id="expressing-the-mean-in-terms-of-boldsymbolepsilon">Expressing the Mean in Terms of <span class="math inline">\(\boldsymbol{\epsilon}\)</span></h3>
<p>Using <span class="math inline">\(\mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}} \left( \mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \, \boldsymbol{\epsilon} \right)\)</span>, the posterior mean becomes: <span class="math display">\[
\tilde{\boldsymbol{\mu}}_t = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \right).
\]</span></p>
</section>
</section>
<section id="variational-lower-bound-elbo" class="level2">
<h2 class="anchored" data-anchor-id="variational-lower-bound-elbo">Variational Lower Bound (ELBO)</h2>
<p>The training objective is derived from maximizing the log-likelihood via variational inference.</p>
<section id="jensens-inequality-derivation" class="level3">
<h3 class="anchored" data-anchor-id="jensens-inequality-derivation">Jensen’s Inequality Derivation</h3>
<p>Starting from the cross-entropy objective: <span class="math display">\[\begin{align}
\mathcal{L}_{\mathrm{CE}} &amp;= -\mathbb{E}_{q(\mathbf{x}_0)} \left[ \log p_\theta(\mathbf{x}_0) \right] \\
&amp;= -\mathbb{E}_{q(\mathbf{x}_0)} \left[ \log \int p_\theta(\mathbf{x}_{0:T}) \, d\mathbf{x}_{1:T} \right] \\
&amp;= -\mathbb{E}_{q(\mathbf{x}_0)} \left[ \log \mathbb{E}_{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \left[ \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right] \right] \\
&amp;\leq -\mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)} \right] \\
&amp;= \mathbb{E}_{q(\mathbf{x}_{0:T})} \left[ \log \frac{q(\mathbf{x}_{1:T} \mid \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \right] \equiv \mathcal{L}_{\mathrm{VLB}}.
\end{align}\]</span></p>
</section>
<section id="decomposition-into-kl-divergences" class="level3">
<h3 class="anchored" data-anchor-id="decomposition-into-kl-divergences">Decomposition into KL Divergences</h3>
<p>Using the identity <span class="math inline">\(q(\mathbf{x}_t \mid \mathbf{x}_{t-1}) = q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \frac{q(\mathbf{x}_t \mid \mathbf{x}_0)}{q(\mathbf{x}_{t-1} \mid \mathbf{x}_0)}\)</span>, the VLB decomposes as:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>ELBO Decomposition
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[\begin{align}
\mathcal{L}_{\mathrm{VLB}} &amp;= L_T + L_{T-1} + \cdots + L_0, \\[6pt]
\text{where} \quad L_T &amp;= D_{\mathrm{KL}}\left( q(\mathbf{x}_T \mid \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_T) \right), \\
L_t &amp;= D_{\mathrm{KL}}\left( q(\mathbf{x}_{t-1} \mid \mathbf{x}_t, \mathbf{x}_0) \,\|\, p_\theta(\mathbf{x}_{t-1} \mid \mathbf{x}_t) \right) \quad \text{for } 1 \leq t \leq T-1, \\
L_0 &amp;= -\log p_\theta(\mathbf{x}_0 \mid \mathbf{x}_1).
\end{align}\]</span></p>
</div>
</div>
<p><strong>Interpretation:</strong></p>
<ul>
<li><span class="math inline">\(L_T\)</span>: Constant (no learnable parameters), can be ignored during training</li>
<li><span class="math inline">\(L_t\)</span>: KL between two Gaussians, computable in closed form</li>
<li><span class="math inline">\(L_0\)</span>: Reconstruction term</li>
</ul>
</section>
</section>
<section id="parameterization-and-training-loss" class="level2">
<h2 class="anchored" data-anchor-id="parameterization-and-training-loss">Parameterization and Training Loss</h2>
<section id="noise-prediction-parameterization" class="level3">
<h3 class="anchored" data-anchor-id="noise-prediction-parameterization">Noise Prediction Parameterization</h3>
<p>Instead of predicting <span class="math inline">\(\mathbf{x}_0\)</span> or <span class="math inline">\(\tilde{\boldsymbol{\mu}}_t\)</span> directly, we let the network predict the noise <span class="math inline">\(\boldsymbol{\epsilon}\)</span> via <span class="math inline">\(\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\)</span>: <span class="math display">\[
\boldsymbol{\mu}_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right).
\]</span></p>
<p>The <span class="math inline">\(L_t\)</span> term becomes: <span class="math display">\[
L_t = \mathbb{E}_{\mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{(1-\alpha_t)^2}{2\alpha_t(1-\bar{\alpha}_t)\|\boldsymbol{\Sigma}_\theta\|_2^2} \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\left( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t \right) \right\|^2 \right].
\]</span></p>
</section>
<section id="simplified-loss" class="level3">
<h3 class="anchored" data-anchor-id="simplified-loss">Simplified Loss</h3>
<p>Ho et al.&nbsp;found that training works better with a simplified objective that ignores the weighting:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>DDPM Simplified Loss
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
L_{\text{simple}} = \mathbb{E}_{t \sim [1,T], \mathbf{x}_0, \boldsymbol{\epsilon}} \left[ \left\| \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta\left( \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t \right) \right\|^2 \right]
\]</span></p>
</div>
</div>
</section>
</section>
<section id="connection-to-score-matching" class="level2">
<h2 class="anchored" data-anchor-id="connection-to-score-matching">Connection to Score Matching</h2>
<p>The noise prediction network is directly related to the score: <span class="math display">\[
s_\theta(\mathbf{x}_t, t) \approx \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) = -\frac{\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}}.
\]</span></p>
<p>This shows that DDPM is implicitly learning the score of the perturbed data distribution, just like SMLD!</p>
<p><strong>Weight connection:</strong> Like SMLD, the weight <span class="math inline">\((1 - \alpha_t)\)</span> in the full ELBO is proportional to the inverse Fisher information: <span class="math display">\[
1 - \alpha_t \propto \frac{1}{\mathbb{E}\| \nabla_{\tilde{\mathbf{x}}} \log p_{\alpha_t}(\tilde{\mathbf{x}} \mid \mathbf{x}) \|^2}.
\]</span></p>
</section>
<section id="sampling-ancestral-sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling-ancestral-sampling">Sampling: Ancestral Sampling</h2>
<p>After training, samples are generated by starting from <span class="math inline">\(\mathbf{x}_T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span> and iterating: <span class="math display">\[
\mathbf{x}_{t-1} = \frac{1}{\sqrt{1-\beta_t}} \left( \mathbf{x}_t + \beta_t \, s_{\theta^*}(\mathbf{x}_t, t) \right) + \sqrt{\beta_t} \, \mathbf{z}_t, \qquad t = T, T-1, \ldots, 1,
\]</span> where <span class="math inline">\(\mathbf{z}_t \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>.</p>
<p>Or equivalently using the noise prediction: <span class="math display">\[
\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t) \right) + \sqrt{\beta_t} \, \mathbf{z}_t.
\]</span></p>
</section>
</section>
<section id="sec-sde-unification" class="level1">
<h1>SDE Unification with EDM Framework</h1>
<p>Both SMLD and DDPM perturb data with multiple noise scales. A key insight from Song et al.&nbsp;is that we can generalize this to an <strong>infinite number of noise scales</strong>, where perturbed data distributions evolve according to a <strong>Stochastic Differential Equation (SDE)</strong>. We present this unification using the notation and framework from EDM <span class="citation" data-cites="karras_elucidating_2022">(<a href="#ref-karras_elucidating_2022" role="doc-biblioref">Karras et al. 2022</a>)</span>.</p>
<section id="general-sde-framework" class="level2">
<h2 class="anchored" data-anchor-id="general-sde-framework">General SDE Framework</h2>
<section id="forward-sde" class="level3">
<h3 class="anchored" data-anchor-id="forward-sde">Forward SDE</h3>
<p>Our goal is to construct a diffusion process <span class="math inline">\(\{\mathbf{x}(t)\}_{t=0}^T\)</span> such that:</p>
<ul>
<li><span class="math inline">\(\mathbf{x}(0) \sim p_0 = p_{\mathrm{data}}\)</span> (data distribution)</li>
<li><span class="math inline">\(\mathbf{x}(T) \sim p_T\)</span> (tractable prior, e.g., Gaussian)</li>
</ul>
<p>This diffusion process is modeled as the solution to an Itô SDE:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Forward SDE
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
d\mathbf{x} = f(\mathbf{x}, t) \, dt + g(t) \, d\mathbf{w},
\]</span> where:</p>
<ul>
<li><span class="math inline">\(\mathbf{w}\)</span> is the standard Wiener process (Brownian motion)</li>
<li><span class="math inline">\(f(\cdot, t): \mathbb{R}^d \to \mathbb{R}^d\)</span> is the drift coefficient</li>
<li><span class="math inline">\(g(\cdot): \mathbb{R} \to \mathbb{R}\)</span> is the diffusion coefficient</li>
</ul>
</div>
</div>
<p>We denote by <span class="math inline">\(p_t(\mathbf{x})\)</span> the probability density of <span class="math inline">\(\mathbf{x}(t)\)</span>, and by <span class="math inline">\(p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))\)</span> the transition kernel from time <span class="math inline">\(0\)</span> to time <span class="math inline">\(t\)</span>.</p>
</section>
<section id="reverse-sde" class="level3">
<h3 class="anchored" data-anchor-id="reverse-sde">Reverse SDE</h3>
<p>A remarkable result from Anderson (1982) states that the reverse of a diffusion process is itself a diffusion process, governed by the <strong>reverse-time SDE</strong>:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>Reverse SDE
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
d\mathbf{x} = \left[ f(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] dt + g(t) \, d\bar{\mathbf{w}},
\]</span> where <span class="math inline">\(\bar{\mathbf{w}}\)</span> is a standard Wiener process when time flows backward from <span class="math inline">\(T\)</span> to <span class="math inline">\(0\)</span>, and <span class="math inline">\(dt\)</span> is an infinitesimal negative timestep.</p>
</div>
</div>
<p>Once the score <span class="math inline">\(\nabla_\mathbf{x} \log p_t(\mathbf{x})\)</span> is known for all <span class="math inline">\(t\)</span>, we can simulate this reverse SDE to sample from <span class="math inline">\(p_0 = p_{\mathrm{data}}\)</span>.</p>
</section>
</section>
<section id="edm-parameterization" class="level2">
<h2 class="anchored" data-anchor-id="edm-parameterization">EDM Parameterization</h2>
<p>The EDM framework <span class="citation" data-cites="karras_elucidating_2022">(<a href="#ref-karras_elucidating_2022" role="doc-biblioref">Karras et al. 2022</a>)</span> provides a clean parameterization using a <strong>denoiser</strong> <span class="math inline">\(D_\theta(\mathbf{x}, \sigma)\)</span> that predicts the clean data from a noisy observation.</p>
<section id="denoiser-and-score-relationship" class="level3">
<h3 class="anchored" data-anchor-id="denoiser-and-score-relationship">Denoiser and Score Relationship</h3>
<p>By Tweedie’s formula, the optimal denoiser and the score are related by: <span class="math display">\[
D_\theta(\mathbf{x}, \sigma) = \mathbf{x} + \sigma^2 s_\theta(\mathbf{x}, \sigma),
\]</span> or equivalently: <span class="math display">\[
s_\theta(\mathbf{x}, \sigma) = \frac{D_\theta(\mathbf{x}, \sigma) - \mathbf{x}}{\sigma^2}.
\]</span></p>
</section>
<section id="unified-training-objective" class="level3">
<h3 class="anchored" data-anchor-id="unified-training-objective">Unified Training Objective</h3>
<p>The EDM-style training objective is: <span class="math display">\[
\mathcal{L}(\theta) = \mathbb{E}_{\sigma \sim p(\sigma)} \mathbb{E}_{\mathbf{x}_0 \sim p_{\mathrm{data}}} \mathbb{E}_{\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})} \left[ \lambda(\sigma) \| D_\theta(\mathbf{x}_0 + \sigma \mathbf{n}, \sigma) - \mathbf{x}_0 \|^2 \right],
\]</span> where <span class="math inline">\(\lambda(\sigma)\)</span> is a weighting function and <span class="math inline">\(p(\sigma)\)</span> is a distribution over noise levels.</p>
<p>This is equivalent to the score matching objective: <span class="math display">\[
\mathcal{L}(\theta) = \mathbb{E}_{\sigma, \mathbf{x}_0, \mathbf{n}} \left[ \lambda(\sigma) \sigma^2 \left\| s_\theta(\mathbf{x}_0 + \sigma \mathbf{n}, \sigma) + \frac{\mathbf{n}}{\sigma} \right\|^2 \right].
\]</span></p>
</section>
</section>
<section id="from-smld-to-variance-exploding-ve-sde" class="level2">
<h2 class="anchored" data-anchor-id="from-smld-to-variance-exploding-ve-sde">From SMLD to Variance Exploding (VE) SDE</h2>
<section id="discrete-chain" class="level3">
<h3 class="anchored" data-anchor-id="discrete-chain">Discrete Chain</h3>
<p>The SMLD perturbation kernels correspond to the Markov chain: <span class="math display">\[
\mathbf{x}_i = \mathbf{x}_{i-1} + \sqrt{\sigma_i^2 - \sigma_{i-1}^2} \, \mathbf{z}_{i-1}, \qquad \mathbf{z}_{i-1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
\]</span> with <span class="math inline">\(\sigma_0 = 0\)</span> and <span class="math inline">\(\mathbf{x}_0 \sim p_{\mathrm{data}}\)</span>.</p>
</section>
<section id="continuous-limit" class="level3">
<h3 class="anchored" data-anchor-id="continuous-limit">Continuous Limit</h3>
<p>Define <span class="math inline">\(\Delta t = 1/N\)</span> and let <span class="math inline">\(t \in \{0, \Delta t, 2\Delta t, \ldots\}\)</span>. Rewriting: <span class="math display">\[
\mathbf{x}(t + \Delta t) = \mathbf{x}(t) + \sqrt{\sigma^2(t + \Delta t) - \sigma^2(t)} \, \mathbf{z}(t) \approx \mathbf{x}(t) + \sqrt{\frac{d\sigma^2(t)}{dt} \Delta t} \, \mathbf{z}(t).
\]</span></p>
<p>As <span class="math inline">\(\Delta t \to 0\)</span>, this converges to the <strong>Variance Exploding (VE) SDE</strong>:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>VE SDE (from SMLD)
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
d\mathbf{x} = \sqrt{\frac{d\sigma^2(t)}{dt}} \, d\mathbf{w}
\]</span></p>
<p><strong>Drift:</strong> <span class="math inline">\(f(\mathbf{x}, t) = 0\)</span></p>
<p><strong>Diffusion:</strong> <span class="math inline">\(g(t) = \sqrt{\frac{d\sigma^2(t)}{dt}}\)</span></p>
<p><strong>Perturbation kernel:</strong> <span class="math display">\[
p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) = \mathcal{N}\left( \mathbf{x}(t); \mathbf{x}(0), \sigma^2(t) \mathbf{I} \right)
\]</span></p>
</div>
</div>
<p>The VE SDE has <strong>exploding variance</strong> as <span class="math inline">\(t \to \infty\)</span> since the variance grows unboundedly with <span class="math inline">\(\sigma^2(t)\)</span>.</p>
</section>
</section>
<section id="from-ddpm-to-variance-preserving-vp-sde" class="level2">
<h2 class="anchored" data-anchor-id="from-ddpm-to-variance-preserving-vp-sde">From DDPM to Variance Preserving (VP) SDE</h2>
<section id="discrete-chain-1" class="level3">
<h3 class="anchored" data-anchor-id="discrete-chain-1">Discrete Chain</h3>
<p>The DDPM forward process is: <span class="math display">\[
\mathbf{x}_i = \sqrt{1 - \beta_i} \, \mathbf{x}_{i-1} + \sqrt{\beta_i} \, \mathbf{z}_{i-1}, \qquad \mathbf{z}_{i-1} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
\]</span></p>
</section>
<section id="continuous-limit-1" class="level3">
<h3 class="anchored" data-anchor-id="continuous-limit-1">Continuous Limit</h3>
<p>Define <span class="math inline">\(\bar{\beta}_i = N \beta_i\)</span> and rewrite: <span class="math display">\[
\mathbf{x}_i = \left(1 - \frac{\bar{\beta}_i}{N}\right) \mathbf{x}_{i-1} + \sqrt{\frac{\bar{\beta}_i}{N}} \, \mathbf{z}_{i-1}.
\]</span></p>
<p>With <span class="math inline">\(\Delta t = 1/N\)</span> and <span class="math inline">\(\beta(t) = \bar{\beta}_i\)</span>: <span class="math display">\[\begin{align}
\mathbf{x}(t + \Delta t) &amp;= (1 - \beta(t)\Delta t) \mathbf{x}(t) + \sqrt{\beta(t)\Delta t} \, \mathbf{z}(t) \\
&amp;\approx \mathbf{x}(t) - \beta(t)\Delta t \, \mathbf{x}(t) + \sqrt{\beta(t)\Delta t} \, \mathbf{z}(t).
\end{align}\]</span></p>
<p>As <span class="math inline">\(\Delta t \to 0\)</span>, this converges to the <strong>Variance Preserving (VP) SDE</strong>:</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>VP SDE (from DDPM)
</div>
</div>
<div class="callout-body-container callout-body">
<p><span class="math display">\[
d\mathbf{x} = -\frac{1}{2}\beta(t) \mathbf{x} \, dt + \sqrt{\beta(t)} \, d\mathbf{w}
\]</span></p>
<p><strong>Drift:</strong> <span class="math inline">\(f(\mathbf{x}, t) = -\frac{1}{2}\beta(t) \mathbf{x}\)</span></p>
<p><strong>Diffusion:</strong> <span class="math inline">\(g(t) = \sqrt{\beta(t)}\)</span></p>
<p><strong>Perturbation kernel:</strong> <span class="math display">\[
p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) = \mathcal{N}\left( \mathbf{x}(t); \sqrt{\bar{\alpha}(t)} \, \mathbf{x}(0), (1 - \bar{\alpha}(t)) \mathbf{I} \right)
\]</span> where <span class="math inline">\(\bar{\alpha}(t) = \exp\left(-\int_0^t \beta(s) \, ds\right)\)</span>.</p>
</div>
</div>
<p>The VP SDE has <strong>bounded variance</strong>. If the initial distribution has unit variance, then the variance remains at unity for all <span class="math inline">\(t\)</span>.</p>
</section>
</section>
<section id="variance-dynamics" class="level2">
<h2 class="anchored" data-anchor-id="variance-dynamics">Variance Dynamics</h2>
<section id="vp-sde-covariance" class="level3">
<h3 class="anchored" data-anchor-id="vp-sde-covariance">VP SDE Covariance</h3>
<p>For the VP SDE, the covariance <span class="math inline">\(\boldsymbol{\Sigma}_{\mathrm{VP}}(t) = \mathrm{Cov}[\mathbf{x}(t)]\)</span> evolves according to: <span class="math display">\[
\frac{d}{dt} \boldsymbol{\Sigma}_{\mathrm{VP}}(t) = \beta(t) \left( \mathbf{I} - \boldsymbol{\Sigma}_{\mathrm{VP}}(t) \right).
\]</span></p>
<p>Solving this ODE: <span class="math display">\[
\boldsymbol{\Sigma}_{\mathrm{VP}}(t) = \mathbf{I} + e^{-\int_0^t \beta(s) \, ds} \left( \boldsymbol{\Sigma}_{\mathrm{VP}}(0) - \mathbf{I} \right).
\]</span></p>
<p>This shows that:</p>
<ul>
<li>The covariance is always bounded</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}_{\mathrm{VP}}(t) \equiv \mathbf{I}\)</span> if <span class="math inline">\(\boldsymbol{\Sigma}_{\mathrm{VP}}(0) = \mathbf{I}\)</span></li>
<li>As <span class="math inline">\(t \to \infty\)</span>, <span class="math inline">\(\boldsymbol{\Sigma}_{\mathrm{VP}}(t) \to \mathbf{I}\)</span> (if <span class="math inline">\(\int_0^\infty \beta(s) \, ds = \infty\)</span>)</li>
</ul>
</section>
<section id="sub-vp-sde" class="level3">
<h3 class="anchored" data-anchor-id="sub-vp-sde">Sub-VP SDE</h3>
<p>Inspired by the VP SDE, one can define the <strong>sub-VP SDE</strong>: <span class="math display">\[
d\mathbf{x} = -\frac{1}{2}\beta(t)\mathbf{x} \, dt + \sqrt{\beta(t)\left(1 - e^{-2\int_0^t \beta(s) \, ds}\right)} \, d\mathbf{w}.
\]</span></p>
<p>This SDE has variance always bounded above by the VP SDE (hence “sub-VP”), and converges to the same limiting distribution.</p>
</section>
</section>
<section id="perturbation-kernels-summary" class="level2">
<h2 class="anchored" data-anchor-id="perturbation-kernels-summary">Perturbation Kernels Summary</h2>
<p>Since VE, VP, and sub-VP SDEs all have affine drift coefficients, their perturbation kernels are Gaussian with closed forms:</p>
<div id="tbl-kernels" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-kernels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Perturbation kernels for different SDEs
</figcaption>
<div aria-describedby="tbl-kernels-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 92%">
</colgroup>
<thead>
<tr class="header">
<th>SDE</th>
<th>Perturbation Kernel <span class="math inline">\(p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>VE</strong></td>
<td><span class="math inline">\(\mathcal{N}\left(\mathbf{x}(0), \sigma^2(t) \mathbf{I}\right)\)</span></td>
</tr>
<tr class="even">
<td><strong>VP</strong></td>
<td><span class="math inline">\(\mathcal{N}\left(\sqrt{\bar{\alpha}(t)} \mathbf{x}(0), (1 - \bar{\alpha}(t)) \mathbf{I}\right)\)</span></td>
</tr>
<tr class="odd">
<td><strong>sub-VP</strong></td>
<td><span class="math inline">\(\mathcal{N}\left(\sqrt{\bar{\alpha}(t)} \mathbf{x}(0), (1 - \bar{\alpha}(t))^2 \mathbf{I}\right)\)</span></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="reverse-sdes" class="level2">
<h2 class="anchored" data-anchor-id="reverse-sdes">Reverse SDEs</h2>
<p>Using the general reverse SDE formula, we can derive the reverse processes:</p>
<section id="ve-reverse-sde" class="level3">
<h3 class="anchored" data-anchor-id="ve-reverse-sde">VE Reverse SDE</h3>
<p><span class="math display">\[
d\mathbf{x} = -\frac{d\sigma^2(t)}{dt} \nabla_\mathbf{x} \log p_t(\mathbf{x}) \, dt + \sqrt{\frac{d\sigma^2(t)}{dt}} \, d\bar{\mathbf{w}}
\]</span></p>
</section>
<section id="vp-reverse-sde" class="level3">
<h3 class="anchored" data-anchor-id="vp-reverse-sde">VP Reverse SDE</h3>
<p><span class="math display">\[
d\mathbf{x} = \left[ -\frac{1}{2}\beta(t)\mathbf{x} - \beta(t) \nabla_\mathbf{x} \log p_t(\mathbf{x}) \right] dt + \sqrt{\beta(t)} \, d\bar{\mathbf{w}}
\]</span></p>
</section>
</section>
<section id="unified-view-score-estimation-for-any-sde" class="level2">
<h2 class="anchored" data-anchor-id="unified-view-score-estimation-for-any-sde">Unified View: Score Estimation for Any SDE</h2>
<p>The training objective for any SDE is the same denoising score matching objective: <span class="math display">\[
\theta^* = \arg\min_\theta \mathbb{E}_{t \sim \mathcal{U}[0,T]} \lambda(t) \, \mathbb{E}_{\mathbf{x}(0) \sim p_{\mathrm{data}}} \mathbb{E}_{\mathbf{x}(t) \sim p_{0t}(\cdot \mid \mathbf{x}(0))} \left\| s_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) \right\|_2^2.
\]</span></p>
<p>Since the perturbation kernels are Gaussian, the score of the kernel has closed form:</p>
<ul>
<li><strong>VE:</strong> <span class="math inline">\(\nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) = -\frac{\mathbf{x}(t) - \mathbf{x}(0)}{\sigma^2(t)}\)</span></li>
<li><strong>VP:</strong> <span class="math inline">\(\nabla_{\mathbf{x}(t)} \log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) = -\frac{\mathbf{x}(t) - \sqrt{\bar{\alpha}(t)} \mathbf{x}(0)}{1 - \bar{\alpha}(t)}\)</span></li>
</ul>
</section>
</section>
<section id="sec-summary" class="level1">
<h1>Summary and Comparison</h1>
<section id="comparison-table" class="level2">
<h2 class="anchored" data-anchor-id="comparison-table">Comparison Table</h2>
<div id="tbl-comparison" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Comparison of diffusion model formulations
</figcaption>
<div aria-describedby="tbl-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>SMLD</th>
<th>DDPM</th>
<th>VE SDE</th>
<th>VP SDE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Forward Process</strong></td>
<td>Add noise at scales <span class="math inline">\(\sigma_i\)</span></td>
<td>Markov chain with <span class="math inline">\(\beta_t\)</span></td>
<td><span class="math inline">\(d\mathbf{x} = \sqrt{\frac{d\sigma^2}{dt}} d\mathbf{w}\)</span></td>
<td><span class="math inline">\(d\mathbf{x} = -\frac{1}{2}\beta \mathbf{x} dt + \sqrt{\beta} d\mathbf{w}\)</span></td>
</tr>
<tr class="even">
<td><strong>Perturbation Kernel</strong></td>
<td><span class="math inline">\(\mathcal{N}(\mathbf{x}, \sigma_i^2 \mathbf{I})\)</span></td>
<td><span class="math inline">\(\mathcal{N}(\sqrt{\bar{\alpha}_t} \mathbf{x}, (1-\bar{\alpha}_t)\mathbf{I})\)</span></td>
<td><span class="math inline">\(\mathcal{N}(\mathbf{x}, \sigma^2(t) \mathbf{I})\)</span></td>
<td><span class="math inline">\(\mathcal{N}(\sqrt{\bar{\alpha}(t)} \mathbf{x}, (1-\bar{\alpha}(t))\mathbf{I})\)</span></td>
</tr>
<tr class="odd">
<td><strong>Variance Behavior</strong></td>
<td>Exploding</td>
<td>Preserving</td>
<td>Exploding</td>
<td>Preserving</td>
</tr>
<tr class="even">
<td><strong>Training Target</strong></td>
<td>Score <span class="math inline">\(s_\theta\)</span></td>
<td>Noise <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span></td>
<td>Score <span class="math inline">\(s_\theta\)</span> or Denoiser <span class="math inline">\(D_\theta\)</span></td>
<td>Score <span class="math inline">\(s_\theta\)</span> or Denoiser <span class="math inline">\(D_\theta\)</span></td>
</tr>
<tr class="odd">
<td><strong>Sampling</strong></td>
<td>Annealed Langevin</td>
<td>Ancestral sampling</td>
<td>Reverse SDE</td>
<td>Reverse SDE</td>
</tr>
<tr class="even">
<td><strong>Time</strong></td>
<td>Discrete (<span class="math inline">\(N\)</span> scales)</td>
<td>Discrete (<span class="math inline">\(T\)</span> steps)</td>
<td>Continuous</td>
<td>Continuous</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ol type="1">
<li><p><strong>Score is central:</strong> All methods learn the score function <span class="math inline">\(\nabla_\mathbf{x} \log p_t(\mathbf{x})\)</span>, whether explicitly (SMLD) or implicitly through noise prediction (DDPM) or denoising (EDM).</p></li>
<li><p><strong>Tweedie connects denoising and score:</strong> The optimal denoiser and the score are related by Tweedie’s formula: <span class="math inline">\(D^*(\mathbf{y}) = \mathbf{y} + \sigma^2 \nabla \log p_\sigma(\mathbf{y})\)</span>.</p></li>
<li><p><strong>Continuous unification:</strong> SMLD and DDPM are discretizations of the VE and VP SDEs respectively. The SDE framework provides a principled way to design new diffusion processes.</p></li>
<li><p><strong>Flexible parameterization:</strong> EDM shows that we can parameterize the model as a denoiser <span class="math inline">\(D_\theta\)</span>, noise predictor <span class="math inline">\(\boldsymbol{\epsilon}_\theta\)</span>, or score <span class="math inline">\(s_\theta\)</span>—they are all equivalent up to simple transformations.</p></li>
<li><p><strong>Weight normalization:</strong> The optimal training weights are proportional to the inverse Fisher information, ensuring balanced contribution across noise levels.</p></li>
</ol>
</section>
<section id="references" class="level2">




</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-karras_elucidating_2022" class="csl-entry" role="listitem">
Karras, Tero, Miika Aittala, Timo Aila, and Samuli Laine. 2022. <span>“Elucidating the Design Space of Diffusion-Based Generative Models,”</span> no. <span>arXiv</span>:2206.00364 (October). <a href="https://doi.org/10.48550/arXiv.2206.00364">https://doi.org/10.48550/arXiv.2206.00364</a>.
</div>
<div id="ref-song_score-based_2021" class="csl-entry" role="listitem">
Song, Yang, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. <span>“Score-Based Generative Modeling Through Stochastic Differential Equations,”</span> no. <span>arXiv</span>:2011.13456 (February). <a href="https://doi.org/10.48550/arXiv.2011.13456">https://doi.org/10.48550/arXiv.2011.13456</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{brosse2024,
  author = {Brosse, Nicolas},
  title = {Origins of {Diffusion} {Models:} {From} {SMLD} and {DDPM} to
    {SDE} {Unification}},
  date = {2024-12-15},
  url = {https://nbrosse.github.io/posts/origin-diffusion-flow-matching/origin-diffusion-flow-matching.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-brosse2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Brosse, Nicolas. 2024. <span>“Origins of Diffusion Models: From SMLD and
DDPM to SDE Unification.”</span> December 15, 2024. <a href="https://nbrosse.github.io/posts/origin-diffusion-flow-matching/origin-diffusion-flow-matching.html">https://nbrosse.github.io/posts/origin-diffusion-flow-matching/origin-diffusion-flow-matching.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>