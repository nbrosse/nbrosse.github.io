---
title: "Closed-Form Flow Matching: High-Dimensional Statistics and the Softmax Collapse"
author: "Nicolas Brosse"
date: "2025-12-10"
bibliography: closed-form-fm-references.bib
categories: [deep learning, generative models, flow matching]
description: "An analysis of the closed-form optimal velocity field in flow matching, explaining why softmax collapse occurs early in high dimensions and how to predict the collapse time."
toc: true
toc-depth: 3
citation:
  url: https://nbrosse.github.io/posts/closed-form-flow-matching/closed-form-fm.html
---

Flow matching has emerged as a powerful framework for training continuous normalizing flows. In this blog post, building on @bertrand_closed-form_2025, we analyze the **closed-form optimal velocity field**: its connection to the conditional vector field and the challenges neural networks face in approximating it. We explore how, in high dimensions, the softmax weights collapse to near one-hot vectors surprisingly early in the interpolation path, and provide a formula to predict when this collapse occurs.

We use @biroli_dynamical_2024 to understand the softmax collapse in flow matching, contrary to what is claimed in @bertrand_closed-form_2025, the framework of @biroli_dynamical_2024 is able to predict the collapse time in flow matching. We adapt their arguments to the context of flow matching and provide a formula to predict the collapse time, in particular for CIFAR-10.

In this blog post, we will present a rigorous analysis of the softmax collapse in flow matching, using tools from statistical physics, specifically the Random Energy Model (REM) and Large Deviation Principles (LDP).

Contributions:

- We do a connection between the observations on the training of the marginal velocity field (based on the conditional velocity field loss) and the collapse of the softmax weights and the probability distribution.
- We provide a self-contained proof of the LDP and REM perhaps better than the one in @bertrand_closed-form_2025 and useful for other readers.

# Setup and Notation

We follow the notation and setup of[ @lipman_flow_2024, Section 2] to present Flow Matching (FM). Given a training dataset with samples from an unknown target distribution $q$ over $\mathbb{R}^d$, the goal is to learn a model that can generate new samples from $q$. Flow Matching (FM) does this by defining a **continuous probability path** $(p_t)_{0\le t\le 1}$ that morphs a **known source distribution** $p_0=p$ (typically noise) into the **data distribution** $p_1=q$. FM learns a **time-dependent velocity field** $u_t$ (a vector field) that describes how samples move along this path. After training, sampling from $q$ works by:

1.  Draw $X_0 \sim p$ (easy noise).
2.  Solve an **ODE** driven by the learned velocity field to transport $X_0$ to $X_1$, which should follow $q$.

FM uses a time-dependent vector field: $u : [0,1]\times \mathbb{R}^d \to \mathbb{R}^d$ and defines the associated **flow map** $\psi_t$ by the ODE: 
$$
\frac{d}{dt}\psi_t(x) = u_t(\psi_t(x)), \quad \psi_0(x)=x. 
$$

If $u_t$ generates the probability path $p_t$, then pushing forward the source sample $X_0\sim p_0$ through the flow gives: $X_t := \psi_t(X_0) \sim p_t$. In particular, integrating to $t=1$ yields: $X_1=\psi_1(X_0) \approx q$. So the core objective is: **learn a neural velocity field** $u^\theta_t$ whose flow maps $p_0$ to $p_1$. Ideally, FM would minimize the mean-squared error between the learned and true velocity fields: 
$$
\mathcal{L}_{\text{FM}}(\theta)=\mathbb{E}_{t,X_t}\Big[\ \|u^\theta_t(X_t)-u_t(X_t)\|^2\ \Big],
\quad t\sim \mathcal{U}[0,1],\ X_t\sim p_t.
$$ 

But the marginal velocity field $u_t$ is usually hard to compute. Let the source be standard Gaussian: $p := p_0 = \mathcal{N}(0,I)$. A convenient path can be built by mixing **conditional paths** $p_{t|1}(x|x_1)$, each conditioned on a data example $X_1=x_1$. The resulting marginal path is: 
$$
p_t(x)=\int p_{t|1}(x|x_1) q(x_1) dx_1,
\quad\text{where}\quad
p_{t|1}(x|x_1)=\mathcal{N}\big(x\mid t x_1,\ (1-t)^2 I\big).
$$

A simple way to sample from this $p_t$ is: $X_t = tX_1 + (1-t)X_0 \sim p_t$, where $X_0\sim p$ and $X_1\sim q$. Condition on a randomly chosen training sample $X_1=x_1$. Define: $X_{t|1}=t x_1+(1-t)X_0 \sim p_{t|1}(\cdot|x_1)$. For this conditional Gaussian path, the ODE gives a **simple closed-form conditional velocity field**: 
$$
u_t(x|x_1)=\frac{x_1-x}{1-t}.
$$

This yields a tractable loss: 
$$
\mathcal{L}_{\text{CFM}}(\theta)=
\mathbb{E}_{t,X_t,X_1}\Big[\ \|u^\theta_t(X_t)-u_t(X_t|X_1)\|^2\ \Big],
\quad t\sim\mathcal{U}[0,1],\ X_0\sim p,\ X_1\sim q,
$$ 

with $X_t=(1-t)X_0+tX_1$. Crucially, this conditional objective provides the **same gradients** as the original FM loss: 
$$
\nabla_\theta \mathcal{L}_{\text{FM}}(\theta)=\nabla_\theta \mathcal{L}_{\text{CFM}}(\theta).
$$

Plugging the conditional velocity into the loss gives a common practical form: 
$$
\mathcal{L}^{\text{OT,Gauss}}_{\text{CFM}}(\theta)
= \mathbb{E}_{t,X_0,X_1}\Big[\ \|u^\theta_t(X_t)-(X_1-X_0)\|^2\ \Big],
\quad t\sim\mathcal{U}[0,1],\ X_0\sim\mathcal{N}(0,I),\ X_1\sim q,
$$ 

where $X_t=(1-t)X_0+tX_1$.

### Loss Decomposition: CFM = FM + Irreducible Variance

The CFM and FM losses are related by a bias-variance decomposition. The CFM loss can be written as:
$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathcal{L}_{\text{FM}}(\theta) + \mathbb{E}_{t, X_t}\left[\mathrm{Var}(X_1 - X_0 \mid X_t)\right].
$$

::: {.callout-note collapse="true"}
## Proof of the loss decomposition

**Key observation:** At the interpolant $X_t = (1-t)X_0 + tX_1$, the conditional velocity field evaluates to:
$$
u_t(X_t \mid X_1) = \frac{X_1 - X_t}{1-t} = \frac{X_1 - (1-t)X_0 - tX_1}{1-t} = X_1 - X_0.
$$

The marginal velocity field at $X_t$ is the conditional expectation of this:
$$
u_t(X_t) = \mathbb{E}[X_1 - X_0 \mid X_t].
$$

**Decomposition:** Let $Y = X_1 - X_0$ denote the target and $\bar{Y} = \mathbb{E}[Y \mid X_t] = u_t(X_t)$ the optimal predictor. We decompose:
$$
\|u^\theta_t(X_t) - Y\|^2 = \|(u^\theta_t(X_t) - \bar{Y}) + (\bar{Y} - Y)\|^2.
$$

Expanding:
$$
= \|u^\theta_t(X_t) - \bar{Y}\|^2 + \|\bar{Y} - Y\|^2 + 2(u^\theta_t(X_t) - \bar{Y})^\top(\bar{Y} - Y).
$$

**The cross-term vanishes:** Taking the conditional expectation given $X_t$:
$$
\mathbb{E}\left[(u^\theta_t(X_t) - \bar{Y})^\top(\bar{Y} - Y) \mid X_t\right] = (u^\theta_t(X_t) - \bar{Y})^\top \mathbb{E}[\bar{Y} - Y \mid X_t] = 0,
$$
since $\mathbb{E}[Y \mid X_t] = \bar{Y}$ by definition.

**Taking expectations:** We obtain:
\begin{align}
\mathcal{L}_{\text{CFM}}(\theta) &= \mathbb{E}_{t,X_t}\left[\|u^\theta_t(X_t) - u_t(X_t)\|^2\right] + \mathbb{E}_{t,X_t}\left[\mathbb{E}\left[\|Y - \bar{Y}\|^2 \mid X_t\right]\right] \\
&= \mathcal{L}_{\text{FM}}(\theta) + \mathbb{E}_{t,X_t}\left[\mathrm{Var}(X_1 - X_0 \mid X_t)\right].
\end{align}

Here, $\mathrm{Var}(X_1 - X_0 \mid X_t) := \mathbb{E}[\|X_1 - X_0 - \mathbb{E}[X_1 - X_0 \mid X_t]\|^2 \mid X_t]$ is the trace of the conditional covariance matrix.
:::

**Alternative expression:** Since $X_1 - X_0 = \frac{X_1 - X_t}{1-t}$ (the conditional velocity at the interpolant), the irreducible variance can be written as:
$$
\mathrm{Var}(X_1 - X_0 \mid X_t) = \frac{1}{(1-t)^2}\mathrm{Var}(X_1 \mid X_t).
$$
This form makes the connection to the conditional velocity field $u_t(x \mid x_1) = \frac{x_1 - x}{1-t}$ explicit: the irreducible variance is the posterior variance of $X_1$ given $X_t$, scaled by the squared velocity coefficient $\frac{1}{(1-t)^2}$.

The second term is the **irreducible variance** — it does not depend on $\theta$ and represents the inherent uncertainty in predicting the conditional velocity $X_1 - X_0$ from $X_t$ alone. This variance captures how much the target $X_1 - X_0$ fluctuates across different $(X_0, X_1)$ pairs that pass through the same interpolant $X_t$.

# Observations on the marginal vector field, conditional vector field and training difficulties

In this section, we present some observations made in @bertrand_closed-form_2025. We focus on the case where the source distribution $p_0$ is the standard normal distribution and the data distribution $p_{\text{data}}$ is the CIFAR-10 dataset. We observe $n$ datapoints from CIFAR-10: 
$$
x^{(1)},\dots,x^{(n)} \sim p_{\text{data}}, \qquad x^{(i)}\in\mathbb{R}^d.
$$

In that scenario, we do not have access to the data distribution $p_{\text{data}}$, but only to the empirical distribution: 
$$
\hat p_{\text{data}}:=\frac1n\sum_{i=1}^n \delta_{x^{(i)}}.
$$ {#eq-empirical-distribution}

The marginal velocity field becomes available in closed form:
$$
u_t(x) = \sum_{i=1}^n \lambda_i(x,t) \frac{x^{(i)} - x}{1-t},
\quad
\lambda_i(x,t)=\mathrm{softmax}\left(-\frac{\|x-t x^{(i)}\|^2}{2(1-t)^2}\right).
$$

where the softmax is taken over $i\in\{1, \dots, n\}$, i.e.
$$
\lambda_i(x,t)=\frac{\exp\left(-\frac{\|x-t x^{(i)}\|^2}{2(1-t)^2}\right)}{\sum_{j=1}^n \exp\left(-\frac{\|x-t x^{(j)}\|^2}{2(1-t)^2}\right)}.
$$

@bertrand_closed-form_2025 focuses on the memorization paradox, the fact that in that context with the empirical distribution @eq-empirical-distribution, if the learning procedure outputs the marginal vector field, we recovers exactly the $n$ datapoints and not random samples from $p_{\text{data}}$.

In their study, they study the relation between the marginal velocity field available in closed form, the conditional velocity field and the empirical velocity field learned by a neural network. We focus on this aspect of their work.

They have 2 main observations:

1. As the dimension grows, the marginal velocity field is rapidly correlated (even when t is very small) to the conditional velocity field.
2. The neural network has difficulties to learn the marginal velocity field for $t$ close to 0.1 and close to 1.

We will expand on these observations in the next sections. The objective of this blog post is to provide some orders of magnitude of the softmax weights of the marginal velocity field, in particular in high dimension for CIFAR-10.

![The histograms of the cosine similarities between $u_t((1 − t)x_0 + tx_1)$ and $u_t((1 − t)x_0 + tx_1  \mid x_1) = x_1 − x_0$ are displayed for various time values $t$ and two datasets: 2-moons and CIFAR-10. [@bertrand_closed-form_2025, figure 1]](figures/article-fig-1.png){#fig-article-fig-1 fig-alt="Histograms of the cosine similarities between $u_t((1 − t)x_0 + tx_1)$ and $u_t((1 − t)x_0 + tx_1 \mid x_1) = x_1 − x_0$ for various time values $t$ and two datasets: 2-moons and CIFAR-10."}

![Proportion of samples $x_t$ for which the cosine similarity between $u_t(x_t)$ and $u_t(x_t \mid x_1)$ exceeds 0.9, as a function of time $t$. This analysis is performed across multiple spatial resolutions of the Imagenette dataset, obtaining $d \times d$ images by spatial subsampling. [@bertrand_closed-form_2025, figure 2]](figures/article-fig-2.png){#fig-article-fig-2 fig-alt="Proportion of samples $x_t$ for which the cosine similarity between $u_t(x_t)$ and $u_t(x_t \mid x_1)$ exceeds 0.9, as a function of time $t$. This analysis is performed across multiple spatial resolutions of the Imagenette dataset, obtaining $d \times d$ images by spatial subsampling."}

![Failure to learn the marginal velocity field on CIFAR-10: the leftmost figure represents the average error between the marginal empirical velocity field $u_t$ and the learned velocity $u^\theta_t$ for multiple values of time $t$. All the quantities are computed/learned on a varying number of training samples ($10$ to $10^4$) of the CIFAR-10 dataset. We are not interested in the other figures in this blog post. [@bertrand_closed-form_2025, figure 3]](figures/article-fig-3.png){#fig-article-fig-3 fig-alt="Failure to learn the marginal velocity field on CIFAR-10: the leftmost figure represents the average error between the marginal empirical velocity field $u_t$ and the learned velocity $u^\theta_t$ for multiple values of time $t$. All the quantities are computed/learned on a varying number of training samples ($10$ to $10^4$) of the CIFAR-10 dataset. We are not interested in the other figures in this blog post."}

# CIFAR-10 Preprocessing and Statistics

We use standard CIFAR-10 preprocessing following the [conditional-flow-matching](https://github.com/atong01/conditional-flow-matching) repository:

``` python
dataset = datasets.CIFAR10(
    root="./data",
    train=True,
    download=True,
    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ]),
)
```

With `ToTensor()` (pixels in $[0,1]$) followed by normalization:

$$
x = \frac{\text{pixel} - 0.5}{0.5} = 2 \cdot \text{pixel} - 1 \in [-1, 1]
$$

Each coordinate is bounded and roughly centered near 0. In high dimension, the vector behaves like a collection of weakly dependent coordinates.

**Key Parameters**

| Parameter  | Value                          | Description                |
|------------|--------------------------------|----------------------------|
| $d$        | $3 \times 32 \times 32 = 3072$ | Dimension                  |
| $\sigma^2$ | $\approx 0.24$                 | Per-coordinate variance    |
| $n$        | $50{,}000$                     | Number of training samples |


A [commonly cited computation](https://github.com/kuangliu/pytorch-cifar/issues/19) over the CIFAR-10 training set gives:

-   mean $\approx (0.4914, 0.4822, 0.4465)$
-   std $\approx (0.247, 0.243, 0.261)$

(Those are for pixel values in $[0,1]$, i.e., after `ToTensor()`.)

With normalization by Torchvision, the variance transforms as $y = \frac{x - \mu}{\sigma}$ with $\mu=0.5$ and $\sigma=0.5$, so:

$$
\mathrm{Var}(y) = \frac{\mathrm{Var}(x)}{\sigma^2} = \frac{\mathrm{Var}(x)}{0.25} = 4 \mathrm{Var}(x).
$$

Using the CIFAR-10 channel stds above:

-   $\mathrm{Var}(x_R) \approx 0.247^2 \approx 0.0610 \Rightarrow \mathrm{Var}(y_R)\approx 4\cdot 0.0610 \approx 0.244$
-   $\mathrm{Var}(x_G) \approx 0.243^2 \approx 0.0590 \Rightarrow \mathrm{Var}(y_G)\approx 0.236$
-   $\mathrm{Var}(x_B) \approx 0.261^2 \approx 0.0681 \Rightarrow \mathrm{Var}(y_B)\approx 0.273$

So  we use the variance $\approx 0.24$ for the CIFAR-10 dataset.

Do a remark on channel => pixel statistics. 

Yes — they should be the same **order of magnitude**, and in fact they’re tightly linked by a simple identity.

Let $X_{u,v}$ be the (say) red-channel pixel at location $(u,v)$ when you sample a random CIFAR image. Let $(U,V)$ be a uniform random pixel location independent of the image, and define the “global random pixel”
$$
Y := X_{U,V}.
$$

Then you have the law of total variance:
$$
\mathrm{Var}(Y) = \mathbb E_{U,V}\big[\mathrm{Var}(X_{u,v})\big] + \mathrm{Var}_{U,V}\big(\mathbb E[X_{u,v}]\big).
$$

So:

* **Average per-pixel variance** is $\mathbb E_{u,v}[\mathrm{Var}(X_{u,v})]$.
* **Global variance over all pixels pooled** is $\mathrm{Var}(Y)$.

They differ by the extra nonnegative term $\mathrm{Var}_{U,V}\big(\mathbb E[X_{u,v}]\big)$, i.e. how much the *mean image* varies across location. On CIFAR-10, that term exists (background/center bias etc.) but it's not typically enormous relative to the pixel variance itself.

They’re not only same order:
$$
\mathbb E_{u,v}[\mathrm{Var}(X_{u,v})] \le \mathrm{Var}(Y)
$$
and the gap is exactly “variance of the per-pixel means”.
After your normalization by 0.5, everything’s scaled by 4 in variance, so the same relationship holds.

So: **yes, same magnitude; global is typically a bit larger** than the average per-pixel variance because of spatial variation in the mean image.

In the following, we assume that the $n$ datapoints from the CFAR-10 dataset are i.i.d. Gaussian vectors with variance $\sigma^2 \approx 0.24$.
Modeling images as random vectors with i.i.d.-ish coordinates of variance $\sigma^2$.

Of course, it is wrong. It is not i.i.d. Gaussian vectors, see @fig-eigenvalues-cifar10.

![The eigenvalues of the covariance matrix of the CIFAR-10 dataset.](figures/eigenvalues-cifar10.png){#fig-eigenvalues-cifar10 fig-alt="The eigenvalues of the covariance matrix of the CIFAR-10 dataset."}

# Marginal Velocity Field Collapse: A Statistical Physics Perspective

This section provides a rigorous analysis of the collapse phenomenon in the marginal velocity field using tools from statistical physics, specifically the Random Energy Model (REM) and Large Deviation Principles (LDP).

## Gibbs Measure and Energy Formulation

### The Marginal Velocity Field as a Gibbs Measure

The marginal velocity field in Flow Matching is given by:
$$
u_t(x) = \sum_{i=1}^n \lambda_i(x,t) \frac{x^{(i)} - x}{1-t}
$$

where the weights $\lambda_i(x,t)$ take the form:
$$
\lambda_i(x,t) = \frac{\exp\left(-\frac{\|x - tx^{(i)}\|^2}{2(1-t)^2}\right)}{\sum_{j=1}^n \exp\left(-\frac{\|x - tx^{(j)}\|^2}{2(1-t)^2}\right)}.
$$

This is precisely a **Boltzmann distribution** at inverse temperature $\beta = 1$ over "states" $i \in \{1, \dots, n\}$ with energy:
$$
E_i(x,t) = \frac{\|x - tx^{(i)}\|^2}{2(1-t)^2}.
$$

The denominator defines the **partition function**:
$$
Z(x,t) = \sum_{j=1}^n e^{-E_j(x,t)}.
$$

### Assumptions on the Data

We consider a **Gaussian proxy** for the training data:
$$
x^{(i)} \stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2 I_d), \quad i = 1, \dots, n.
$$

This approximation captures the essential high-dimensional geometry while enabling analytical tractability. For CIFAR-10 after standard normalization, $\sigma^2 \approx 0.24$ provides a reasonable proxy.

### The "Planted" Trajectory

Consider a trajectory starting from noise and targeting a specific training image:
$$
x_t = (1-t)x_0 + tx_1, \quad x_0 \sim \mathcal{N}(0, I_d), \quad x_1 = x^{(1)}.
$$

We call $i = 1$ the **special (planted) index**. For this index:
$$
x_t - tx^{(1)} = (1-t)x_0
$$

which gives the **planted energy**:
$$
E_1(x_t, t) = \frac{\|(1-t)x_0\|^2}{2(1-t)^2} = \frac{\|x_0\|^2}{2}.
$$

By the law of large numbers for chi-squared random variables:
$$
\frac{E_1}{d} \xrightarrow{d \to \infty} \frac{1}{2} \quad \text{almost surely.}
$$

Hence the planted contribution to the partition function is:
$$
Z_1 = e^{-E_1} \approx e^{-d/2}.
$$

### The "Bulk" (REM-like) Energies

For $j \neq 1$, we have:
$$
x_t - tx^{(j)} = (1-t)x_0 + t(x_1 - x^{(j)}).
$$

Under the Gaussian proxy, $x_1 - x^{(j)} \sim \mathcal{N}(0, 2\sigma^2 I_d)$ (independent of $x_0$). Therefore:
$$
x_t - tx^{(j)} \sim \mathcal{N}\left(0, s^2(t) I_d\right), \quad s^2(t) := (1-t)^2 + 2\sigma^2 t^2.
$$

The bulk energies become:
$$
E_j(x_t, t) = \frac{\|x_t - tx^{(j)}\|^2}{2(1-t)^2} \approx \frac{s^2(t)}{2(1-t)^2} \cdot \chi_d^2.
$$

Define the **time-dependent energy scale**:
$$
c(t) = \frac{s^2(t)}{2(1-t)^2} = \frac{1}{2}\left(1 + \frac{2\sigma^2 t^2}{(1-t)^2}\right).
$$

**Key observation:** The bulk energies are approximately i.i.d. across $j \geq 2$, each extensive (order $d$). This is the classic setup where **Random Energy Model (REM)** tools apply.

## Collapsed vs Uncollapsed States (Glass vs Liquid)

### Definition of Collapse

The central question is whether the partition function $Z(x_t, t)$ is dominated by:

- **One special term** (the planted state), or
- **Exponentially many comparable terms** (the bulk).

This leads to two phases:

| Phase | Statistical Physics | Flow Matching | Behavior |
|-------|---------------------|---------------|----------|
| **Collapsed** | Glass / Condensed | Memorization | $\lambda_1 \approx 1$ |
| **Uncollapsed** | Liquid | Generalization | Mass spread over many $j$'s |

### Glass Phase (Collapsed / Memorization)

In the collapsed phase, $Z \approx Z_1 = e^{-E_1}$, meaning $\lambda_1 \approx 1$.

The marginal velocity field then reduces to:
$$
u_t(x_t) \approx \frac{x^{(1)} - x_t}{1-t} = \frac{x_1 - x_t}{1-t} = x_1 - x_0.
$$

This is precisely the **conditional velocity field**. The marginal flow aligns perfectly with the teacher trajectory, leading to memorization of training data.

### Liquid Phase (Uncollapsed / Generalization)

In the liquid phase, $Z \approx Z_{\text{bulk}} = \sum_{j=2}^n e^{-E_j}$, with mass distributed across many states.

The velocity field is a genuine mixture of directions toward different training points, enabling interpolation and generalization.

### The Phase Transition

The transition between phases depends on:

- **Energy advantage** of the planted state (order $d$)
- **Entropy** from having $n$ competitors (order $\log n$)

The competition is controlled by the **dimensionless ratio**:
$$
\alpha = \frac{\log n}{d}.
$$

For typical datasets (e.g., CIFAR-10 with $d = 3072$, $n = 50{,}000$):
$$
\alpha = \frac{\log(50{,}000)}{3072} \approx 0.0035 \ll 1.
$$

This tiny $\alpha$ means the entropy budget is negligible compared to extensive energy gaps, placing the system deep in the collapsed regime for most values of $t$.

## Simple Heuristic Analysis

### Typical Energy Gap

Compare a typical bulk energy to the planted energy. The **typical energy gap** is:
$$
E_j - E_1 \approx \frac{d}{2}\left(\frac{s^2(t)}{(1-t)^2} - 1\right) = \frac{d}{2} \cdot \frac{2\sigma^2 t^2}{(1-t)^2} = \frac{d \sigma^2 t^2}{(1-t)^2}.
$$

Therefore, the log-weight ratio is:
$$
\log \frac{\lambda_j}{\lambda_1} \approx -(E_j - E_1) \approx -\frac{d \sigma^2 t^2}{(1-t)^2}.
$$

This shows **exponential-in-$d$ suppression** of bulk weights relative to the planted weight.

### Crude Annealed Entropy Estimate

A naive estimate of the bulk partition function uses the **annealed approximation**:
$$
Z_{\text{bulk}} \approx (n-1) \cdot \mathbb{E}[e^{-E_j}].
$$

However, this is often wrong because $e^{-E_j}$ has exponentially broad fluctuations. The sum is dominated by **rare low-energy states**, not typical ones. This is precisely where REM physics becomes essential.

### Energy vs Entropy Competition

The competition can be summarized as:

- **Planted contribution:** $Z_1 \sim e^{-d/2}$
- **Bulk contribution (naive):** $Z_{\text{bulk}} \sim n \cdot e^{-d \cdot c(t)} = e^{\alpha d - d \cdot c(t)}$

For the planted state to dominate, we need the energy advantage to exceed the entropy:
$$
\frac{d \sigma^2 t^2}{(1-t)^2} \gg \log n.
$$

Rearranging, collapse holds whenever:
$$
\frac{t}{1-t} \gg \sqrt{\frac{\log n}{d \sigma^2}}.
$$

For CIFAR-like numbers, the right-hand side is tiny ($\approx 0.12$), so collapse can occur at small $t$.

### Simple Collapse Time Estimate

Setting the two sides equal gives a heuristic collapse time:
$$
t_C^{\text{heuristic}} \approx \frac{\sqrt{\alpha/\sigma^2}}{1 + \sqrt{\alpha/\sigma^2}}.
$$

For CIFAR-10 ($\alpha \approx 0.0035$, $\sigma^2 \approx 0.24$):
$$
t_C^{\text{heuristic}} \approx 0.11.
$$

**Caveat:** This estimate compares typical bulk energy to planted energy. A rigorous analysis must consider the **best** competitor among $n-1$ states, not a typical one. This requires REM/LDP tools.

## Rigorous REM/LDP Derivation

### Large Deviation Principles: Foundations

#### Why LDP?

The Law of Large Numbers tells us *where* random variables concentrate, but not *how unlikely* deviations are. Large Deviation Theory provides **exponential decay rates** for rare events.

#### Definition

A sequence of random variables $X_d$ satisfies a **Large Deviation Principle (LDP)** with rate function $I(x)$ if:
$$
\mathbb{P}(X_d \approx x) \asymp e^{-d \, I(x)}
$$

where:

- $d$ is the large parameter (dimension)
- $I(x) \geq 0$ with $I(x) = 0$ at the typical value
- Larger $I(x)$ means exponentially rarer events

#### Cramér's Theorem

For i.i.d. random variables $X_1, \dots, X_d$ with finite moment generating function, the empirical average $S_d = \frac{1}{d}\sum_{k=1}^d X_k$ satisfies an LDP:
$$
\mathbb{P}(S_d \approx x) \asymp e^{-d \, I(x)}
$$

where the **rate function** is the Legendre transform of the log-MGF:
$$
I(x) = \sup_{\lambda \in \mathbb{R}} \left( \lambda x - \log \mathbb{E}[e^{\lambda X_1}] \right).
$$

### LDP for Chi-Squared (Energy Density)

#### Deriving the Rate Function

Let $Z_k \sim \mathcal{N}(0,1)$ i.i.d., so $\chi_d^2 = \sum_{k=1}^d Z_k^2$. Define:
$$
Y_d = \frac{\chi_d^2}{d} = \frac{1}{d}\sum_{k=1}^d Z_k^2.
$$

For $X = Z^2$, the log-MGF is:
$$
\log \mathbb{E}[e^{\lambda Z^2}] = -\frac{1}{2}\log(1 - 2\lambda), \quad \lambda < \frac{1}{2}.
$$

Applying Cramér's theorem and computing the Legendre transform:
$$
I_\chi(y) = \sup_{\lambda < 1/2}\left(\lambda y + \frac{1}{2}\log(1 - 2\lambda)\right) = \frac{1}{2}(y - 1 - \log y), \quad y > 0.
$$

**Key properties:**

- Minimum at $y = 1$ with $I_\chi(1) = 0$ (typical value)
- Convex and non-negative
- Deviations are exponentially unlikely in $d$

#### Energy Density LDP

For bulk energies with $E = c(t) \cdot \chi_d^2$, the energy density $e = E/d = c \cdot Y_d$ satisfies:
$$
\mathbb{P}(e \approx \varepsilon) \asymp e^{-d \, I(\varepsilon)}
$$

where:
$$
\boxed{I(\varepsilon) = \frac{1}{2}\left(\frac{\varepsilon}{c} - 1 - \log\frac{\varepsilon}{c}\right)}
$$

This is the **rigorous backbone** for all REM arguments.

### Bulk Free Energy via Laplace Principle

#### Complexity (Entropy of States)

With $n = e^{\alpha d}$ bulk energies, the expected **number of states** with energy density near $\varepsilon$ is:
$$
\#(\varepsilon) \approx n \cdot \mathbb{P}(e \approx \varepsilon) \approx \exp\{d(\alpha - I(\varepsilon))\}.
$$

The quantity:
$$
\Sigma(\varepsilon) = \alpha - I(\varepsilon)
$$

is the **complexity** (entropy density of states at energy $\varepsilon$).

#### Laplace Principle

The bulk partition function is approximately:
$$
Z_{\text{bulk}} = \sum_{j=2}^n e^{-E_j} \approx \int d\varepsilon \, \exp\{d(\Sigma(\varepsilon) - \varepsilon)\}.
$$

By the Laplace principle (large-$d$ saddle point):
$$
\boxed{\Phi(1) = \lim_{d \to \infty} \frac{1}{d}\log Z_{\text{bulk}} = \sup_{\varepsilon > 0}\left[\alpha - I(\varepsilon) - \varepsilon\right]}
$$

### Glass vs Liquid Phase Transition

#### The Variational Problem

Define $F(\varepsilon) = \alpha - I(\varepsilon) - \varepsilon$. The maximizer determines the phase.

#### Liquid Phase (Interior Solution)

If the maximizer is interior, it satisfies $F'(\varepsilon) = 0$, i.e., $I'(\varepsilon) = -1$.

Computing:
$$
I'(\varepsilon) = \frac{1}{2}\left(\frac{1}{c} - \frac{1}{\varepsilon}\right) = -1
$$

yields the **liquid saddle point**:
$$
\boxed{\varepsilon_* = \frac{c}{1 + 2c}}
$$

This solution is valid only if there are exponentially many states there: $\Sigma(\varepsilon_*) = \alpha - I(\varepsilon_*) \geq 0$.

#### Glass Phase (Boundary Solution)

If $\alpha - I(\varepsilon_*) < 0$, there are too few states at the liquid saddle. The supremum hits the boundary where complexity vanishes:
$$
I(\varepsilon_g) = \alpha
$$

The partition sum is then dominated by the **lowest-energy extremes**: this is the glass/condensed phase.

#### Freezing Threshold

The critical entropy level separating phases is:
$$
\boxed{\alpha_c(c) = I(\varepsilon_*) = \frac{1}{2}\left(\log(1 + 2c) - \frac{2c}{1 + 2c}\right)}
$$

**Phase diagram:**

- **Liquid phase:** $\alpha \geq \alpha_c(c)$, maximizer at $\varepsilon_*$, free energy $\Phi(1) = \alpha - I(\varepsilon_*) - \varepsilon_*$
- **Glass phase:** $\alpha < \alpha_c(c)$, maximizer at $\varepsilon_g$ with $I(\varepsilon_g) = \alpha$, free energy $\Phi(1) = -\varepsilon_g$

### Collapse Criterion from Free Energy

#### Full Partition Function

The complete partition function is:
$$
Z = e^{-E_1} + Z_{\text{bulk}}, \quad \lambda_1 = \frac{e^{-E_1}}{Z}.
$$

Since $E_1/d \to 1/2$, we have:
$$
\frac{1}{d}\log\frac{Z_{\text{bulk}}}{e^{-E_1}} \to \Phi(1) + \frac{1}{2}.
$$

#### Collapse Condition

$$
\boxed{\lambda_1 \to 1 \iff \Phi(1) < -\frac{1}{2}}
$$

More precisely, for large $d$:
$$
\lambda_1 \approx \frac{1}{1 + \exp\{d(\Phi(1) + 1/2)\}}.
$$

#### Glass Phase Collapse Criterion

In the glass phase, $\Phi(1) = -\varepsilon_g$, so collapse requires:
$$
\boxed{-\varepsilon_g < -\frac{1}{2} \iff \varepsilon_g > \frac{1}{2}}
$$

where $\varepsilon_g$ is defined implicitly by $I(\varepsilon_g) = \alpha$.

**Interpretation:** Collapse occurs when even the *best* spurious competitor (with energy density $\varepsilon_g$) has higher energy than the planted state (energy density $1/2$).

#### Liquid Phase Collapse Criterion

In the liquid phase, collapse requires:
$$
\alpha - I(\varepsilon_*) - \varepsilon_* < -\frac{1}{2}.
$$

This gives an explicit inequality in $c$ and $\alpha$.

### Explicit Collapse Time

#### Collapse Time Equation

The collapse time $t_C$ is defined by $\varepsilon_g(t_C) = 1/2$. Setting $\varepsilon_g = 1/2$ in $I(\varepsilon_g) = \alpha$:
$$
\alpha = I(1/2) = \frac{1}{2}\left(\frac{1/2}{c} - 1 - \log\frac{1/2}{c}\right) = \frac{1}{2}\left(\frac{1}{2c} - 1 + \log(2c)\right).
$$

This gives the **REM-rigorous collapse boundary**:
$$
\boxed{\alpha = \frac{1}{2}\left(\log(2c(t_C)) + \frac{1}{2c(t_C)} - 1\right)}
$$

with $c(t) = \frac{1}{2}\left(1 + \frac{2\sigma^2 t^2}{(1-t)^2}\right)$.

#### CIFAR-10 Proxy Calculation

For CIFAR-10: $d = 3072$, $n = 50{,}000$, $\sigma^2 = 0.24$.

Compute:
$$
\alpha = \frac{\log(50{,}000)}{3072} \approx 0.003522.
$$

Solving the collapse equation yields:
$$
\boxed{t_C \approx 0.341}
$$

with corresponding $c(t_C) \approx 0.5644$.

**Interpretation:** Under the i.i.d. Gaussian proxy + REM idealization, for $t \lesssim 0.34$, the planted datapoint is no longer guaranteed to dominate because among $n$ training points, one can typically find at least one spurious point whose energy matches or beats the planted energy. For $t \gtrsim 0.34$, collapse occurs and the planted state dominates.

#### Why CIFAR is Always in the Glass Phase

For CIFAR-like $\alpha \ll 1$, we have $\alpha \ll \alpha_c(c(t))$ for all $t \in [0,1)$. The system is **always in the glass phase**, meaning that the bulk partition function is dominated by a few extreme low-energy states.

### Summary of Key Equations

#### Bulk Energy LDP
$$
\mathbb{P}\left(\frac{E}{d} \approx \varepsilon\right) \asymp e^{-d \, I(\varepsilon)}, \quad I(\varepsilon) = \frac{1}{2}\left(\frac{\varepsilon}{c} - 1 - \log\frac{\varepsilon}{c}\right)
$$

#### Bulk Quenched Free Energy
$$
\Phi(1) = \sup_{\varepsilon > 0}\left[\alpha - I(\varepsilon) - \varepsilon\right], \quad \alpha = \frac{\log n}{d}
$$

#### Glass vs Liquid Threshold
$$
\alpha_c(c) = \frac{1}{2}\left(\log(1 + 2c) - \frac{2c}{1 + 2c}\right)
$$

#### Collapse Criterion
$$
\lambda_1 \to 1 \iff \Phi(1) < -\frac{1}{2}
$$

In the glass phase ($\alpha < \alpha_c$):
$$
\lambda_1 \to 1 \iff \varepsilon_g > \frac{1}{2}, \quad I(\varepsilon_g) = \alpha
$$

#### Collapse Time (Glass Phase)
$$
\alpha = \frac{1}{2}\left(\log(2c(t_C)) + \frac{1}{2c(t_C)} - 1\right)
$$

## Extension to Non-Isotropic Covariance

### Motivation

The isotropic Gaussian proxy $x^{(i)} \sim \mathcal{N}(0, \sigma^2 I_d)$ is convenient but unrealistic. Real image datasets have structured covariance with varying eigenvalues (some directions have high variance, others low). The REM/LDP framework extends naturally to non-isotropic covariance. The structure of the theory remains identical — only the **rate function** $I(\varepsilon)$ changes.

### Setup: General Covariance Model

Assume training images follow:
$$
x^{(i)} \sim \mathcal{N}(0, C), \quad i = 1, \dots, n
$$

where $C$ is a $d \times d$ positive definite covariance matrix with eigendecomposition:
$$
C = U \text{diag}(\mu_1, \dots, \mu_d) U^\top.
$$

### Deriving the Effective Covariance

Recall the bulk energy for $j \geq 2$:
$$
E_j = \frac{\|x_t - tx^{(j)}\|^2}{2(1-t)^2}, \quad x_t = (1-t)x_0 + tx_1.
$$

**Step 1: Decompose the displacement**
$$
x_t - tx^{(j)} = (1-t)x_0 + t(x_1 - x^{(j)}).
$$

**Step 2: Identify the distributions**

- $x_0 \sim \mathcal{N}(0, I_d)$ (noise)
- $x_1 - x^{(j)} \sim \mathcal{N}(0, 2C)$ (difference of two i.i.d. samples from $\mathcal{N}(0, C)$)

**Step 3: Compute the covariance of $x_t - tx^{(j)}$**

Since $x_0$ and $(x_1 - x^{(j)})$ are independent:
$$
\text{Cov}(x_t - tx^{(j)}) = (1-t)^2 I_d + 2t^2 C =: \Sigma_t.
$$

**Step 4: Define the effective covariance for the energy**

The energy involves $\|x_t - tx^{(j)}\|^2 / (2(1-t)^2)$. Let $g \sim \mathcal{N}(0, I_d)$ be a standard Gaussian vector. Then:
$$
x_t - tx^{(j)} \stackrel{d}{=} \Sigma_t^{1/2} g
$$
and the energy becomes:
$$
E_j = \frac{g^\top \Sigma_t g}{2(1-t)^2} = \frac{1}{2} g^\top C_t g
$$
where the **effective covariance** is:
$$
C_t = \frac{\Sigma_t}{(1-t)^2} = I_d + \frac{2t^2}{(1-t)^2} C.
$$

### Eigenvalues of the Effective Covariance

If $C$ has eigenvalues $\{\mu_k\}_{k=1}^d$, then $C_t$ has eigenvalues:
$$
\lambda_k^{(t)} = 1 + \frac{2t^2}{(1-t)^2} \mu_k, \quad k = 1, \dots, d.
$$

The bulk energy density is:
$$
\frac{E_j}{d} = \frac{1}{2d} \sum_{k=1}^d \lambda_k^{(t)} Z_k^2
$$
where $Z_k \sim \mathcal{N}(0,1)$ are i.i.d. (the components of $g$ in the eigenbasis of $C_t$).

**Recovery of isotropic case:** When $C = \sigma^2 I_d$, all $\mu_k = \sigma^2$, so:
$$
\lambda_k^{(t)} = 1 + \frac{2\sigma^2 t^2}{(1-t)^2} = 2c(t)
$$
which matches our earlier formula (the factor of 2 accounts for the $1/2$ in $E_j = \frac{1}{2}g^\top C_t g$).

### LDP for Weighted Chi-Squared: Gärtner-Ellis Theorem

The energy density is a **weighted sum of chi-squared variables**:
$$
\frac{E_j}{d} = \frac{1}{2d} \sum_{k=1}^d \lambda_k^{(t)} Z_k^2.
$$

To find its LDP, we use the **Gärtner-Ellis theorem**, which generalizes Cramér's theorem to non-i.i.d. settings.

**Step 1: Compute the scaled cumulant generating function**

For a single term $\lambda Z^2$ with $Z \sim \mathcal{N}(0,1)$:
$$
\mathbb{E}[e^{\theta \lambda Z^2}] = \frac{1}{\sqrt{1 - 2\theta\lambda}}, \quad \theta < \frac{1}{2\lambda}.
$$
Thus:
$$
\log \mathbb{E}[e^{\theta \lambda Z^2}] = -\frac{1}{2}\log(1 - 2\theta\lambda).
$$

**Step 2: Sum over all eigenvalues**

The scaled log-MGF for the energy density $e = E_j/d$ is:
$$
\Lambda_t(\theta) = \lim_{d \to \infty} \frac{1}{d} \log \mathbb{E}\left[e^{d\theta \cdot \frac{1}{2d}\sum_k \lambda_k^{(t)} Z_k^2}\right] = \lim_{d \to \infty} \frac{1}{d} \sum_{k=1}^d \left(-\frac{1}{2}\log\left(1 - \theta\lambda_k^{(t)}\right)\right).
$$

This limit exists if the **empirical spectral distribution** of $C_t$ converges:
$$
\Lambda_t(\theta) = \int \left(-\frac{1}{2}\log(1 - \theta\lambda)\right) d\nu_t(\lambda)
$$
where $\nu_t$ is the limiting spectral measure of $\{lambda_k^{(t)}\}$.

**Step 3: Legendre transform gives the rate function**

By the Gärtner-Ellis theorem:
$$
\boxed{I_t(\varepsilon) = \sup_{\theta < 1/\lambda_{\max}^{(t)}} \left(\theta\varepsilon - \Lambda_t(\theta)\right)}
$$

This is the **drop-in replacement** for the isotropic rate function.

### Verifying the Isotropic Case

For $C = \sigma^2 I_d$, all eigenvalues are equal: $\lambda_k^{(t)} = \lambda^{(t)} = 1 + \frac{2\sigma^2 t^2}{(1-t)^2}$.

Then:
$$
\Lambda_t(\theta) = -\frac{1}{2}\log(1 - \theta\lambda^{(t)}).
$$

The Legendre transform:
$$
I_t(\varepsilon) = \sup_\theta \left(\theta\varepsilon + \frac{1}{2}\log(1 - \theta\lambda^{(t)})\right).
$$

Setting the derivative to zero:
$$
\varepsilon - \frac{\lambda^{(t)}/2}{1 - \theta\lambda^{(t)}} = 0 \quad \Rightarrow \quad \theta^* = \frac{1}{\lambda^{(t)}} - \frac{1}{2\varepsilon}.
$$

Substituting back and simplifying:
$$
I_t(\varepsilon) = \frac{1}{2}\left(\frac{2\varepsilon}{\lambda^{(t)}} - 1 - \log\frac{2\varepsilon}{\lambda^{(t)}}\right).
$$

With $c(t) = \lambda^{(t)}/2$, this becomes:
$$
I_t(\varepsilon) = \frac{1}{2}\left(\frac{\varepsilon}{c(t)} - 1 - \log\frac{\varepsilon}{c(t)}\right)
$$
which matches our earlier isotropic formula exactly.

### The Full Pipeline with General Covariance

The REM framework carries through unchanged:

1. **Bulk free energy:**
$$
\Phi(1,t) = \sup_{\varepsilon > 0}\left[\alpha - I_t(\varepsilon) - \varepsilon\right]
$$

2. **Collapse criterion:**
$$
\lambda_1 \to 1 \iff \Phi(1,t) < -\frac{1}{2}
$$

3. **Planted weight approximation:**
$$
\lambda_1(t) \approx \frac{1}{1 + \exp\{d(\Phi(1,t) + 1/2)\}}
$$

Only $I_t(\varepsilon)$ changes — the rest of the theory is identical.

### Practical Implementation

**To compute $t_C$ and $\lambda_1(t)$ for real data:**

1. **Estimate the covariance spectrum:** Compute the eigenvalues $\{\mu_k\}$ of the sample covariance of your training data.

2. **Compute the effective spectrum at each $t$:**
$$
\lambda_k^{(t)} = 1 + \frac{2t^2}{(1-t)^2} \mu_k
$$

3. **Evaluate the scaled log-MGF numerically:**
$$
\Lambda_t(\theta) = \frac{1}{d}\sum_{k=1}^d \left(-\frac{1}{2}\log(1 - \theta\lambda_k^{(t)})\right)
$$

4. **Compute the rate function via numerical Legendre transform:**
$$
I_t(\varepsilon) = \sup_{\theta} \left(\theta\varepsilon - \Lambda_t(\theta)\right)
$$

5. **Solve the variational problem:**
$$
\Phi(1,t) = \sup_\varepsilon\left[\alpha - I_t(\varepsilon) - \varepsilon\right]
$$

6. **Find $t_C$:** Solve $\Phi(1,t_C) = -1/2$.

# Error Analysis: Training Difficulties Across Time

As observed in @fig-article-fig-3, the learned velocity field $u^\theta_t$ exhibits a characteristic error profile when approximating the marginal velocity field $u_t$: low error near $t = 0$, a peak around $t \approx 0.1$, a decrease for intermediate $t$, and a final spike near $t = 1$. This section explains these four regimes using the irreducible variance framework and the REM collapse analysis developed above.

## The Irreducible Variance and Its Decay

Recall from the Setup that the CFM loss decomposes as:
$$
\mathcal{L}_{\text{CFM}}(\theta) = \mathcal{L}_{\text{FM}}(\theta) + \mathbb{E}_{t, X_t}\left[\mathrm{Var}(X_1 - X_0 \mid X_t)\right].
$$

The irreducible variance $\mathrm{Var}(X_1 - X_0 \mid X_t)$ captures the inherent stochasticity in the training targets: given a fixed interpolant $X_t$, different $(X_0, X_1)$ pairs can produce the same $X_t$, leading to different targets $X_1 - X_0$.

**Key property:** This variance **decreases monotonically as $t$ increases**:

- For $t \approx 0$: $X_t \approx X_0$ (mostly noise), so $X_t$ reveals almost nothing about $X_1$. The target $X_1 - X_0 \approx X_1$ has high variance across samples.
- For $t \approx 1$: $X_t \approx X_1$ (mostly signal), so the target $X_1 - X_0$ is nearly determined by $X_t$. The variance vanishes.

This decay of irreducible variance suggests that learning should become easier as $t$ increases — yet the observed error profile is more complex, with a peak at intermediate times. The explanation lies in the interplay between variance and the complexity of the optimal predictor.

## Training Error Trajectory: Four Regimes

### Regime 1: $t \approx 0$ — Easy Despite High Variance

Despite the high irreducible variance at small $t$, the training error is **low** near $t = 0$. This apparent paradox resolves when we examine what the neural network must predict.

For small $t$, the interpolant is dominated by noise: $X_t = (1-t)X_0 + tX_1 \approx X_0$. The marginal velocity field at $X_t$ is:
$$
u_t(X_t) = \frac{\mathbb{E}[X_1 \mid X_t] - X_t}{1-t}.
$$

When $t \approx 0$, the interpolant $X_t \approx X_0$ carries almost no information about $X_1$. Under the empirical distribution, the posterior mean satisfies:
$$
\mathbb{E}[X_1 \mid X_t] \approx \bar{x} \approx 0
$$
where $\bar{x}$ is the mean of the training data (approximately zero after normalization).

Therefore, the optimal velocity field simplifies to:
$$
u_t(X_t) \approx \frac{0 - X_t}{1-t} \approx \frac{-X_0}{1} = -X_0.
$$

This is essentially a **negation map**: the neural network only needs to learn $u^\theta_t(x) \approx -x$, which is trivial. The high variance in targets $X_1 - X_0$ averages out to a simple function of $X_t$ alone.

### Regime 2: $t \approx t_C$ — Peak Difficulty (Collapse Transition)

The error peaks around $t \approx 0.1$--$0.2$, which corresponds to the approach toward the **collapse transition** at $t_C \approx 0.34$ (for CIFAR-10).

In this regime, the posterior $p(X_1 \mid X_t)$ is **neither uniform nor concentrated**:

- The interpolant $X_t$ now contains some information about $X_1$, so the posterior is no longer uniform over training points.
- But collapse has not yet occurred, so multiple training points still have significant posterior weight.

This creates several sources of difficulty:

1. **Complex posterior structure:** The neural network must learn to predict $\mathbb{E}[X_1 \mid X_t]$, which is a weighted average over multiple training points. The weights $\lambda_i(X_t, t)$ vary rapidly with $X_t$.

2. **Signal-to-noise transition:** The interpolant $X_t = (1-t)X_0 + tX_1$ is transitioning from noise-dominated to signal-dominated. The network must extract $X_1$ from $X_t$, but $X_t$ is still far from $X_1$.

3. **Sharp phase transition:** The REM analysis shows that the energy gaps scale as:
$$
E_j - E_1 \approx \frac{d \sigma^2 t^2}{(1-t)^2}.
$$
Near $t_C$, the system transitions from having exponentially many competing states to a single dominant state. This transition is sharp in high dimensions, creating rapid variation in the velocity field as a function of both $x$ and $t$.

4. **High sensitivity:** Near the transition, the map $x \mapsto \arg\max_i \lambda_i(x,t)$ is extremely sensitive to perturbations. Small changes in $X_t$ can cause different training points to dominate the posterior.

### Regime 3: $t > t_C$ — Decreasing Error (Concentration)

After the collapse transition ($t > t_C \approx 0.34$), the error **decreases**. The posterior concentrates on the planted index with $\lambda_1(X_t, t) \approx 1$.

In this regime:
$$
u_t(X_t) \approx \frac{x^{(1)} - X_t}{1-t} = \frac{X_1 - X_t}{1-t} = X_1 - X_0.
$$

The marginal velocity field becomes essentially equal to the **conditional velocity field**. The neural network's task simplifies: given $X_t$, it must predict $X_1 - X_0$, but now $X_t$ strongly determines which $X_1$ generated it.

The learning problem has two favorable properties:

1. **Low posterior uncertainty:** With $\lambda_1 \approx 1$, there is little ambiguity about which training point generated $X_t$.

2. **Increasing signal:** As $t$ grows, $X_t$ becomes closer to $X_1$, making it easier to infer the target direction $X_1 - X_0$.

### Regime 4: $t \to 1$ — Final Blow-Up

Near $t = 1$, the error spikes again due to two compounding effects: **numerical instabilities** and **Voronoi discontinuities**.

#### Numerical Instabilities

The marginal velocity field involves the ratio:
$$
u_t(x) = \sum_{i=1}^n \lambda_i(x,t) \frac{x^{(i)} - x}{1-t}.
$$

As $t \to 1$:

- The denominator $(1-t)$ approaches zero.
- The softmax weights $\lambda_i$ involve energies $E_i = \frac{\|x - tx^{(i)}\|^2}{2(1-t)^2}$ that blow up.

While these singularities cancel at the interpolant (see the Technical Note below), computing the marginal velocity field numerically becomes unstable. Small floating-point errors are amplified by the division by $(1-t)$.

#### Voronoi Discontinuities (Nearest-Neighbor Regime)

As $t \to 1$, the softmax temperature $(1-t)^2 \to 0$ drives the weights toward a **one-hot distribution**:
$$
\lambda_i(x,t) \xrightarrow{t \to 1} \mathbf{1}\left[i = \arg\min_j \|x - x^{(j)}\|\right].
$$

The marginal velocity field approaches:
$$
u_t(x) \approx \frac{x^{(i^*)} - x}{1-t}, \quad \text{where } i^* = \arg\min_j \|x - x^{(j)}\|.
$$

This is a **piecewise-constant direction field** that switches abruptly across **Voronoi cell boundaries** in $\mathbb{R}^d$. Such discontinuous fields are notoriously difficult to approximate with smooth neural networks, explaining the error spike near $t \approx 1$.

## Summary: Error Profile Across Time

| Regime | Time | Error | Explanation |
|--------|------|-------|-------------|
| 1 | $t \approx 0$ | Low | Posterior mean $\approx 0$; target is $\approx -X_0$ (trivial) |
| 2 | $t \approx t_C$ | **Peak** | Complex posterior; phase transition; high sensitivity |
| 3 | $t > t_C$ | Decreasing | Posterior concentrates; clear signal |
| 4 | $t \to 1$ | **Spike** | Numerical instabilities; Voronoi discontinuities |

## Technical Note: Why Targets Remain Bounded

An apparent paradox: the CFM loss trains with targets $X_1 - X_0$, which are bounded and constant across time. Yet the marginal velocity field contains $\frac{1}{1-t}$ factors that blow up. How is this consistent?

### Resolution: Cancellation at the Interpolant

The key is that we evaluate the velocity field at the **interpolant** $X_t = (1-t)X_0 + tX_1$, where singularities cancel.

For the planted index $i = 1$ (where $x^{(1)} = X_1$):
$$
x^{(1)} - X_t = X_1 - (1-t)X_0 - tX_1 = (1-t)(X_1 - X_0).
$$

Therefore:
$$
\frac{x^{(1)} - X_t}{1-t} = X_1 - X_0.
$$

The $(1-t)$ factor **cancels**. In the collapsed regime ($\lambda_1 \approx 1$):
$$
u_t(X_t) \approx X_1 - X_0,
$$
which is bounded.

For non-planted indices $i \neq 1$, the individual terms $\frac{x^{(i)} - X_t}{1-t}$ do blow up as $t \to 1$. However, the corresponding weights $\lambda_i \to 0$ exponentially fast due to softmax collapse, keeping the weighted sum bounded.

The neural network's difficulty is **not** unbounded targets, but rather **sharp spatial discontinuities** near Voronoi boundaries and **rapid temporal variation** near the collapse transition.



