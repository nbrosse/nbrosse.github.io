[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas’ Notebook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\nDescription\n\n\n\n\n\n\nFeb 6, 2025\n\n\nPredicting MHC-Peptide Binding with Machine Learning\n\n\ndeep learning, biology\n\n\nUsing machine learning to predict peptide binding affinity to Major Histocompatibility Complex (MHC) molecules.\n\n\n\n\nApr 24, 2024\n\n\nEncoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT\n\n\ndeep learning, biology\n\n\nA comparative analysis of Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT) for encoding distances in 3D molecular representations, using the Uni-Mol framework.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mhc/mhc.html",
    "href": "posts/mhc/mhc.html",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "",
    "text": "This blog post explores the application of machine learning techniques to predict the binding affinity between peptides and Major Histocompatibility Complex (MHC) molecules. In Section 1, we introduce the biological aspects of MHC. In Section 2, we then present the deep learning approach and the results of MHC antigen presentation."
  },
  {
    "objectID": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "href": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "MHC Class I and Class II",
    "text": "MHC Class I and Class II\nMHC molecules are divided into two main classes: MHC class I and MHC class II.\nMHC Class I\nMHC class I molecules are present on most cells (except red blood cells). They present antigens from inside the cell. When a cell is infected or becomes cancerous, proteins within the cell are broken down into smaller fragments called epitopes. These epitopes are loaded onto MHC class I molecules and displayed on the cell surface. Killer T cells (cytotoxic T lymphocytes or CTLs) recognize these epitopes and can destroy infected or cancerous cells.\n\n\n\n\n\n\nFigure 1: MHC class I pathway: Proteins in the cytosol are degraded by the proteasome, liberating peptides internalized by TAP channel in the endoplasmic reticulum, there associating with MHC-I molecules freshly synthesized. MHC-I/peptide complexes enter Golgi apparatus, are glycosylated, enter secretory vesicles, fuse with the cell membrane, and externalize on the cell membrane interacting with T lymphocytes. This figure shows the detailed steps of how MHC Class I molecules get loaded. Source\n\n\n\nIn humans, the main MHC class I molecules are HLA-A, HLA-B, and HLA-C (HLA stands for Human Leukocyte Antigen).\nMHC Class II\nMHC class II molecules are primarily found on specialized immune cells called antigen-presenting cells (APCs) like macrophages, dendritic cells, and B cells. They present antigens from outside the cell. APCs engulf foreign invaders and break them down into epitopes in a process called phagocytosis. These epitopes are loaded onto MHC class II molecules and displayed on the cell surface. Helper T cells recognize these epitopes and activate other immune cells to fight the infection. These Helper T cells have receptors that specifically bind to MHC Class II molecules. If a helper T cell recognizes a foreign epitope presented by MHC class II, it becomes activated and starts to coordinate the immune response. It releases chemical signals (cytokines) that help other immune cells, like B cells and killer T cells, to fight off the infection.\n\n\n\n\n\n\nFigure 2: This diagram shows how MHC class I molecules present antigens from inside the cell to cytotoxic T cells (CD8+), leading to the destruction of infected cells. Notice how the antigen is processed inside the cell and then presented on the cell surface. On the contrary, MHC class II molecules present antigens from outside the cell to helper T cells (CD4+), which then activate other immune cells. Notice how this differs from MHC class I, which presents antigens from inside the cell.(Antunes et al. 2018)\n\n\n\n\n\n\n\n\n\nFigure 3: Molecular structures of class I and class II MHCs. Molecular representation of a class I MHC (A, C) and a class II MHC (B, D). The upper panel shows a top view, while the bottom panel shows a cross section side-view of the binding clefts. Note that the binding cleft of a class I receptor is deeper, with “closed” extremities, while the class II cleft is shallower, with open extremities. The pockets involved in binding primary “anchor” residues are indicated. Together, structural differences in the shape of the cleft and the location of binding pockets have an impact on the overall conformation of bound ligands (e.g., peptides tend to adopt bulged conformations when bound to class I, and more linear conformations when bound to class II). (Antunes et al. 2018)\n\n\n\nIn humans, the main MHC class II molecules are HLA-DP, HLA-DQ, and HLA-DR.\nHuman Leukocyte Antigens (HLA)\nIn humans, MHC molecules are called Human Leukocyte Antigens (HLAs). The genes for HLA proteins are located within the MHC region on chromosome 6.\nHLAs are important for:\n\nOrgan transplantation: HLA matching is crucial to prevent organ rejection.\nAutoimmune diseases: Certain HLA types are associated with increased risk of autoimmune diseases.\nDrug responses: HLA variations can influence how individuals respond to certain medications.\nEvolutionary advantage: HLA diversity is important for population survival against various pathogens.\n\n\n\n\n\n\n\nFigure 4: Codominant expression of HLA genes. Each person inherits HLA genes from both parents, resulting in the expression of multiple HLA types. This increases the diversity of antigens that can be presented. Source\n\n\n\nMHC Diversity\nThe MHC is highly diverse, with many different versions (alleles) of each MHC gene. This diversity is essential because different MHC molecules bind to different peptides. A diverse population is more likely to have individuals who can present antigens from new pathogens, ensuring better survival chances for the species."
  },
  {
    "objectID": "posts/mhc/mhc.html#datasets",
    "href": "posts/mhc/mhc.html#datasets",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Datasets",
    "text": "Datasets\nThe IPD-IMGT/HLA Database is a specialized resource focusing on the sequences of the human major histocompatibility complex (MHC) or human leukocyte antigen (HLA) system. It provides comprehensive information about HLA alleles, including their sequences, nomenclature, and associated metadata. This database is crucial for researchers in immunology, transplantation, and vaccine development, as accurate HLA typing is essential for understanding immune responses and predicting transplant compatibility. The IEDB (Immune Epitope Database) is a widely used resource for curated experimental data on immune epitopes. It catalogs epitopes recognized by T cells and B cells in various diseases and conditions. The IEDB facilitates research in epitope discovery, vaccine design, and understanding immune recognition, offering tools and data for analyzing and predicting immune responses."
  },
  {
    "objectID": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "href": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Relevance to Machine Learning",
    "text": "Relevance to Machine Learning\nThe interaction between an MHC molecule and a peptide is highly dependent on the amino acid sequence of the peptide and the specific type of MHC molecule. This sequence-structure-function relationship makes MHC-peptide binding prediction a suitable problem for machine learning. Experimental data on MHC-peptide binding affinities, though sometimes sparse, is available for training and evaluating predictive models. The high polymorphism of MHC genes, with numerous variants (alleles) existing in the population, adds complexity and motivates the development of specific prediction models.\nKey MHC Concepts:\n\nMHC Class I and Class II: The two main classes of MHC molecules.\nEpitope: The specific part of the peptide recognized by the T cell receptor.\nPolymorphism: The existence of multiple versions (alleles) of MHC genes within a population. Different MHC alleles bind to different sets of peptides.\nBinding Affinity: The strength of the interaction between an MHC molecule and a peptide. Often measured experimentally, it serves as the target variable for many machine learning models.\n\nMachine Learning Approaches\nMHC-peptide binding prediction aims to develop models that accurately predict the binding affinity between a given peptide sequence and a specific MHC allele. This can be framed as a regression or classification task.\n\nInput: Peptide sequence, MHC allele (represented as a sequence or encoding).\nOutput: Binding affinity (e.g., IC50 value, Kd value) or a binary label (binder/non-binder).\n\nFeature engineering and model selection are crucial for building effective predictors. Common approaches include:\n\nSequence-based Features: Amino acid composition, n-grams, physicochemical properties.\nStructure-based Features: (If available) Information about the 3D structure of the MHC-peptide complex.\nMHC Allele Encoding: Techniques such as one-hot encoding, amino acid embeddings, or other methods to represent the MHC allele sequence.\nMachine Learning Algorithms: Linear regression, Support Vector Machines (SVMs), Random Forests, Neural Networks (including Convolutional Neural Networks and Transformers)."
  },
  {
    "objectID": "posts/mhc/mhc.html#project-overview",
    "href": "posts/mhc/mhc.html#project-overview",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Project Overview",
    "text": "Project Overview\n\n\n\n\n\n\nNote\n\n\n\nThe code is available at https://github.com/nbrosse/mhcpred.\n\n\nThe goal of this project is to build a machine learning classifier that predicts whether a given peptide will be presented by a specific MHC class I protein, identified by its allele name. The data used for this project is derived from the training and evaluation data of NetMHCPan4.1 (Reynisson et al. 2020), a well-established framework for MHC binding prediction. The data is split into training and testing sets, with the training data further divided into five folds for cross-validation.\nThe dataset contains a binary target variable, “hit” (1 if the peptide is presented by the MHC, 0 otherwise), and two features:\n\n“peptide”: The amino acid sequence of the peptide. These short chains of amino acids are potential antigens that could be presented to the immune system.\n“allele”: The name of the MHC class I allele. MHC molecules are highly polymorphic, meaning there are many different versions (alleles) within the human population. Each allele has a slightly different binding groove, affecting which peptides it can bind and present. You can find details on the naming convention here (nomenclature).\n\n\n\n\n\n\n\nNote\n\n\n\nPredicting MHC antigen presentation is a complex field. This project provides a simplified introduction to the problem. For a more in-depth understanding, we recommend exploring NetMHCPan (Reynisson et al. 2020) and MHCflurry (O’Donnell, Rubinsteyn, and Laserson 2020) and the references cited within those publications. Note that the specific data used in this project is derived from NetMHCPan4.1 but must remain private.\n\n\nWe begin with Exploratory Data Analysis (EDA) to understand the characteristics of our data.\n\nEDA\n\n# Import data loading functions\nfrom mhcpred.data import get_train_data, get_test_data\n\n# Load training and test data\ndf_train = get_train_data()\ndf_test = get_test_data()\n\n\n# View first few rows of training data\ndf_train.head()\n\n\n\n\n\n\n\n\npeptide\nallele\nhit\nfold\n\n\n\n\n0\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n0\n\n\n1\nKESKINQVF\nHLA-B*44:02\nTrue\n0\n\n\n2\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n0\n\n\n3\nRTIADSLINSF\nHLA-B*57:03\nTrue\n0\n\n\n4\nEEKTIIKKL\nHLA-B*44:03\nTrue\n0\n\n\n\n\n\n\n\n\n# Get allele counts in training data\ndf_train[[\"allele\"]].value_counts()\n\nallele     \nHLA-A*02:01    265252\nHLA-B*07:02    201038\nHLA-B*57:01    184773\nHLA-A*29:02    181136\nHLA-B*40:02    145817\n                ...  \nHLA-A*69:01        12\nHLA-A*02:06         6\nHLA-A*26:02         6\nHLA-A*26:03         6\nHLA-A*25:01         6\nName: count, Length: 130, dtype: int64\n\n\n\ndf_test[\"allele\"].value_counts()\n\nallele\nHLA-A*02:02    77053\nHLA-A*02:06    54510\nHLA-A*02:11    48445\nHLA-B*53:01    46991\nHLA-B*15:17    45917\nHLA-A*02:05    45136\nHLA-B*15:03    44968\nHLA-A*33:01    43333\nHLA-A*66:01    41538\nHLA-C*12:03    36448\nHLA-C*03:03    35568\nHLA-A*11:01    33424\nHLA-A*30:02    33180\nHLA-C*08:02    32416\nHLA-A*23:01    30467\nHLA-A*32:01    28036\nHLA-B*40:02    23768\nHLA-B*14:02    21601\nHLA-B*37:01    20048\nHLA-B*40:01    18908\nHLA-B*45:01    18750\nHLA-B*18:01    18284\nHLA-B*58:01    17946\nHLA-B*15:02    16702\nHLA-B*15:01    16624\nHLA-A*30:01    15837\nHLA-C*07:02    15293\nHLA-B*46:01    14015\nHLA-B*38:01     9509\nHLA-B*35:03     8275\nHLA-A*26:01     7730\nHLA-C*05:01     7033\nHLA-A*25:01     6906\nHLA-A*68:01     5648\nHLA-B*08:01     3365\nHLA-B*07:02     2469\nName: count, dtype: int64\n\n\n\n# Get positive samples per allele in training\ndf_train.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*01:01     7156\nHLA-A*01:03        7\nHLA-A*02:01    13025\nHLA-A*02:03     1873\nHLA-A*02:04     3155\n               ...  \nHLA-C*12:04        3\nHLA-C*14:02     2441\nHLA-C*15:02     1873\nHLA-C*16:01     2970\nHLA-C*17:01      602\nName: hit, Length: 130, dtype: int64\n\n\n\ndf_train.hit.sum()\n\n197547\n\n\n\nlen(df_train)\n\n3679405\n\n\n\n# ~5.37% positive rate\ndf_train.hit.sum() / len(df_train)\n\n0.05368993084479692\n\n\n\ndf_test.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*02:02    3063\nHLA-A*02:05    2016\nHLA-A*02:06    1975\nHLA-A*02:11    2035\nHLA-A*11:01    2309\nHLA-A*23:01    1697\nHLA-A*25:01     396\nHLA-A*26:01     555\nHLA-A*30:01     892\nHLA-A*30:02    2415\nHLA-A*32:01    1436\nHLA-A*33:01    2138\nHLA-A*66:01    1988\nHLA-A*68:01     433\nHLA-B*07:02     159\nHLA-B*08:01     180\nHLA-B*14:02    1056\nHLA-B*15:01     769\nHLA-B*15:02     637\nHLA-B*15:03    1953\nHLA-B*15:17    1712\nHLA-B*18:01     784\nHLA-B*35:03     330\nHLA-B*37:01    1253\nHLA-B*38:01     619\nHLA-B*40:01    1268\nHLA-B*40:02    1333\nHLA-B*45:01     760\nHLA-B*46:01     575\nHLA-B*53:01    2016\nHLA-B*58:01     866\nHLA-C*03:03    2003\nHLA-C*05:01     383\nHLA-C*07:02     593\nHLA-C*08:02    1546\nHLA-C*12:03    1273\nName: hit, dtype: int64\n\n\n\ndf_test.hit.sum() / len(df_test)\n\n0.04800130213150049\n\n\n\n# Find alleles only in test set\nset(df_test.allele.unique()) - set(df_train.allele.unique())\n\n{'HLA-A*02:02', 'HLA-A*02:11', 'HLA-A*33:01', 'HLA-B*53:01'}\n\n\n\nDataset Class Imbalance\n\nTraining Set:\n\nTotal samples: 3,679,405\nPositive rate: 5.37%\n\nTest Set:\n\nTotal samples: 453,934\nPositive rate: 4.8%\n\n\nAllele Distribution\n\nMost frequent: HLA-A*02:01 (265,252 samples)\nLeast frequent: Multiple alleles with only 6 samples\nDistribution: Highly imbalanced across alleles\n\nTest-Only Alleles\n\nHLA-A*02:02\nHLA-A*02:11\nHLA-A*33:01\nHLA-B*53:01\n\n\nSource: EDA\n\nUsing MHCflurry Pretrained Models for Prediction\nWe leverage the mhcflurry package to build our classifier. MHCflurry is a tool specifically designed for MHC binding affinity prediction. See also the associated paper (O’Donnell, Rubinsteyn, and Laserson 2020). MHCflurry is a software package focused on predicting how strongly peptides bind to MHC class I molecules. It’s based on machine learning models trained on a large dataset of experimentally measured peptide-MHC binding affinities. The current version uses neural networks trained with a mix of binding affinity and mass spectrometry data (ligand presentation).\nWe use the Binding Affinity pretrained model from mhcflurry to predict the binding affinity of peptides to MHC class I molecules using Class1AffinityPredictor. The following code assumes you have installed mhcflurry and downloaded the required pretrained models.\nmhcflurry-downloads fetch models_class1_presentation\npython scripts/mhcflurry_benchmark.py\ndef predict_with_mhcflurry() -&gt; pd.DataFrame:\n    predictor = Class1AffinityPredictor.load()\n    df_test = get_test_data()\n    mhcflurry_predictions = predictor.predict_to_dataframe(\n        peptides=df_test.peptide.values,\n        alleles=df_test.allele.values,\n        allele=None,\n    )\n    df = pd.merge(df_test, mhcflurry_predictions, on=[\"allele\", \"peptide\"], how=\"left\")\n    df.to_csv(str(output_path / \"mhcflurry_predictions.csv\"), index=False)\n    return df\nThe output is of the form:\n\n\n\nTable 1: MHCflurry pretrained model predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npeptide\nhit\nallele\nprediction\nprediction_low\nprediction_high\nprediction_percentile\n\n\n\n\nAAPATRAAL\nTrue\nHLA-B*35:03\n94.297\n59.902\n144.624\n0.205\n\n\nAAPSAAREL\nTrue\nHLA-B*35:03\n116.19\n79.847\n169.241\n0.262\n\n\nAEISQIHQSVTD\nTrue\nHLA-B*35:03\n26103.26\n22695.389\n28415\n15.739\n\n\nALEEQLQQIRAE\nTrue\nHLA-B*35:03\n24797.131\n19988.967\n28062.65\n13.571\n\n\nAQDPLLLQM\nTrue\nHLA-B*35:03\n2164.336\n745.888\n5390.727\n1.413\n\n\nASAPPGPPA\nTrue\nHLA-B*35:03\n1398.729\n387.675\n3293.692\n1.157\n\n\nDAHKGVAL\nTrue\nHLA-B*35:03\n84.315\n54.736\n133.899\n0.175\n\n\nDNPIQTVSL\nTrue\nHLA-B*35:03\n1386.767\n565.122\n3667.21\n1.151\n\n\nDPEAFLVQI\nTrue\nHLA-B*35:03\n245.485\n133.986\n394.752\n0.484\n\n\n\n\n\n\nThe first 3 columns come from the test dataset.\n\npeptide: The amino acid sequence of the peptide being evaluated.\nhit: The ground truth, a boolean value indicating whether the peptide is known to be presented by the given MHC allele (True) or not (False).\nallele: The name of the MHC class I allele being considered.\n\nThe following columns are added by the binding affinity predictions:\n\nprediction: The raw prediction score from the MHCflurry model. Higher values generally indicate a stronger predicted binding affinity. These values are not directly interpretable in isolation.\nprediction_low/prediction_high: These represent the lower and upper bounds of a 95% confidence interval around the prediction value. They provide an estimate of the uncertainty associated with the prediction.\nprediction_percentile: This is the most useful column for interpreting the results. It represents the percentile rank of the prediction score compared to a background distribution of scores for random peptides. A lower percentile indicates a stronger predicted binding affinity. For example, a percentile of 1.0 means that the predicted score is in the top 1% of all possible scores.\n\nThe uncertainty estimation comes from the ensemble of neural networks used for the prediction. A percentile threshold (e.g., 2%) is commonly used to determine whether a peptide is likely to bind (lower is better).\n\n\nPrediction: Fitting a Class1BinaryNeuralNetwork\nWe now fit a Class1BinaryNeuralNetwork on the training dataset. The code is available at https://github.com/nbrosse/mhcpred.\nHere’s a glimpse of the training data structure:\n\n\n\nTable 2: Training data.\n\n\n\n\n\npeptide\nallele\nhit\n\n\n\n\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n\n\nKESKINQVF\nHLA-B*44:02\nTrue\n\n\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n\n\nRTIADSLINSF\nHLA-B*57:03\nTrue\n\n\n\n\n\n\nChallenges arise in encoding the peptide and allele sequences for use in a neural network. Peptides have variable lengths, and alleles are represented by their names.\nThe MHCFlurry package provides a mapping between alleles and their corresponding MHC molecule sequences within the allele_sequences.csv file. This mapping is crucial for encoding the alleles.\n\n\n\nTable 3: Allele sequences.\n\n\n\n\n\nAllele\nSequence\n\n\n\n\nHLA-A*01:01\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:02\nYSAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:03\nYFAMYQENMAHTDANTLYGIMYDRDYTWVARVYRGYA\n\n\nHLA-A*01:04\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYX\n\n\nHLA-A*01:06\nYFAMYQENMAHTDANTLYGIIYDRDYTWVALAYRGYA\n\n\n\n\n\n\nFirst, we import necessary libraries. We also import components from our own mhcpred library, which contains the neural network architecture and data loading functions.\nimport pickle\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport numpy as np\nimport pandas as pd\nfrom mhcflurry.allele_encoding import AlleleEncoding\nfrom mhcflurry.encodable_sequences import EncodableSequences\nfrom sklearn.model_selection import train_test_split\n\nfrom mhcpred.class1_binary_nn import Class1BinaryNeuralNetwork\nfrom mhcpred.config import settings\nfrom mhcpred.data import get_test_data, get_train_data\nfrom mhcpred.hyperparameters import base_hyperparameters\nWe load the allele sequences, training data, and test data using helper functions. The allele sequences are crucial for encoding the MHC alleles.\nallele_sequences = pd.read_csv(\n    str(data_path / \"allele_sequences.csv\"), index_col=0\n).iloc[:, 0]\n\ndf_total_train = get_train_data()\ndf_test = get_test_data()\nWe determine the alleles present in our data and filter the loaded allele sequences to only include those we’ll be using.\nalleles_in_use = set(df_total_train.allele).union(set(df_test.allele))\nallele_sequences_in_use = allele_sequences[allele_sequences.index.isin(alleles_in_use)]\nThe AlleleEncoding class is designed to cache encodings for a sequence of alleles. It maps allele names to integer indices and sequences, allowing consistent use of these mappings, especially as inputs to neural networks. The EncodableSequences class is used to encode variable-length peptides into fixed-size numerical matrices. It caches various encodings of a list of sequences and provides methods to encode these sequences into fixed-length categorical or vector representations.\nWe also split the training data into training and validation sets using train_test_split from sklearn. Stratified splitting ensures the class balance is maintained across the training and validation sets. The validation data is also preprocessed by encoding the peptides and alleles.\nallele_encoding = AlleleEncoding(\n    alleles=allele_sequences_in_use.index.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\ndf_train, df_val = train_test_split(\n    df_total_train, test_size=0.1, shuffle=True, stratify=df_total_train.hit.values\n)\n\nval_peptides = EncodableSequences(df_val.peptide.values)\nval_alleles = AlleleEncoding(\n    alleles=df_val.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\nAlleleEncoding provides a robust and efficient way to manage and encode allele sequences. It handles the complexities of mapping allele names to indices, storing and padding sequences, and providing different encoding options. The AlleleEncoding class manages allele sequences efficiently:\n\nAllele Universe vs. Used Alleles: The class distinguishes between two sets of alleles:\n\n\nAllele Universe: The complete set of alleles the system knows about. This is defined by the allele_to_sequence dictionary, mapping allele names to their amino acid sequences.\nUsed Alleles: The specific set of alleles used in a particular analysis or task. This is provided as a list when creating an AlleleEncoding instance.\n\n\nallele_to_index Mapping: A dictionary (self.allele_to_index) is created to map each allele in the universe to a unique integer index. This includes a special index for None values, often used as padding. This mapping ensures consistency: the same allele always gets the same index.\nSequence Storage (self.sequences): The amino acid sequences for all alleles in the universe are stored in a Pandas Series (self.sequences). Critically, these sequences are padded to the same length using “X” characters. This padding is essential for creating fixed-length numerical representations, which many machine learning models require.\nBorrowing (borrow_from): The borrow_from parameter allows you to create a new AlleleEncoding instance that inherits the allele universe and mappings from an existing instance. This is a powerful way to ensure consistency across different parts of your code. You don’t have to redefine the allele_to_sequence mapping every time.\nEncoding: The class provides methods to encode the allele sequences into numerical matrices, suitable for machine learning.\n\n\nallele_representations(encoding_name): Encodes the entire allele universe. This is useful for pre-calculating encodings for all known alleles.\nfixed_length_vector_encoded_sequences(encoding_name): Encodes the used alleles (the subset provided when the object was initialized). This uses the pre-calculated encodings from allele_representations and selects only the encodings for the alleles in self.alleles, in the correct order. This gives you a matrix where each row represents an allele sequence.\n\n\nEncoding Methods (encoding_name): The type of encoding can be specified using the encoding_name parameter. Common options include “BLOSUM62” (a substitution matrix) and “one-hot” encoding.\n\nBLOSUM62 (Blocks Substitution Matrix) is a widely used substitution matrix in bioinformatics. It represents the likelihood of one amino acid being substituted for another during evolution. The matrix assigns a score to each pair of amino acids, reflecting their similarity. Higher scores indicate a higher probability of substitution (or that the two amino acids are more similar). Negative scores indicate substitutions that are less likely or even unfavorable.\nThe AlleleEncoding class uses BLOSUM62 to convert amino acid sequences into numerical representations. Each amino acid in the sequence is replaced by a vector of 21 numbers (20 amino acids + the “X” character). Each of these 21 numbers is the BLOSUM62 score between the amino acid in the sequence and the amino acid represented by the position in the 21-element vector.\n\nAmino Acid Indexing: First, each amino acid is converted to an index. There’s a mapping from amino acid letter to index.\nBLOSUM62 Lookup: For each amino acid in the sequence, the code looks up its corresponding row in the BLOSUM62 matrix. This row represents the similarity scores between that amino acid and all other amino acids (and ‘X’).\nVector Representation: The row from the BLOSUM62 matrix becomes the vector representation of that amino acid. So, “M” would be represented by a vector of 21 numbers (the scores of M with every other amino acid and X), and “A” would also be represented by its own 21-number vector.\nSequence Encoding: The encoded sequence becomes a matrix. If the original sequence was of length n, the encoded sequence is now an n x 21 matrix.\n\nThe train_data_iterator function is a generator that yields batches of training data. This function also handles filtering of alleles that might be present in the training data but not in the allele_sequences data to handle potential data inconsistencies.\ndef train_data_iterator(\n    df_train: pd.DataFrame,\n    train_allele_encoding: AlleleEncoding,\n    batch_size: int = 1024,\n) -&gt; Iterator[tuple[AlleleEncoding, EncodableSequences, np.ndarray]]:\n    \"\"\"\n    This function creates a data generator for training the neural network.\n    It iterates over the training data in batches and yields tuples of \n    (allele_encoding, peptide_sequences, labels).  It also handles filtering\n    of alleles not found in the initial allele encoding.\n    \"\"\"\n    # Get unique alleles in the training set.\n    alleles = df_train.allele.unique()\n    # Filter alleles to keep only those for which sequences are available.\n    usable_alleles = [\n        c for c in alleles if c in train_allele_encoding.allele_to_sequence\n    ]\n    print(\"Using %d / %d alleles\" % (len(usable_alleles), len(alleles)))\n    print(\n        \"Skipped alleles: \",\n        [c for c in alleles if c not in train_allele_encoding.allele_to_sequence],\n    )\n    df_train = df_train.query(\"allele in @usable_alleles\")\n\n    # Calculate the number of batches.\n    n_splits = np.ceil(len(df_train) / batch_size)\n\n    # Infinite loop to allow for multiple epochs.\n    while True:\n        # Split the training data into batches.\n        epoch_dfs = np.array_split(df_train.copy(), n_splits)\n        for k, df in enumerate(epoch_dfs):\n            if len(df) == 0:\n                continue\n            # Encode peptides and alleles for the current batch.\n            encodable_peptides = EncodableSequences(df.peptide.values)\n            allele_encoding = AlleleEncoding(\n                alleles=df.allele.values,\n                borrow_from=train_allele_encoding,  # Reuse encoding from main allele_encoding\n            )\n            # Yield the encoded data and labels (hit column).\n            yield (allele_encoding, encodable_peptides, df.hit.values)\nThe neural network model is initialized using the Class1BinaryNeuralNetwork class. Base hyperparameters are used for initialization. The model is then trained using the fit_generator method. This method takes the training data generator, validation data, and other parameters like the number of epochs and steps per epoch. The steps_per_epoch is calculated based on the training data size and batch size.\nmodel = Class1BinaryNeuralNetwork(**base_hyperparameters)\nsteps_per_epoch = np.ceil(len(df_train) / batch_size)\nbatch_size = 1024  # Define batch_size here\n\ntrain_generator = train_data_iterator(df_train, allele_encoding, batch_size) #create the generator\n\nmodel.fit_generator(\n    generator=train_generator,\n    validation_peptide_encoding=val_peptides,\n    validation_affinities=df_val.hit.values,\n    validation_allele_encoding=val_alleles,\n    validation_inequalities=None,\n    validation_output_indices=None,\n    steps_per_epoch=steps_per_epoch,\n    epochs=2,\n)\nThe Class1BinaryNeuralNetwork neural network takes two inputs:\n\nAllele: A single input representing the MHC allele.\nPeptide: A sequence of 45 amino acids represented as a 21-dimensional vector for each amino acid (likely using BLOSUM62 encoding or a similar technique).\n\nThe network then processes these inputs through several layers:\n\nEmbedding Layer: The allele input is passed through an embedding layer. This layer learns a 777-dimensional vector representation for each allele, capturing its key characteristics relevant to peptide binding.\nFlatten Layers: These layers reshape the input data. The peptide input, which is initially a 45x21 matrix, is flattened into a 945-element vector. Similarly, the 1x777 allele embedding is flattened into a 777-element vector. This prepares the data for the subsequent dense layers.\nConcatenate Layer: The flattened representations of the peptide and allele are combined into a single 1722-element vector. This crucial step merges the information from both inputs, allowing the network to learn the combined effect of allele and peptide on binding affinity.\nDense Layers: These are fully connected layers. The first dense layer transforms the 1722-element vector into a 1024-element vector, and the second further reduces it to 512 elements. These layers learn complex non-linear relationships between the combined allele and peptide representation, extracting features crucial for predicting binding affinity.\nDropout Layers: Dropout is a regularization technique. During training, these layers randomly “drop out” (ignore) a fraction of neurons. This prevents the network from overfitting to the training data and improves its ability to generalize to unseen data.\nOutput Layer: The final dense layer has a single output neuron. This neuron outputs a single value, representing the predicted binding affinity between the given peptide and MHC allele. Since we’re predicting a binary “hit” variable, a sigmoid activation function is used in this layer to output a probability between 0 and 1.\n\nThis architecture is designed to effectively learn the complex patterns governing MHC-peptide binding.\nFinally, the trained model is used to make predictions on the test data. The test data is preprocessed in the same way as the training data, and the predict method of the model is used to generate predictions. These predictions are then added to the test dataframe and saved to a CSV file.\ntest_peptides = df_test.peptide.values\ntest_allele_encoding = AlleleEncoding(\n    alleles=df_test.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\npredictions = model.predict(\n    peptides=test_peptides,\n    allele_encoding=test_allele_encoding,\n)\n\ndf_test[\"predictions\"] = predictions\ndf_test.to_csv(str(output_path / \"mhcpred_predictions.csv\"), index=False)\nWe evaluate the predictions of the two methods (mhcflurry and mhcpred) using standard binary classification metrics.\n\n\nMetrics\nThis notebook contains training metrics history and classification metrics computed on the predictions by - mhcflurry (benchmark) - mhcpred\n\nfrom pathlib import Path\nimport pickle\n\nfrom mhcpred.config import settings\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nmodels_path = Path(settings.models_path)\noutput_path = Path(settings.output_path)\n\nInformation on the training history\nI prefer to use tensorboard, but it is not implemented in the mhcflurry package. The information is quite scarce, but when you execute the code, you have the loss for each step and not only for the whole epoch. Of course, it is a very basic version of logging and should be improved.\n\nwith open(str(models_path / \"model.pickle\"), \"rb\") as f:\n    model = pickle.load(f)\n\n\nmodel.fit_info\n\n[{'learning_rate': 0.0010000000474974513,\n  'loss': [0.09700655937194824, 0.06465369462966919],\n  'val_loss': [0.06880103051662445, 0.05075661838054657],\n  'time': 524.7155420780182,\n  'num_points': 6628048}]\n\n\nBinary classification metrics\nWe compute the usual binary classification metrics on the unbalanced test dataset: accuracy, balanced accuracy, confusion matrix and classification report by scikit-learn.\nWe report the unbalanced accuracy because the dataset is very unbalanced so the accuracy only is not a good measure of accuracy (the model can predict always False and it works quite well).\nmhcflurry metrics\n\nmhcflurry_rank_percentile_threshold = 2  # rank threshold for positive hits\n# It comes from the mhcflurry article.\n\n\ndf = pd.read_csv(str(output_path / \"mhcflurry_predictions.csv\"))\ny_pred = df.prediction_percentile.values &lt;= mhcflurry_rank_percentile_threshold\ny_true = df.hit.values\nacc = accuracy_score(y_true=y_true, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=y_true, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.99    900996\n        True       0.67      0.86      0.76     45423\n\n    accuracy                           0.97    946419\n   macro avg       0.83      0.92      0.87    946419\nweighted avg       0.98      0.97      0.97    946419\n\n\n\n\nacc, balanced_acc\n\n(0.9731936911663861, 0.9217833819652606)\n\n\nThe metrics are quite good. We note that we do not have a good precision on the True class (0.67), the model has a tendency to predict True too often, so we have too many False Positives. We see it on the confusion matrix, 19234 False Positives.\nmhcpred metrics\n\nmhcpred_proba_threshold = 0.5  # by default, but we try to tune it later\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9731657332258088, 0.7775326900487307)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.98      0.99      0.99    900725\n        True       0.82      0.56      0.67     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.90      0.78      0.83    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nmhcpred has worse performances compared to mhcflurry, see the balanced accuracy. On the True class, in that case, the recall is not good (0.56), the model has a tendency to predict False too often, on the confusion matrix we have 20000 True Negatives. It indicates that if we lower the threshold, we may improve the model.\nThreshold tuning\nWe plot the precision recall curve to try to identify a better threshold.\n\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n\nprecision, recall, thresholds = precision_recall_curve(y_true=y_true, probas_pred=df.predictions.values)\ndisp = PrecisionRecallDisplay(precision=precision, recall=recall)\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprecision_recall_thresholds = pd.DataFrame({\n    \"precision\": precision[:-1],\n    \"recall\": recall[:-1],\n    \"thresholds\": thresholds,\n})\n\n\nprecision_recall_thresholds\n\n\n\n\n\n\n\n\nprecision\nrecall\nthresholds\n\n\n\n\n0\n0.048001\n1.000000\n0.000114\n\n\n1\n0.048001\n1.000000\n0.000116\n\n\n2\n0.048001\n1.000000\n0.000117\n\n\n3\n0.048001\n1.000000\n0.000125\n\n\n4\n0.048002\n1.000000\n0.000125\n\n\n...\n...\n...\n...\n\n\n889313\n1.000000\n0.000110\n0.992152\n\n\n889314\n1.000000\n0.000088\n0.992280\n\n\n889315\n1.000000\n0.000066\n0.992347\n\n\n889316\n1.000000\n0.000044\n0.992431\n\n\n889317\n1.000000\n0.000022\n0.992971\n\n\n\n\n889318 rows × 3 columns\n\n\n\nA threshold of approx. 0.2 seems to be a good compromise for precision/recall.\n\nmhcpred_proba_threshold = 0.2\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9697360118629252, 0.8462451280426622)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98    900725\n        True       0.68      0.71      0.69     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.83      0.85      0.84    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nWe see that we have improved the balanced accuracy. We have a deterioration of the precision but a better recall.\nSource: Metrics"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why I Write\nWelcome to my blog, a personal space where I share notes on my work and explorations in science, machine learning, and the broader tech landscape. These posts serve as a journal for my thoughts and discoveries, but I hope they also provide value and inspiration to anyone interested in these dynamic fields. Whether you’re a fellow researcher, a tech enthusiast, or simply curious, I invite you to explore and engage with the ideas I present here. Feel free to reach out if you’d like to connect or collaborate on exciting projects !\nShort Bio\nI am a Data Scientist with expertise in applied mathematics and artificial intelligence. My professional experience spans diverse domains, including signal processing and anomaly detection at Thales, as well as drug discovery at Iktos. Throughout my career, I have been deeply involved in research and development, driven by a passion for innovation and the challenge of addressing complex problems."
  },
  {
    "objectID": "posts/encoding-distances/unimol-gbf.html",
    "href": "posts/encoding-distances/unimol-gbf.html",
    "title": "Encoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT",
    "section": "",
    "text": "This blog post compares two different methods for encoding distances in 3D molecules and protein pockets: Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT). We analyze their performance within the Uni-Mol framework, a universal 3D molecular representation learning model. We observed unstable gradients with GBFPT and hypothesized that DCEPT, inspired by AlphaFold’s distance representation, might offer a more stable alternative. We found that while DCEPT exhibits more stable training behavior, GBFPT ultimately yields superior pocket embeddings for retrieval tasks.\nBefore diving into the details, let’s define the acronyms used in this article:\n\nGBFPT: Gaussian Basis Function with Pair Type\nDCEPT: Discretization Categorical Embedding with Pair Type\n\n\nIntroduction to Uni-Mol and Distance Encoding\nCode for Uni-Mol is available at https://github.com/dptech-corp/Uni-Mol, and the related article is (Zhou et al. 2023). In brief, Uni-Mol is a 3D foundation model for molecules and pockets based on a SE(3) Transformer architecture. It comprises two pretrained models: one for molecular conformations and another for protein pocket data. Uni-Mol is pretrained on large-scale unlabeled data and is able to directly take 3D positions as both inputs and outputs. Uni-Mol backbone is a Transformer based model that can capture the input 3D information and predict 3D positions directly. Uni-Mol pretraining is done on two large-scale datasets: a 209M molecular conformation dataset and a 3M candidate protein pocket dataset, for pretraining 2 models on molecules and protein pockets, respectively. In the pretraining phase, Uni-Mol has to predict masked atoms, as well as masked noisy atoms coordinates and distances for effectively learning 3D spatial representation. The overall pretraining architecture is illustrated in Figure 2 and the framework is given in Figure 1 (taken from (Zhou et al. 2023)).\n\n\n\n\n\n\nFigure 1: Schematic illustration of the Uni-Mol framework\n\n\n\n\n\n\n\n\n\nFigure 2: Left: the overall pretraining architecture. Middle: the model inputs, including atom representation and pair representation. Right: details in the model block.\n\n\n\nBackground: 3D Spatial Encoding in Uni-Mol\nWe focus here on the encoding of the coordinates in distances (pair representation in Figure 2 middle part) and the decoding part, prediction of distances (pair-dist head in Figure 2 left part). In (Zhou et al. 2023) Section D.1, 3D spatial positional encodings benchmark, they investigate the performance of different 3D spatial positional encoding on the 3D molecular pretraining. In particular, they benchmarked:\n\nGaussian kernel (GK), a simply Gaussian density function.\nGaussian kernel with pair type (GKPT) (Shuaibi et al. 2021). Based on GK, an affine transformation according to the pair type is applied on pair distances, before applying the Gaussian kernel.\nRadial Bessel basis (RBB) (Gasteiger, Yeshwanth, and Günnemann 2021). A Bessel based radial function.\nDiscretization categorical embedding (DCE). They convert the continued distances to the discrete bins, by Discretization. With binned distances, embedding-based positional encoding is directly used.\nDelta coordinate (DC) (Zhao et al. 2021). Following Point Transformer, the deltas of coordinates are directly used as pair-wise spatial relative positional encoding.\nGaussian kernel with pair type and local graph (GKPTLG). Based on GKPT, they set up a model with locally connected graphs. In particular, the cutoff radius is set to 6 Å.\n\nThe validation loss during pretraining for each encoding is summarized in Figure 3 (taken from (Zhou et al. 2023)). From the results, they drew the following conclusions:\n\nThe performance of DCE and GK are almost the same, and outperform RBB and DC. And they choose GK as the basic encoding.\nCompared with GK, GKPT converges faster. This indicates the pair type is critical in the 3D spatial positional encoding.\nCompared with GKPT, GKPTLG converges slower. This indicates the locally cutoff graph is not effective for self-supervised learning, and the default fully connected graph structure inherent in the Transformer architecture is more effective.\nAs GKPT outperforms all other encoding, they use it in the backbone model of Uni-Mol.\n\n\n\n\n\n\n\nFigure 3: Validation loss in pretraining for different 3D spatial encodings\n\n\n\nThe code for the GKPT encoding is given by:\nimport torch\nimport torch.nn as nn\n\n@torch.jit.script\ndef gaussian(x, mean, std):\n    pi = 3.14159\n    a = (2 * pi) ** 0.5\n    return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std)\n\n\nclass GaussianLayer(nn.Module):\n    def __init__(self, K=128, edge_types=1024):\n        super().__init__()\n        self.K = K\n        self.means = nn.Embedding(1, K)\n        self.stds = nn.Embedding(1, K)\n        self.mul = nn.Embedding(edge_types, 1)\n        self.bias = nn.Embedding(edge_types, 1)\n        nn.init.uniform_(self.means.weight, 0, 3)\n        nn.init.uniform_(self.stds.weight, 0, 3)\n        nn.init.constant_(self.bias.weight, 0)\n        nn.init.constant_(self.mul.weight, 1)\n\n    def forward(self, x, edge_type):\n        mul = self.mul(edge_type).type_as(x)\n        bias = self.bias(edge_type).type_as(x)\n        x = mul * x.unsqueeze(-1) + bias\n        x = x.expand(-1, -1, -1, self.K)\n        mean = self.means.weight.float().view(-1)\n        std = self.stds.weight.float().view(-1).abs() + 1e-5\n        return gaussian(x.float(), mean, std).type_as(self.means.weight)\nK represents the number of Gaussian basis functions, edge_types the number of possible edge types, x the distance matrix (for an initial 3D molecule or pocket) and edge_type the corresponding edge type matrix. Edge types represent the different types of atom pairs (e.g., C-C, C-O, C-N, etc.)\n\n\nExperimental Setup\n\n\n\n\n\n\nNote\n\n\n\nAll the Uni-Mol experiments run for this article are based on a small pockets dataset inspired from the PDBbind database (http://www.pdbbind.org.cn/), a collection of protein-ligand complexes and their binding affinities. The dataset was split into training and validation sets based on pocket similarity. The wandb project is available https://wandb.ai/nicolasb/unimol_analysis/ as well as a summary report https://api.wandb.ai/links/nicolasb/kdz59bry.\n\n\nMotivation for DCEPT and Implementation\nWhen we train Uni-Mol on a small dataset of pockets inspired from the PDBbind database, we remark that the gradients related to GaussianLayer parameters are not stable and can take very large values. To address the gradient instability observed with GBFPT, and drawing inspiration from AlphaFold’s use of discretization, we explored using a Discretization Categorical Embedding with Pair Type (DCEPT) as an alternative. In Figure 4, some gradients are of the order of one thousand. Uni-Mol relies on Uni-Core which implements gradient clipping and these high values do not affect the stability of the training.\n\n\n\n\n\n\nFigure 4: Gradients of GaussianLayer parameters (GBFPT encoding)\n\n\n\nNevertheless, we wanted to try another encoding that would be naturally stable without exploding gradients. Furthermore, DCE is the encoding used in AlphaFold (Jumper et al. 2021). We implemented Discretization categorical embedding with Pair Type encoding (DCEPT) that takes into account the edge type. A distogram is a discrete representation of the distance matrix, where distances are binned into predefined intervals. The binning process transforms continuous distances into discrete categories.\nimport torch\nimport torch.nn as nn\n\n# Constants\nPAD_DIST = 0\n\nclass NonLinearModule(nn.Module):\n    def __init__(self, input_dim, out_dim, activation_fn):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, out_dim)\n        self.activation = getattr(nn, activation_fn)()\n\n    def forward(self, x):\n        return self.activation(self.linear(x))\n\nclass DistEncoding(nn.Module):\n    def __init__(\n        self,\n        distogram_nb_bins: int,\n        nb_edge_types: int,\n        embedding_dim: int,\n        edge_type_padding_idx: int,\n        encoder_attention_heads: int,\n        activation_fn: str,\n    ):\n        \"\"\"\n        Initializes the DistEncoding module for encoding distances and edge types.\n\n        Args:\n            distogram_nb_bins: Number of bins for the distogram (distance discretization).\n            nb_edge_types: Number of possible edge types (e.g., different bond types).\n            embedding_dim: Dimension of the embeddings for distances and edge types.\n            edge_type_padding_idx: Padding index for edge type embeddings.\n            encoder_attention_heads: Number of attention heads in the Transformer encoder.\n            activation_fn: Activation function to use in the projection layer.\n        \"\"\"\n        super(DistEncoding, self).__init__()\n\n        # Embedding layer for the distogram (discretized distances)\n        self.dist_embedding = nn.Embedding(\n            num_embeddings=distogram_nb_bins,\n            embedding_dim=embedding_dim,\n            padding_idx=PAD_DIST,  # Use PAD_DIST for padding\n        )\n\n        # Embedding layer for edge types\n        self.edge_type_embedding = nn.Embedding(\n            num_embeddings=nb_edge_types,\n            embedding_dim=embedding_dim,\n            padding_idx=edge_type_padding_idx,\n        )\n\n        # Projection layer to combine distance and edge type embeddings and project\n        # to the correct dimension for attention bias.\n        self.projection = NonLinearModule(\n            input_dim=2 * embedding_dim,  # Concatenate dist and edge embeddings\n            out_dim=encoder_attention_heads,  # Output dimension matches attention heads\n            activation_fn=activation_fn,\n        )\n\n    def forward(\n        self, distogram: torch.Tensor, edge_types: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the DistEncoding module.\n\n        Args:\n            distogram: Tensor of discretized distances (batch_size, seq_len, seq_len).\n            edge_types: Tensor of edge types (batch_size, seq_len, seq_len).\n\n        Returns:\n            attn_bias: Tensor of attention biases (batch_size, num_heads, seq_len, seq_len).\n        \"\"\"\n        n_node = distogram.size(-1)  # Sequence length (number of nodes/atoms)\n\n        # Embed the discretized distances\n        dist_embeddings = self.dist_embedding(distogram)  # (B, L, L, D)\n\n        # Embed the edge types\n        edge_types_embeddings = self.edge_type_embedding(edge_types)  # (B, L, L, D)\n\n        # Concatenate distance and edge type embeddings\n        embeddings = torch.cat((dist_embeddings, edge_types_embeddings), dim=-1)  # (B, L, L, 2D)\n\n        # Project the combined embeddings to generate attention bias\n        attn_bias = self.projection(embeddings)  # (B, L, L, H) where H = num_heads\n\n        # Reshape the attention bias to the correct format for the Transformer\n        attn_bias = attn_bias.permute(0, 3, 1, 2).contiguous()  # (B, H, L, L)\n        attn_bias = attn_bias.view(-1, n_node, n_node) # (B*H, L, L) - Correct if attention is applied per head.\n        return attn_bias\ndistogram_nb_bins is the number of bins (128 by default), nb_edge_types the number of edge types, embedding_dim the dimension of the embedding (128 by default), encoder_attention_heads the number of attention heads in the transformer because the distance encoding is directly injected in the attention matrix. distogram is the distogram (discretization of the distance matrix) and edge_types the edge types. We concatenate the two embeddings creating de facto the DCEPT and then project to feed into the attention matrix.\nTraining Dynamics\nDuring the training, the gradients related to DistEncoding parameters do not take large values and are naturally stable without clipping gradients. This is illustrated in Figure 5.\n\n\n\n\n\n\nFigure 5: Gradients of DistEncoding parameters (DCEPT encoding)\n\n\n\nFollowing (Jumper et al. 2021) distogram prediction task, we also replace the distance prediction task (mean squared error loss) implemented in Uni-Mol by a distogram prediction task (cross entropy loss). We remark that this loss replacement does not change the characteristics of Uni-Mol.\nOn the small pockets inspired from the PDBbind database, we notice that the training and validation loss curves are better with DCEPT encoding compared to GBFPT encoding (on average), see Figure 6. More precisely, the masked_token_loss and the masked_acc metrics related to the recovery of masked atoms seem to stagnate a little at first with DCEPT encoding compared to GBFPT encoding. It may be due to the fact that DCEPT are at first completely random embeddings and less intuitive for the neural network. However, the masked_coord_loss is better with DCEPT encoding both in the training and validation sets. Note that the masked_distogram_loss corresponds to the distogram loss (cross entropy loss) used in AlphaFold (Jumper et al. 2021) and is implemented only for DCEPT encoding. For DCEPT encoding, we also add a distance prediction head with the corresponding MSE loss taken from Uni-Mol and a small multiplication factor (0.01). The rationale for including this head was to maintain some of the original Uni-Mol distance prediction capabilities alongside the distogram prediction. That explains why the DCEPT masked_dist_loss decreases slightly slower than GBFPT. Several additional experiments (not shown here) demonstrate that using a distogram or distance loss does not change the behavior of Uni-Mol.\nIn conclusion, according to the loss curves and the stability of the gradients, DCEPT seems to be a better encoding than GBFPT (or at least as good as) during pre-training.\n\n\n\n\n\n\nFigure 6: Uni-Mol training and validation losses with GBFPT and DCEPT encoding\n\n\n\n\n\nDownstream Performance: Pocket Retrieval\nHowever, Uni-Mol stands as a foundational model pre-trained through unsupervised methods. The pre-training metrics do not reflect the expected capabilities of the model. Notably, we expect that the pockets embeddings obtained with Uni-Mol should be good proxies of the pockets themselves: if two pockets are close to each other, their embeddings should be close in cos similarity or euclidean distance.\nWe have collected a dataset of 5 pockets (taken from 2oax, 3oxc, 5kxi, 5zk3 and 6v7a proteins) and for each pocket, a group of similar and dissimilar pockets. We compute the cos similarities between each reference pocket and the similar/dissimilar pockets and we sort the pockets by their cos similarity. Better embeddings translate into more similar pockets in the top retrieved pockets. More precisely, we sort the pockets by their cos similarities, we select the top 100 pockets, we count the number of similar pockets in the top 100 and we get a number between 0 and 1, the higher the better. We test two different embeddings: either, the vector corresponding to the [CLS] token (see (Zhou et al. 2023) Section 2.2) (indicated by _cls) or the mean of the pocket atoms vectors (indicated by _mean). Table 1 summarizes the results for each encoding, embedding and reference pocket and we remark that\n\nGBFPT is superior to DCEPT for pockets retrieval,\nThe mean of the pocket atoms vectors is better or near as good as the [CLS] embedding.\n\n\n\n\nTable 1: Uni-Mol pockets retrieval with GBFPT or DCEPT encoding (higher is better)\n\n\n\n\n\n\n6v7a\n2oax\n5kxi\n5zk3\n3oxc\n\n\n\n\nunimol_gbfpt_cls\n0.46\n1.0\n0.32\n0.3\n1.0\n\n\nunimol_gbfpt_mean\n0.79\n1.0\n0.38\n0.29\n1.0\n\n\nunimol_dcept_cls\n0.3\n1.0\n0.26\n0.29\n1.0\n\n\nunimol_dcept_mean\n0.51\n1.0\n0.27\n0.28\n1.0\n\n\n\n\n\n\nInvestigating Noise Sensitivity\nIn conclusion, despite better pre-training behavior and metrics, DCEPT encoding is disappointing when it comes to embeddings comparison. We suppose that this defect comes from a higher sensitivity of the discretization procedure. Two distance matrices from two close pockets may be more distinctly differentiated with DCEPT encoding compared to GBFPT encoding. This could be due to the information loss inherent in discretizing continuous distances. GBFPT might be capturing subtle long-range interactions that DCEPT, with its discrete representation, misses. To test this hypothesis, we take the 6v7a pocket and we noise its coordinates with a uniform noise between 0 and 1A. Since we have a batch size of 16, we fill up a batch with the reference pocket 6v7a and 15 noisy pockets. For each pocket, the distance matrix is encoded by GBFPT or DCEPT and we get an encoding of size 128 for each distance in the distance matrix. We compute the cos similarities between each encoding and the reference encoding in the reference matrix distance of 6v7a and we obtain the overall statistics of these cos similarities for GBFPT and DCEPT. In Table 2, we have the absolute errors statistics between 1 and the cos similarities of the noisy pockets from 6v7a, the lower the better because the vectors are similar. We remark that as presumed DCEPT encoding is less robust to noise compared to GBFPT encoding.\n\n\n\nTable 2: Statistics of cos similarities errors for noisy pockets from 6v7a\n\n\n\n\n\n\n\n\n\n\n\nAbsolute errors cos similarities GBFPT encoding (1 - cos similarities)\nAbsolute errors cos similarities DCEPT encoding (1 - cos similarities)\n\n\n\n\nmean\n0.002\n0.020\n\n\nmedian\n0.000\n0.007\n\n\n\n\n\n\n\n\nConclusion\nThese results suggest that while DCEPT offers advantages during pre-training in terms of gradient stability, the discretization process might lead to a loss of information and reduced robustness, ultimately hindering the quality of the generated pocket embeddings for downstream tasks like pocket retrieval. Further research is needed to investigate the optimal bin size for distograms and to explore alternative distance encoding techniques that balance stability and information retention. This analysis was conducted on a small, custom-built pocket dataset, and future work should evaluate these encoding methods on larger, more diverse datasets to ensure the generalizability of our findings.\n\n\n\n\n\nReferences\n\nGasteiger, Johannes, Chandan Yeshwanth, and Stephan Günnemann. 2021. “Directional Message Passing on Molecular Graphs via Synthetic Coordinates.” In Advances in Neural Information Processing Systems, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:15421–33. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2021/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nShuaibi, Muhammed, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi, and C. Lawrence Zitnick. 2021. “Rotation Invariant Graph Neural Networks Using Spin Convolutions.” https://arxiv.org/abs/2106.09575.\n\n\nZhao, Hengshuang, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. 2021. “Point Transformer.” In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 16239–48. https://doi.org/10.1109/ICCV48922.2021.01595.\n\n\nZhou, Gengmo, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. “Uni-Mol: A Universal 3D Molecular Representation Learning Framework.” In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=6K2RM6wVqKu."
  }
]