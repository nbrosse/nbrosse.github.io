[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas‚Äô Notebook",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\n\nTitle\n\n\n\nCategories\n\n\n\nDescription\n\n\n\n\n\n\n\n\nJan 18, 2026\n\n\nAnalyzing Raman Spectra with Machine Learning\n\n\nmachine learning, chemistry\n\n\nAnalyzing Raman spectra with machine learning approaches.\n\n\n\n\n\n\nAug 15, 2025\n\n\nLLM-Powered Slide Decks: A Comparison of Formats\n\n\nLLM\n\n\nA comparison of formats for LLM-powered slide decks.\n\n\n\n\n\n\nJun 30, 2025\n\n\nLLM Observability with Langfuse and a FastHTML Chatbot\n\n\nLLM, python\n\n\nIntegrating Langfuse for LLM observability in a FastHTML chatbot application.\n\n\n\n\n\n\nMar 20, 2025\n\n\nPDF RAG: Enhanced PDF Processing\n\n\ndeep learning, LLM, RAG\n\n\nPDF RAG: Exploring techniques for processing PDF documents with Language Models, including raw conversion, reformatting, structure parsing and RAG.\n\n\n\n\n\n\nFeb 26, 2025\n\n\nPython Concurrency\n\n\npython\n\n\nAn introduction to Python concurrency, covering core concepts, practical examples, and considerations for its use.\n\n\n\n\n\n\nFeb 18, 2025\n\n\nPDF Parsing for LLM Input\n\n\ndeep learning, LLM\n\n\nAn exploration of current PDF parsing capabilities for Large Language Model (LLM) input.\n\n\n\n\n\n\nFeb 6, 2025\n\n\nPredicting MHC-Peptide Binding with Machine Learning\n\n\ndeep learning, biology\n\n\nUsing machine learning to predict peptide binding affinity to Major Histocompatibility Complex (MHC) molecules.\n\n\n\n\n\n\nApr 24, 2024\n\n\nEncoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT\n\n\ndeep learning, biology\n\n\nA comparative analysis of Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT) for encoding distances in 3D molecular representations, using the Uni-Mol framework.\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html",
    "href": "posts/llm-observability/llm-observability.html",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "",
    "text": "As Large Language Models (LLMs) become increasingly integrated into real-world applications, understanding and monitoring their behavior is essential. LLM observability‚Äîthe practice of tracing, logging, and analyzing model interactions‚Äîenables developers to diagnose issues, optimize performance, and ensure responsible AI usage.\nThis blog post presents a proof of concept for implementing LLM observability using Langfuse, an open-source observability platform, together with a responsive chatbot built with FastHTML. The goal is to showcase practical techniques for capturing and visualizing LLM interactions, providing valuable insights for building more reliable and transparent AI systems. I demonstrate how to integrate Langfuse into a FastHTML chatbot application for enhanced observability. The associated code is available on llm-observability.\nThe post is structured as follows:"
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#why-not-use-streamlit",
    "href": "posts/llm-observability/llm-observability.html#why-not-use-streamlit",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Why not use Streamlit ?",
    "text": "Why not use Streamlit ?\nMy initial thought was to utilize Streamlit, a popular Python library known for its ease of use in creating web applications rapidly. Streamlit is indeed very accessible for quick prototyping. However, Streamlit often relies on a full-page reload or significant re-rendering of components upon user interaction. For a dynamic, conversational interface like a chatbot, this behavior seems unnatural and can lead to a clunky user experience.\nBelow in Listing¬†1, we provide the official example of a Streamlit chatbot application.\n\n\n\nListing¬†1: Example of a Streamlit chatbot application\n\n\nimport streamlit as st\n\nst.title(\"Echo Bot\")\n\n# Initialize chat history\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []\n\n# Display chat messages from history on app rerun\nfor message in st.session_state.messages:\n    with st.chat_message(message[\"role\"]):\n        st.markdown(message[\"content\"])\n\n# React to user input\nif prompt := st.chat_input(\"What is up?\"):\n    # Display user message in chat message container\n    st.chat_message(\"user\").markdown(prompt)\n    # Add user message to chat history\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    response = f\"Echo: {prompt}\"\n    # Display assistant response in chat message container\n    with st.chat_message(\"assistant\"):\n        st.markdown(response)\n    # Add assistant response to chat history\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n\n\n\nStreamlit re-runs the entire Python script from top to bottom every time the user interacts with the app (e.g., submits a message, clicks a button) or when the app reloads. To keep data between these reruns, Streamlit uses st.session_state. Without it, variables would reset on each rerun. Which means that at each reload, the entire chat history is re-rendered from the st.session_state.messages list. This can lead to a less responsive user experience, especially in a chatbot context where users expect real-time interaction. It feels counterintuitive to have to manage the chat history in a session state variable, which is not a natural way to handle dynamic content updates."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#why-fasthtml",
    "href": "posts/llm-observability/llm-observability.html#why-fasthtml",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Why FastHTML?",
    "text": "Why FastHTML?\nConsequently, I shifted my focus to FastHTML. FastHTML is a newer approach/API developed by Answer AI designed for building dynamic web interfaces with potentially more granular control over updates, avoiding full-page reloads. I found a pre-existing, fully-functional chatbot example built with FastHTML. This example provided with additional extensions is a great starting point for building a responsive chatbot application. It uses FastHTML‚Äôs capabilities to create a chat interface that updates dynamically without reloading the entire page, providing a more fluid user experience."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#building-a-chatbot-with-fasthtml-google-gemini-and-langfuse",
    "href": "posts/llm-observability/llm-observability.html#building-a-chatbot-with-fasthtml-google-gemini-and-langfuse",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Building a Chatbot with FastHTML, Google Gemini, and Langfuse",
    "text": "Building a Chatbot with FastHTML, Google Gemini, and Langfuse\nBelow is a step by step detailed analysis of the fasthtml app. The application is built using the FastHTML web framework and leverages Google‚Äôs Gemini for its conversational AI capabilities. For observability and user feedback, it integrates with Langfuse.\n\nOverview of Functionality\nThe application provides a web-based chat interface where a user can have a conversation with an AI assistant. Key features include:\n\nReal-time Interaction: Uses WebSockets for instant message delivery between the client and server.\nSession Management: Each user conversation is tracked in a unique session, both for the AI model‚Äôs memory and for observability tracing.\nAI Integration: Connects to the Google Gemini API to generate intelligent responses.\nLLM Observability: Integrates Langfuse to trace entire conversations, log individual AI generations (turns), and record performance metrics like token usage.\nUser Feedback: Includes ‚Äúthumbs up/down‚Äù buttons on AI responses, allowing users to provide feedback that is logged directly into Langfuse.\nGraceful Cleanup: Properly finalizes traces and releases resources when a user clears the chat or closes the browser tab.\n\n\n\nInitial Setup and Configuration\nThis section covers the initial boilerplate: importing necessary libraries, defining constants, and setting up the basic HTML structure and styling.\n# Import necessary libraries and modules.\nimport traceback # For formatting exception tracebacks for logging.\nfrom typing import Literal # For creating type hints for specific string values.\nimport uuid # For generating unique session IDs.\nfrom fasthtml.common import * # Core components from the FastHTML library.\nfrom fasthtml.components import Zero_md # A specific component for rendering Markdown.\nfrom dotenv import load_dotenv # To load environment variables from a .env file.\nfrom google import genai # The Google Generative AI client library.\nfrom google.genai.chats import Chat # The specific Chat object from the Google library.\nfrom langfuse import get_client # For getting the Langfuse client instance.\nfrom langfuse._client.span import LangfuseSpan # The Langfuse Span object for tracing.\nimport os # For accessing environment variables.\n\nfrom langfuse.model import ModelUsage # Langfuse model for tracking token usage.\n\n# Load environment variables from a .env file into the environment.\nload_dotenv()\n\n# --- Configuration Constants ---\n\n# The name of the environment variable that holds the Google API key.\nGOOGLE_API_KEY_ENV = \"GOOGLE_API_KEY\"\n# The specific Google Generative AI model to be used for the chat.\nGOOGLE_MODEL_NAME = \"gemini-2.0-flash\" # Ensure this model name is correct for your Google AI setup\n\n# --- Langfuse Constants ---\n# The name for the parent span that encompasses an entire chat conversation.\nLANGFUSE_CONVERSATION_SPAN_NAME = \"chat_conversation\"\n# The name for a generation span, representing a single turn from the language model.\nLANGFUSE_GENERATION_NAME = \"llm_turn\"\n# The name used in Langfuse for scores that come from user feedback.\nLANGFUSE_SCORE_NAME = \"user_feedback_score\"\n\n# Define a type alias 'Role' which can only be the string \"user\" or \"assistant\".\ntype Role = Literal[\"user\", \"assistant\"]\n\n# Set up the HTML headers for the application.\n# This includes CSS and JavaScript for styling and functionality.\nhdrs = (\n    picolink, # A minimal CSS framework.\n    Script(src=\"https://cdn.tailwindcss.com\"), # Tailwind CSS for utility-first styling.\n    Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\"), # DaisyUI component library for Tailwind.\n    Script(type=\"module\", src=\"https://cdn.jsdelivr.net/npm/zero-md@3?register\"), # Zero-md for rendering markdown content on the client side.\n)\nThis section of the code handles the initial setup and configuration for the chatbot application. It begins by importing essential libraries for web development (such as fasthtml), type hinting, unique session ID generation, environment variable management, AI integration with Google Gemini, and observability with Langfuse. The load_dotenv() function is called to securely load environment variables (like API keys) from a .env file, keeping sensitive information out of the source code. Several configuration constants are defined to centralize settings such as the Google model name, environment variable names, and Langfuse tracing identifiers, making the codebase easier to maintain and update. A Role type is specified using Python‚Äôs Literal type hint, restricting valid roles to ‚Äúuser‚Äù or ‚Äúassistant‚Äù for improved code safety and clarity. The hdrs tuple collects all necessary CSS and JavaScript resources to be included in the HTML header, including the minimal picolink CSS, TailwindCSS for utility-first styling, daisyUI for ready-made UI components, and Zero-md for client-side Markdown rendering of chat messages.\n\n\nMarkdown Rendering Helper\ndef render_local_md(md: str) -&gt; Zero_md:\n    \"\"\"Renders a markdown string using the Zero-md component.\n    \n    It injects custom CSS to override the default white background and dark text,\n    allowing the markdown to inherit the styling of its container (e.g., the chat bubble).\n    \"\"\"\n    # CSS to unset the default background and color, making it transparent.\n    css = '.markdown-body {background-color: unset !important; color: unset !important;}'\n    # A template to hold the custom style.\n    css_template = Template(Style(css), data_append=True)\n    # The Zero_md component containing the style and the markdown content.\n    return Zero_md(css_template, Script(md, type=\"text/markdown\"))\nThis function is a utility for rendering Markdown content within the chat interface. It uses the Zero_md component from FastHTML to convert Markdown strings into HTML. The function first defines custom CSS to override the default white background and dark text color of the Markdown, allowing it to inherit the styles of its parent container (like a chat bubble). This ensures that the rendered Markdown blends seamlessly with the chat UI. The function is drawn from Isaac Flath article.\nNote that there exist other ways to render Markdown in FastHTML, see Section¬†5.\n\n\nThe SessionsManager Class\nThis class is the cornerstone of the application‚Äôs state management. It encapsulates all interactions with the external Google and Langfuse services, and it keeps track of individual user sessions. This design prevents global state and allows the application to handle multiple concurrent users cleanly.\nclass SessionsManager:\n    \"\"\"\n    Manages all session-related states, including Google GenAI chat sessions\n    and Langfuse tracing spans, mapping them by a unique session ID.\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initializes the SessionsManager, setting up Google and Langfuse clients.\"\"\"\n        # Retrieve the Google API key from environment variables.\n        google_api_key = os.getenv(GOOGLE_API_KEY_ENV)\n        if not google_api_key:\n            raise ValueError(f\"Environment variable '{GOOGLE_API_KEY_ENV}' is not set.\")\n        \n         # Configure and create the Google GenAI client.\n        self.google_client = genai.Client(api_key=google_api_key)\n        \n        # Check for and initialize the Langfuse client.\n        required_langfuse_envs = [\"LANGFUSE_PUBLIC_KEY\", \"LANGFUSE_SECRET_KEY\", \"LANGFUSE_HOST\"]\n        missing = [env for env in required_langfuse_envs if not os.getenv(env)]\n        if missing:\n            raise ValueError(f\"Missing required Langfuse environment variables: {', '.join(missing)}\")\n        self.langfuse_client = get_client()\n        # Verify that the Langfuse credentials are correct.\n        if not self.langfuse_client.auth_check():\n            raise RuntimeError(\"Failed to initialize Langfuse client. Check your environment variables.\")\n\n        # Dictionaries to store active sessions, keyed by session_id.\n        self._chats: dict[str, Chat] = {} # Stores Google GenAI chat objects.\n        self._conversations_spans: dict[str, LangfuseSpan] = {} # Stores Langfuse conversation spans.\n\n    def get_google_chat_session(self, session_id: str) -&gt; Chat:\n        \"\"\"\n        Retrieves the Google GenAI chat session for a given session ID.\n        If a session doesn't exist, it creates a new one.\n        \"\"\"\n        if session_id not in self._chats:\n            # Create a new chat session using the specified model.\n            self._chats[session_id] = self.google_client.chats.create(model=GOOGLE_MODEL_NAME)\n        return self._chats[session_id]\n\n    def clear_google_chat_session(self, session_id: str) -&gt; None:\n        \"\"\"Removes the Google GenAI chat session for a given session ID, effectively resetting it.\"\"\"\n        if session_id in self._chats:\n            del self._chats[session_id]\n\n    def get_conversation_span(self, session_id: str) -&gt; LangfuseSpan:\n        \"\"\"\n        Retrieves or creates the parent Langfuse span for the entire conversation.\n        This span groups all related LLM turns (generations) into a single trace.\n        \"\"\"\n        if session_id not in self._conversations_spans:\n            # Start a new span (which also creates a new trace).\n            self._conversations_spans[session_id] = self.langfuse_client.start_span(name=LANGFUSE_CONVERSATION_SPAN_NAME)\n            # Associate the trace with the user's session ID for filtering in Langfuse.\n            self._conversations_spans[session_id].update_trace(user_id=session_id)\n        return self._conversations_spans[session_id]\n\n    def end_conversation_span(self, session_id: str) -&gt; None:\n        \"\"\"\n        Ends the current Langfuse conversation span and ensures all buffered\n        data is sent to the Langfuse server.\n        \"\"\"\n        if session_id in self._conversations_spans:\n            self._conversations_spans[session_id].end()\n            del self._conversations_spans[session_id]\n            self.langfuse_client.flush() # Manually flush to ensure data is sent.\nThe SessionsManager class is responsible for managing all session-related state and external service interactions in the chatbot application. Its constructor (__init__) ensures that all required environment variables for both Google Gemini and Langfuse are present, raising clear errors if any are missing, which helps catch configuration issues early. It initializes the Google GenAI client and the Langfuse client, and sets up two dictionaries‚Äî_chats for storing active Google chat sessions and _conversations_spans for tracking Langfuse conversation spans‚Äîeach keyed by a unique session_id. The get_google_chat_session method retrieves or creates a Google chat session for a given user, maintaining conversation context across messages. Similarly, get_conversation_span retrieves or creates a top-level Langfuse span for the conversation, which acts as a parent for all LLM generations and is tagged with the session_id for traceability. The clear_google_chat_session and end_conversation_span methods handle cleanup by removing session objects from memory and, in the case of Langfuse, marking the span as complete and flushing any buffered data to ensure all observability information is sent.\n\n\nApplication Initialization, UI Components and Main Page\nThis section defines the FastHTML components that make up the user interface and initializes the FastHTML application itself.\n# Initialize the FastHTML application with the defined headers and a default CSS class.\napp = FastHTML(hdrs=hdrs, cls=\"p-4 max-w-lg mx-auto\", exts=\"ws\")\n\n# Create a single instance of the SessionsManager to be used by the entire application.\nsessions_manager = SessionsManager()\n\n\ndef ChatMessage(msg: str, role: Role, trace_id: str | None = None, observation_id: str | None = None) -&gt; Div:\n    \"\"\"\n    A component function that renders a single chat message bubble.\n    \"\"\"\n    rendered_msg = render_local_md(msg) # Convert markdown text to a renderable component.\n    # Determine bubble color based on the role (user or assistant).\n    bubble_class = \"chat-bubble-primary\" if role == \"user\" else 'chat-bubble-secondary'\n    # Determine bubble alignment based on the role.\n    chat_class = \"chat-end\" if role == \"user\" else 'chat-start'\n\n    feedback_buttons_html = \"\"\n\n    # Only show feedback buttons for assistant messages that have tracing info.\n    if role == \"assistant\" and trace_id and observation_id:\n        feedback_container_id = f\"feedback-{observation_id}\"\n        # Values to be sent when the user clicks the \"like\" button.\n        vals_up = {\"observation_id\": observation_id, \"trace_id\": trace_id, \"score\": 1}\n        # Values to be sent when the user clicks the \"dislike\" button.\n        vals_down = {\"observation_id\": observation_id, \"trace_id\": trace_id, \"score\": 0}\n\n        # Create the feedback buttons using htmx attributes for AJAX POST requests.\n        feedback_buttons_html = Div(\n            Button(\n                \"üëç\",\n                hx_post=\"/score_message\", hx_vals=vals_up, # POST to /score_message with 'up' values.\n                hx_target=f\"#{feedback_container_id}\", hx_swap=\"outerHTML\", # Replace the buttons with the response.\n                cls=\"btn btn-xs btn-ghost\"\n            ),\n            Button(\n                \"üëé\",\n                hx_post=\"/score_message\", hx_vals=vals_down, # POST to /score_message with 'down' values.\n                hx_target=f\"#{feedback_container_id}\", hx_swap=\"outerHTML\", # Replace the buttons with the response.\n                cls=\"btn btn-xs btn-ghost\"\n            ),\n            id=feedback_container_id,\n            cls=\"flex space-x-1 mt-1\"\n        )\n\n    # Construct the final chat message div.\n    return Div(cls=f\"chat {chat_class}\")(\n        Div(role, cls=\"chat-header\"),\n        Div(rendered_msg, cls=f\"chat-bubble {bubble_class}\"),\n        feedback_buttons_html if feedback_buttons_html else \"\",\n    )\n\ndef ChatInput() -&gt; Input:\n    \"\"\"\n    A component function that returns the chat input field.\n    The `hx_swap_oob='true'` attribute allows this component to be targeted\n    for an \"Out of Band\" swap, which is used to clear the input after a message is sent.\n    \"\"\"\n    return Input(\n        name='msg', \n        id='msg-input', \n        placeholder=\"Type a message\",\n        cls=\"input input-bordered w-full\", \n        hx_swap_oob='true',\n        autocomplete=\"off\",  # Disable browser's native autocomplete.\n    )\n\n# The main application route, handling GET requests to the root URL.\n@app.get(\"/\")\ndef index():\n    \"\"\"Defines the main chat page UI.\"\"\"\n    # The main form element that handles WebSocket communication.\n    page = Form(\n        ws_send=True, # Automatically sends form data over the WebSocket on submit.\n        hx_ext=\"ws\", # Enables the htmx WebSocket extension.\n        ws_connect=\"/wscon\", # The WebSocket endpoint to connect to.\n    )(\n        # The container where chat messages will be appended.\n        Div(id=\"chatlist\", cls=\"chat-box h-[73vh] overflow-y-auto\"),\n        # The container for the input field and buttons.\n        Div(cls=\"flex space-x-2 mt-2\")(\n            Group(\n                ChatInput(), \n                Button(\"Send\", cls=\"btn btn-primary\", hx_vals='{\"action\": \"send\"}'),\n                Button(\"Clear Chat\", cls=\"btn btn-warning\", hx_post=\"/clear_chat\", hx_target=\"#chatlist\", hx_swap=\"innerHTML\", hx_include=\"[name='session_id']\"),\n                ),\n        ),\n        # A hidden input to store the unique session ID for this client.\n        Hidden(name=\"session_id\", id=\"session-id\", hx_swap_oob=\"true\", value=str(uuid.uuid4())),\n    )\n    # Return the page wrapped in a title.\n    return Titled('Chatbot Demo', page)\nThe app = FastHTML(...) line creates the application instance, injecting the previously defined CSS and JavaScript resources (hdrs), applying TailwindCSS classes for layout, and enabling the HTMX WebSocket extension for real-time communication. A single SessionsManager instance is created to handle all session and external service management.\nThe ChatMessage function generates the HTML for each chat bubble, styling it based on the sender‚Äôs role and rendering the message content as Markdown using the render_local_md helper. For assistant messages, if tracing information is available, it conditionally adds ‚Äúthumbs up/down‚Äù feedback buttons, which use HTMX attributes to send POST requests to the server and update the UI in place.\nThe ChatInput function creates the chat input field, using the hx_swap_oob='true' attribute to ensure the input is cleared after each message, regardless of the main target of the server response. This design provides a responsive, interactive chat interface with integrated observability and user feedback mechanisms.\nThe main page route is defined in the index function, which returns the initial HTML structure for the chatbot interface. This structure includes a display area for chat messages (chatlist), an input field for user messages, ‚ÄúSend‚Äù and ‚ÄúClear Chat‚Äù buttons, and a hidden input field containing a unique session ID to track the user‚Äôs conversation. The form is configured for real-time updates using WebSocket communication, leveraging HTMX and FastHTML extensions. Specifically, the @app.get(\"/\") decorator registers the handler for the root URL, constructing the main page with a form that connects to the /wscon WebSocket endpoint (ws_connect=\"/wscon\") and submits data over the WebSocket (ws_send=True). The chat messages are displayed in a div with id=\"chatlist\", while the ‚ÄúClear Chat‚Äù button triggers a standard HTTP POST request, ensuring the session ID is included via hx_include=\"[name='session_id']\". The hidden input for session_id is initialized with a placeholder value, enabling session tracking for each user.\n\n\nWebSocket\nThe functions below handle WebSocket connections, disconnections, and message processing. They manage the lifecycle of user sessions, including session ID generation, message handling, and cleanup on disconnection.\nasync def on_connect(ws, send):\n    \"\"\"Callback function executed when a new WebSocket connection is established.\"\"\"\n    # Generate a new unique ID for this session.\n    session_id = str(uuid.uuid4())\n    # Store the session ID in the WebSocket's scope for later access.\n    ws.scope['session_id'] = session_id\n    # Send a new hidden input field to the client via an OOB swap.\n    # This updates the `session-id` input on the client with the server-generated ID.\n    await send(Hidden(name=\"session_id\", id=\"session-id\", value=session_id, hx_swap_oob=\"true\"))\n    print(f\"SERVER: WebSocket connected. Session ID: {session_id}.\")\n\n\nasync def on_disconnect(ws):\n    \"\"\"\n    Callback function executed when a WebSocket connection is closed.\n    This is used for cleaning up server-side resources associated with the session.\n    \"\"\"\n    # Retrieve the session ID from the connection's scope.\n    session_id = ws.scope.get('session_id', None)\n    if not session_id:\n        print(\"ERROR: WebSocket disconnect called without a session ID. Cannot clean up.\")\n        return\n    print(f\"SERVER: WebSocket disconnected for Session ID: {session_id}. Cleaning up session.\")\n    try:\n        # Get the session objects.\n        current_chat_session = sessions_manager.get_google_chat_session(session_id=session_id)\n        conv_span = sessions_manager.get_conversation_span(session_id=session_id)\n        # Extract the full chat history from the Google chat object.\n        messages = [\n            {\"role\": message.role, \"content\": getattr(message.parts[0], \"text\", \"\")}\n            for message in current_chat_session.get_history()\n            if hasattr(message, \"role\") and hasattr(message, \"parts\") and message.parts\n        ]\n        # Update the Langfuse span with the full conversation history before ending it.\n        if messages and conv_span:\n            conv_span.update(\n                input=messages[:-1],  # All messages except the last are considered input.\n                output=messages[-1],  # The final message is the output of the whole conversation.\n            )\n        # Clean up the session data from the manager.\n        sessions_manager.clear_google_chat_session(session_id=session_id)\n        sessions_manager.end_conversation_span(session_id=session_id)\n        print(f\"SERVER: Cleanup complete for session: {session_id}.\")\n    except Exception as e:\n        # Log any errors that occur during the cleanup process.\n        print(f\"ERROR during WebSocket disconnect cleanup for Session ID: {session_id}: {e}\\n{traceback.format_exc()}\")\n\n\n@app.ws(\"/wscon\", conn=on_connect, disconn=on_disconnect)\nasync def ws_chat_handler(msg:str, ws, send):\n    \"\"\"\n    The main WebSocket message handler. This is called every time a message is\n    received from a client over the WebSocket.\n    \"\"\"\n    # Get the session ID associated with this WebSocket connection.\n    session_id = ws.scope.get('session_id', None)\n    if not session_id:\n        print(\"ERROR: WebSocket handler called without a session ID. Cannot process message.\")\n        return\n\n    # Ignore empty messages from the client.\n    if not msg.strip():\n        await send(ChatInput()) # Resend a clean input field.\n        return\n\n    # Ensure the conversation span and chat session are ready.\n    conv_span = sessions_manager.get_conversation_span(session_id=session_id)\n    current_chat_session = sessions_manager.get_google_chat_session(session_id=session_id)\n\n    # Get the trace ID for this conversation to pass to the UI for feedback.\n    trace_id = conv_span.trace_id\n\n    # --- Optimistic UI Update ---\n    # Immediately send the user's message back to them so it appears in the chat list.\n    await send(Div(ChatMessage(msg=msg, role=\"user\"), hx_swap_oob='beforeend', id=\"chatlist\"))\n    # Send a new, empty input field to clear the user's input.\n    await send(ChatInput())\n\n    try:\n        # Start a Langfuse generation span to trace this specific LLM call.\n        with conv_span.start_as_current_generation(name=LANGFUSE_GENERATION_NAME, input=msg, model=GOOGLE_MODEL_NAME) as generation:\n            # Send the user's message to the Google GenAI API.\n            response = current_chat_session.send_message(msg)\n            r = response.text.rstrip()  # Get the response text and clean it.\n            \n            # Create a ModelUsage object with token counts from the response metadata.\n            usage = ModelUsage(\n                input=response.usage_metadata.prompt_token_count,\n                output=response.usage_metadata.candidates_token_count,\n                total=response.usage_metadata.total_token_count,\n            )\n            # Update the Langfuse generation with the output and token usage.\n            generation.update(output=r, usage_details=usage)\n            # Get the unique ID of this generation for the feedback mechanism.\n            observation_id = generation.id\n\n        # Send the assistant's response to the client's chat list.\n        await send(Div(ChatMessage(msg=r, role=\"assistant\", trace_id=trace_id, observation_id=observation_id), hx_swap_oob='beforeend', id=\"chatlist\"))\n    except Exception as e:\n        # Log the full error on the server for debugging.\n        print(f\"ERROR in WebSocket handler during AI call: {e}\\n{traceback.format_exc()}\")\n        \n        # Log the error as an \"event\" within the Langfuse conversation trace for observability.\n        if conv_span:\n            conv_span.create_event(\n                name=\"llm_turn_error\", \n                level=\"ERROR\", \n                status_message=str(e), \n                metadata={\"traceback\": traceback.format_exc()}\n            )\n        \n        # Send a user-friendly error message to the chat UI.\n        error_ui_msg = \"Sorry, I encountered an issue processing your message. Please try again.\"\n        await send(Div(ChatMessage(msg=error_ui_msg, role=\"assistant\"), hx_swap_oob='beforeend', id=\"chatlist\"))\nThe on_connect asynchronous function is triggered when a client establishes a WebSocket connection. It generates a unique session_id using uuid.uuid4() and stores this identifier in the WebSocket‚Äôs scope dictionary, which holds connection-specific data. The function then sends a hidden input component containing the new session ID back to the client. Thanks to the hx_swap_oob=\"true\" attribute and a matching id, HTMX seamlessly replaces the placeholder session-id on the client side with this server-generated value.\nThe on_disconnect function is called when the client disconnects, such as when the browser tab is closed. This function is essential for resource cleanup. It retrieves the session_id from the WebSocket‚Äôs scope and uses the sessions_manager to access the final chat history. Before ending the session, it updates the top-level Langfuse span with the complete conversation, specifying the full input and output. This ensures that Langfuse maintains a comprehensive record of the chat. The function then calls the manager‚Äôs methods to end the span and clear the session data from memory. Wrapping this logic in a try/except block ensures that any errors during cleanup do not disrupt the server.\nThe @app.ws(...) decorator registers the main WebSocket handler for the /wscon endpoint and associates the on_connect and on_disconnect functions for connection lifecycle management. The handler receives the user‚Äôs message (msg), the WebSocket connection object (ws), and a send function for returning data to the client. It begins by retrieving the session_id, ignoring empty messages, and obtaining the active chat session and Langfuse span from the sessions_manager.\nFor immediate user feedback, the handler sends two components back to the client: the user‚Äôs message, rendered as a ChatMessage and appended to the chat list using hx_swap_oob='beforeend' and id=\"chatlist\", and a new ChatInput() component, which clears the input field due to its hx_swap_oob='true' attribute.\nThe core logic for handling the AI response is wrapped in a try/except block. Within this block, a Langfuse generation span is started using a context manager, which logs the input message and model name, and automatically tracks timing. The user‚Äôs message is sent to the Google Gemini API, and the response text and token usage metadata are extracted. The generation span is then updated with the AI‚Äôs output and usage details, and the unique observation_id for this generation is retrieved from Langfuse. This ID, along with the trace_id, is used to enable the feedback buttons in the chat UI.\nFinally, the AI‚Äôs response is sent to the client as a ChatMessage, including the necessary identifiers for feedback. If an error occurs during the API call, the exception is caught, logged to the server console, and an error event is created in Langfuse for observability. The user also receives a friendly error message in the chat interface.\n\n\nSupporting HTTP Endpoints\nThese are standard HTTP endpoints that support auxiliary actions like clearing the chat and scoring messages.\n@app.post(\"/clear_chat\")\ndef clear_chat(session_id: str):\n    \"\"\"\n    HTTP endpoint to handle the 'Clear Chat' button press.\n    It resets the chat session on the server.\n    \"\"\"\n    # Retrieve the current session objects.\n    current_chat_session = sessions_manager.get_google_chat_session(session_id=session_id)\n    conv_span = sessions_manager.get_conversation_span(session_id=session_id)\n    \n    # Extract chat history before clearing, to update the Langfuse trace.\n    messages = [\n        {\"role\": message.role, \"content\": getattr(message.parts[0], \"text\", \"\")}\n        for message in current_chat_session.get_history()\n        if hasattr(message, \"role\") and hasattr(message, \"parts\") and message.parts\n    ]\n    # Update the Langfuse span with the full conversation history.\n    if messages and conv_span:\n        conv_span.update(\n            input=messages[:-1],\n            output=messages[-1],\n        )\n\n    # Clear the server-side session data and end the Langfuse span.\n    sessions_manager.clear_google_chat_session(session_id=session_id)\n    sessions_manager.end_conversation_span(session_id=session_id)\n\n    # Return an empty chat list (to replace the old one) and a fresh input field.\n    return Div(id=\"chatlist\", cls=\"chat-box h-[73vh] overflow-y-auto\"), ChatInput()\n\n\n@app.post(\"/score_message\")\ndef score_message(trace_id: str, observation_id: str, score: int):\n    \"\"\"\n    HTTP endpoint to handle user feedback (üëç/üëé).\n    It logs a score in Langfuse for the specific LLM generation.\n    \"\"\"\n    try:\n        # Use the Langfuse client to create a score.\n        sessions_manager.langfuse_client.create_score(\n            name=LANGFUSE_SCORE_NAME,\n            trace_id=trace_id, # Link the score to the correct conversation trace.\n            observation_id=observation_id, # Link the score to the specific message.\n            value=score, # The score value (1 for like, 0 for dislike).\n            data_type=\"BOOLEAN\", # The type of the score value.\n        )\n        # Ensure the score is sent immediately.\n        sessions_manager.langfuse_client.flush()\n        # Return a \"Thanks!\" message to replace the feedback buttons in the UI.\n        return P(\"Thanks!\", cls=\"text-xs text-success mt-1 ml-2\")\n    except Exception as e:\n        # Log any error that occurs while trying to record the score.\n        print(f\"Error scoring message: {observation_id}, score: {score}, error: {e}\\n{traceback.format_exc()}\")\n        # Also, try to log this scoring failure as an event in Langfuse for better observability.\n        try:\n            sessions_manager.langfuse_client.event(\n                trace_id=trace_id,\n                parent_observation_id=observation_id,\n                name=\"scoring_error\",\n                level=\"ERROR\",\n                input={\"observation_id\": observation_id, \"score_attempted\": score},\n                output={\"error_message\": str(e)},\n                metadata={\"traceback\": traceback.format_exc()}\n            )\n            sessions_manager.langfuse_client.flush()\n        except Exception as langfuse_event_err:\n            print(f\"CRITICAL: Failed to log scoring error to Langfuse: {langfuse_event_err}\")\n\n        # Return an error message to the user.\n        return P(\"Error.\", cls=\"text-xs text-error mt-1 ml-2\")\nThe two HTTP endpoints, @app.post(\"/clear_chat\") and @app.post(\"/score_message\"), provide essential support for the chatbot‚Äôs user experience and observability features.\nThe /clear_chat endpoint is triggered when the user clicks the ‚ÄúClear Chat‚Äù button. It receives the session_id from the form data and performs cleanup similar to the WebSocket disconnect logic: it finalizes the current Langfuse trace by updating it with the full conversation history, clears the server-side chat session, and ends the Langfuse span. The endpoint then returns an empty chat list (a Div with id=\"chatlist\") to replace the current chat content, and a new ChatInput() component to clear the input field. This ensures the UI is reset for a fresh conversation.\nThe /score_message endpoint handles user feedback submitted via the ‚Äúthumbs up‚Äù or ‚Äúthumbs down‚Äù buttons on assistant messages. FastHTML automatically parses the hx-vals JSON data from the button into function arguments (trace_id, observation_id, and score). The endpoint calls sessions_manager.langfuse_client.create_score(), linking the feedback to the specific AI generation and trace in Langfuse. If the score is recorded successfully, it returns a simple ‚ÄúThanks!‚Äù message, which replaces the feedback buttons in the UI to prevent repeated submissions. Robust error handling is included: if the score submission fails, the error is logged, and an error event is also attempted in Langfuse for observability. The user sees an error message in the UI if something goes wrong.\nTogether, these endpoints ensure that chat state can be reset cleanly and that user feedback is reliably captured and linked to the correct AI responses for later analysis in Langfuse."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#approach-1-custom-javascript-with-marked.js",
    "href": "posts/llm-observability/llm-observability.html#approach-1-custom-javascript-with-marked.js",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Approach 1: Custom JavaScript with marked.js",
    "text": "Approach 1: Custom JavaScript with marked.js\nThis method involves manually including the marked.js library and writing a small script to process elements on the page.\n# Include the script in the &lt;head&gt; of your application\nmarkdown_js = \"\"\"\nimport { marked } from \"https://cdn.jsdelivr.net/npm/marked/lib/marked.esm.js\";\nproc_htmx('.markdown', e =&gt; e.innerHTML = marked.parse(e.textContent));\n\"\"\"\napp = FastHTML(hdrs=(..., Script(markdown_js, type='module')))\nMechanism: A JavaScript snippet imports marked.js from a CDN. It then uses the FastHTML helper proc_htmx('.markdown', ...) to find elements with the markdown class whenever content is loaded or swapped by HTMX. For each element found, the script takes its plain text content, parses it as Markdown, and replaces the element‚Äôs innerHTML with the resulting HTML.\nUsage: Place your raw Markdown string inside any container element and assign it the markdown class.\n# Example: Rendering a Markdown-formatted chat message\nDiv(\"**User:** How do I render Markdown in FastHTML?\\n\\n**Assistant:** Use the `Div` component with the `markdown` class to display formatted content.\", cls=\"markdown\")\nStyling is handled externally. You must provide your own CSS rules that target the HTML elements generated by marked.js (e.g., h1, p, ul) within the .markdown container.\n/* Example external CSS */\n.markdown p {\n  margin-bottom: 1rem;\n}\n.markdown h1 {\n  font-size: 2em;\n}\nKey Characteristics\n\nGlobal Processing: Relies on a global script that processes elements based on a CSS class.\nSimple Logic: The core mechanism is straightforward‚Äîfind an element, parse its text, and replace its content.\nSeparation of Concerns: Rendering logic (JS) is separate from styling (CSS).\nManual Setup: Requires you to write and maintain the script and library import."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#approach-2-fasthtml-built-in-components",
    "href": "posts/llm-observability/llm-observability.html#approach-2-fasthtml-built-in-components",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Approach 2: FastHTML Built-in Components",
    "text": "Approach 2: FastHTML Built-in Components\nFastHTML provides convenient components, MarkdownJS and HighlightJS, that bundle the necessary client-side libraries and initialization scripts.\nfrom fasthtml.common import *\n\n# Include the components in the &lt;head&gt;\nhdrs = (MarkdownJS(), HighlightJS(langs=['python', 'javascript', 'html', 'css']), )\napp, rt = fast_app(hdrs=hdrs)\nMechanism\n\nMarkdownJS(): Injects a script that automatically finds elements with a specific class (typically marked) and renders their text content as Markdown. This is a ‚Äúbatteries-included‚Äù version of the custom marked.js approach.\nHighlightJS(): Injects the Highlight.js library and a script to apply syntax highlighting to code blocks (e.g., &lt;pre&gt;&lt;code&gt;...&lt;/code&gt;&lt;/pre&gt;). It works seamlessly with MarkdownJS, which generates the required HTML structure for fenced code blocks.\n\nUsage: Place your raw Markdown string in a container element and assign it the marked class.\n@rt('/')\ndef get(req):\n    content = \"\"\"\n    ## Example\n    Here is some _markdown_ with a **code block**:\n\n    ```python\n    def hello():\n        print(\"Hello, world!\")\n    ```\n    \"\"\"\n    return Titled(\"Markdown Example\", Div(content, cls=\"marked\"))\nStyling\n\nMarkdown: General elements (paragraphs, lists, headings) are styled by your main CSS, such as PicoCSS or custom stylesheets.\nSyntax Highlighting: HighlightJS includes default CSS themes for code blocks.\n\nKey Characteristics\n\nConvenience: Uses pre-packaged components, reducing boilerplate for common tasks.\nIntegrated Solution: MarkdownJS and HighlightJS are designed to work together out of the box.\nClient-Side Rendering: Both parsing and highlighting occur in the user‚Äôs browser.\nFastHTML Native: A well-supported and idiomatic way to handle Markdown in the FastHTML ecosystem."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#approach-3-using-the-zero-md-web-component",
    "href": "posts/llm-observability/llm-observability.html#approach-3-using-the-zero-md-web-component",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Approach 3: Using the zero-md Web Component",
    "text": "Approach 3: Using the zero-md Web Component\nThis modern approach uses the &lt;zero-md&gt; web component, which encapsulates both rendering and styling.\n# Register the web component in the &lt;head&gt;\nhdrs = (\n    Script(type=\"module\", src=\"https://cdn.jsdelivr.net/npm/zero-md@3?register\"),\n)\nMechanism: zero-md is a self-contained custom HTML element (&lt;zero-md&gt;). It manages its own rendering lifecycle and styling, often using a Shadow DOM to prevent style conflicts. The Markdown source is passed to it inside a &lt;script type=\"text/markdown\"&gt; tag.\nUsage: A helper function can abstract the component‚Äôs structure, making it easy to use.\n# Helper function to generate the &lt;zero-md&gt; component\ndef render_local_md(md_content_string):\n    # This CSS overrides zero-md defaults to inherit color from the parent\n    css = '.markdown-body { background-color: unset !important; color: unset !important; }'\n    \n    # zero-md is styled via an internal &lt;template&gt;\n    css_template = Template(Style(css), data_append=True)\n    \n    return Zero_md(\n        css_template,\n        Script(md_content_string, type=\"text/markdown\")\n    )\n\n# Example: Rendering a Markdown-formatted chat message using zero-md\nsample_message = \"\"\"\n**User:** How do I render Markdown in Fa    stHTML?\n\n**Assistant:** Use the `render_local_md` helper function to display formatted Markdown content in your chat UI.\n\"\"\"\n\nrender_local_md(sample_message)\nStyling: zero-md comes with its own default styles. You can customize them by passing a &lt;template&gt; containing &lt;style&gt; tags, as shown in the helper function. These styles are scoped to the component, preventing them from affecting the rest of your page.\nKey Characteristics\n\nEncapsulation: Functionality and styling are self-contained, reducing the risk of CSS conflicts.\nDeclarative: You use a custom HTML tag (&lt;zero-md&gt;) to embed Markdown.\nRich Features: Often includes built-in syntax highlighting and theming."
  },
  {
    "objectID": "posts/llm-observability/llm-observability.html#comparison-of-approaches",
    "href": "posts/llm-observability/llm-observability.html#comparison-of-approaches",
    "title": "LLM Observability with Langfuse and a FastHTML Chatbot",
    "section": "Comparison of Approaches",
    "text": "Comparison of Approaches\n\nSummary Table\n\n\n\n\n\n\n\n\n\nFeature\nCustom marked.js\nFastHTML Built-ins\nzero-md Web Component\n\n\n\n\nType\nCustom JS snippet\nFastHTML components\nWeb Component (&lt;zero-md&gt;)\n\n\nInvocation\nGlobal JS on .markdown class\nAuto-processes .marked class\nExplicit use of &lt;zero-md&gt; tag\n\n\nMD Source\nElement‚Äôs textContent\nElement‚Äôs textContent\n&lt;script type=\"text/markdown\"&gt;\n\n\nStyling (MD)\nExternal CSS\nExternal CSS\nInternal defaults, customizable via &lt;template&gt;\n\n\nStyling (Code)\nRequires separate solution\nVia HighlightJS themes\nBuilt-in or via &lt;template&gt;\n\n\nEncapsulation\nLow (global scope)\nMedium (managed by components)\nHigh (Shadow DOM)\n\n\nDependencies\nManual include of marked.js\nBundled by FastHTML\nExternal include of zero-md\n\n\nEase of Use\nSimple but manual\nVery simple and integrated\nSimple with a Python helper\n\n\n\n\n\nKey Differences and Recommendations\nThe best approach depends on your project‚Äôs needs for simplicity, control, and integration.\n\nSimplicity vs.¬†Encapsulation: The FastHTML Built-ins offer the simplest, most integrated experience. The zero-md approach provides superior encapsulation, making it ideal for complex UIs where style conflicts are a concern (e.g., when using utility-first CSS frameworks like Tailwind). The custom marked.js approach is a good middle-ground if you want control without external dependencies beyond the library itself.\nStyling and Integration: With the marked.js and MarkdownJS approaches, the rendered HTML is part of the main document and is styled by global CSS. This makes global theming easy but can lead to style collisions. With zero-md, styling is scoped, which prevents conflicts but requires learning its specific customization method (via &lt;template&gt;). The example css for zero-md shows how to ‚Äúbreak out‚Äù of its default theme to better blend with the surrounding page.\n\n\nWhen to Choose Each Approach\n\nUse the Custom marked.js Approach for simple scenarios where you want full control over the script and prefer to manage styling globally without adding extra FastHTML components.\nUse the FastHTML Built-ins (MarkdownJS & HighlightJS) as the default, recommended choice for most FastHTML projects. It‚Äôs the most convenient, ‚Äúbatteries-included‚Äù solution for rendering Markdown with syntax highlighting.\nUse the zero-md Web Component when style encapsulation is critical, especially when integrating into a complex UI with frameworks like Tailwind/DaisyUI. It is a more robust and modern solution for embedding self-contained content blocks."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html",
    "href": "posts/py-concurrency/py-concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "This blog post is organized as follows:\nEach section builds on the previous ones to provide a comprehensive understanding of Python concurrency."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#what-is-concurrency",
    "href": "posts/py-concurrency/py-concurrency.html#what-is-concurrency",
    "title": "Python Concurrency",
    "section": "What Is Concurrency?",
    "text": "What Is Concurrency?\nConcurrency is the simultaneous occurrence of events. In Python, these events are called:\n\nThread\nTask\nProcess\n\nThese all represent a sequence of instructions that run in order. They can be stopped, and the CPU can switch to a different one. The state of each sequence is saved so it can be restored.\nThreads, tasks, and processes differ in their details. Multiple system processes can enable Python to run these sequences at the same time.\nThreads and asynchronous tasks run on a single processor, meaning they run one at a time. They take turns to speed up the overall process.\nIn a multi-threaded approach, the operating system knows about each thread and can interrupt it to run a different thread. This is also true for processes and is called preemptive multitasking.\nIn preemptive multitasking, code in the thread doesn‚Äôt need to do anything special to make the switch. A context switch can happen in the middle of a Python statement. This is because Python statements consist of low-level bytecode instructions.\nAsynchronous tasks use cooperative multitasking. The tasks cooperate by announcing when they‚Äôre ready to be switched out. The code in the task has to change to make this happen.\nThe benefit is that you know where your task will be swapped out, making it easier to understand the flow of execution. A task won‚Äôt be swapped out in the middle of a Python statement unless that statement is appropriately marked."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#what-is-parallelism",
    "href": "posts/py-concurrency/py-concurrency.html#what-is-parallelism",
    "title": "Python Concurrency",
    "section": "What Is Parallelism?",
    "text": "What Is Parallelism?\nParallelism involves executing separate processes, each running in its own Python interpreter.\nEach process can run on a different CPU core. This means they can run at the same time.\nHere‚Äôs a summary of Python modules for concurrency and parallelism:\n\n\n\n\n\n\n\n\n\nPython Module\nCPU\nMultitasking\nSwitching Decision\n\n\n\n\nasyncio\nOne\nCooperative\nThe tasks decide when to give up control.\n\n\nthreading\nOne\nPreemptive\nThe operating system decides when to switch tasks external to Python.\n\n\nmultiprocessing\nMany\nPreemptive\nThe processes all run at the same time on different processors.\n\n\n\nThese modules will be explored in this blog post.\nthreading and multiprocessing are low-level building blocks. They can often be replaced with concurrent.futures, which provides a higher-level interface. asyncio offers a different approach to concurrency.\nEach type of concurrency can be useful in its own way."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#synchronous-version",
    "href": "posts/py-concurrency/py-concurrency.html#synchronous-version",
    "title": "Python Concurrency",
    "section": "Synchronous Version",
    "text": "Synchronous Version\nThis version doesn‚Äôt use concurrency:\n\n\n\nListing¬†1: io_non_concurrent.py\n\n\nimport time\n\nimport requests\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with requests.Session() as session:\n        for url in sites:\n            download_site(url, session)\n\ndef download_site(url, session):\n    with session.get(url) as response:\n        print(f\"Read {len(response.content)} bytes from {url}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThis program downloads site contents from a list of addresses and prints their sizes. A session object from requests is used. Creating a Session object allows the library to retain state across requests and reuse the connection to speed things up. The session is created in download_all_sites() and then the list of sites is iterated through, downloading each one. Finally, the execution time is printed. Here‚Äôs an example of the final output:\n$ python io_non_concurrent.py\nRead 10966 from https://www.jython.org\nRead 276 from http://olympus.realpython.org/dice\n ‚ãÆ\nDownloaded 160 sites in 14.289619207382202 seconds\nThese results may vary depending on network conditions."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#multi-threaded-version",
    "href": "posts/py-concurrency/py-concurrency.html#multi-threaded-version",
    "title": "Python Concurrency",
    "section": "Multi-Threaded Version",
    "text": "Multi-Threaded Version\nWriting a multi-threaded program takes more effort. Here‚Äôs the same program using concurrent.futures and threading:\n\n\n\nListing¬†2: io_threads.py\n\n\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\n\nthread_local = threading.local()\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        executor.map(download_site, sites)\n\ndef download_site(url):\n    session = get_session_for_thread()\n    with session.get(url) as response:\n        print(f\"Read {len(response.content)} bytes from {url}\")\n\ndef get_session_for_thread():\n    if not hasattr(thread_local, \"session\"):\n        thread_local.session = requests.Session()\n    return thread_local.session\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThe overall structure is the same.\nA ThreadPoolExecutor is created to manage the threads. In this case, five workers or threads are requested. The correct number of threads is not constant from one task to another. With IO-bound problems, you‚Äôre not limited to the number of CPU cores. However, at some point, diminishing returns will occur due to the overhead of switching threads. Experimentation is recommended. A ThreadPoolExecutor creates a pool of threads, each of which can run concurrently. The executor controls how and when each of the threads in the pool will run. Using a thread pool can be beneficial when you have limited system resources but still want to handle many tasks.\nIn this version, the executor calls download_site() instead of doing it manually in a loop. The executor.map() method distributes the workload across the available threads. This method takes:\n\nA function to be executed on each data item\nA collection of data items to be processed by that function\n\nSince the function passed to .map() must take one argument, download_site() was modified to only accept a URL.\nBecause the operating system controls when tasks get interrupted, any data shared between the threads needs to be thread-safe. requests.Session() isn‚Äôt thread-safe.\nOne strategy is to use a thread-safe data structure. Another strategy is to use thread-local storage. When threading.local() is called, an object that resembles a global variable but is specific to each individual thread is created.\nWhen get_session_for_thread() is called, the session it looks up is specific to the particular thread on which it‚Äôs running. So each thread will create a session the first time it calls get_session_for_thread() and then will use that session on each subsequent call.\nHere‚Äôs the output:\n$ python io_threads.py\nRead 10966 from https://www.jython.org\nRead 276 from http://olympus.realpython.org/dice\n ‚ãÆ\nDownloaded 160 sites in 3.190047219999542 seconds\nThis is faster than the non-concurrent version.\nHere‚Äôs the execution timing diagram:\n\n\n\nTiming Diagram of a Threading Solution\n\n\nThe program uses multiple threads to have many open requests out to web sites at the same time. It takes more code to make this happen, and you have to give some thought to what data is shared between threads. Threads can interact in ways that are subtle and hard to detect. These interactions can cause race conditions that frequently result in random, intermittent bugs that can be difficult to find."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asynchronous-version",
    "href": "posts/py-concurrency/py-concurrency.html#asynchronous-version",
    "title": "Python Concurrency",
    "section": "Asynchronous Version",
    "text": "Asynchronous Version\nUsing multithreading can cut down the total execution time. Python‚Äôs asyncio module enables asynchronous I/O and can be even faster.\nAsynchronous processing is a concurrency model that‚Äôs suited for I/O-bound tasks. It avoids the overhead of context switching between threads by employing the event loop, non-blocking operations, and coroutines. Asynchronous code needs only one thread of execution to run concurrently.\nThe event loop controls how and when each asynchronous task gets to execute. It continuously loops through your tasks while monitoring their state. When the current task starts waiting for an I/O operation to finish, the loop suspends it and switches to another task. When the expected event occurs, the loop resumes the suspended task in the next iteration.\nA coroutine is similar to a thread but more lightweight. You can spawn many more coroutines than threads without significant overhead.\nBlocking function calls aren‚Äôt allowed in coroutines. A blocking call prevents other code from running while it‚Äôs waiting for data to arrive. A non-blocking call can give up control and wait to be notified when the data is ready.\nIn Python, you create a coroutine object by calling an asynchronous function, also known as a coroutine function. These are defined with the async def statement. Only within the body of an asynchronous function can you use the await keyword, which pauses the execution of the coroutine until the awaited task is completed:\nimport asyncio\n\nasync def main():\n    await asyncio.sleep(3.5)\nIn this case, main() is defined as an asynchronous function. The await keyword makes a non-blocking call to asyncio.sleep(), simulating a delay. While main() awaits the wake-up event, other tasks could potentially run.\nNote: To run the sample code above, you‚Äôll need to either wrap the call to main() in asyncio.run() or await main() in Python‚Äôs asyncio REPL.\nThe Requests library is blocking, so a non-blocking counterpart, such as aiohttp, is needed. After installing this library, you can use it in the asynchronous version of the code:\n\n\n\nListing¬†3: io_asyncio.py\n\n\nimport asyncio\nimport time\nimport aiohttp\n\nasync def main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    await download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\nasync def download_all_sites(sites):\n    async with aiohttp.ClientSession() as session:\n        tasks = [download_site(url, session) for url in sites]\n        await asyncio.gather(*tasks, return_exceptions=True)\n\nasync def download_site(url, session):\n    async with session.get(url) as response:\n        print(f\"Read {len(await response.read())} bytes from {url}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\nThis version looks similar to the synchronous one.\nHere are the main differences:\n\nasyncio is imported from Python‚Äôs standard library.\nThe aiohttp library is imported.\nFunctions are redefined as asynchronous ones with the async keyword.\nThe await keyword is prepended to download_all_sites().\nThe async with statement is leveraged to create asynchronous context managers.\nA list of tasks is created using a list comprehension.\nasyncio.gather() is used to run all the tasks concurrently.\nThe completion of the session‚Äôs HTTP GET request is awaited before printing.\n\nThe session is shared across all tasks because they‚Äôre all running on the same thread.\nOne of the advantages of asyncio is that it scales well. Each task takes fewer resources and less time to create than a thread.\nThe asynchronous version is the fastest:\n$ python io_asyncio.py\nRead 10966 bytes from https://www.jython.org\nRead 10966 bytes from https://www.jython.org\n ‚ãÆ\nDownloaded 160 sites in 0.49083488899850636 seconds\nHere‚Äôs the execution timing diagram:\n\n\n\nTiming Diagram of a Asyncio Solution\n\n\nAdding async and await can be a complication, but it forces you to think about when a given task will get swapped out. The asyncio example can be run with hundreds of tasks without slowing it down. You need special asynchronous versions of libraries to gain the full advantage of asyncio. If one of the tasks doesn‚Äôt cooperate, then all the advantages of cooperative multitasking get thrown away. See Section¬†4 for a deep dive into asynchronous I/O with AsyncIOs"
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#process-based-version",
    "href": "posts/py-concurrency/py-concurrency.html#process-based-version",
    "title": "Python Concurrency",
    "section": "Process-Based Version",
    "text": "Process-Based Version\nThe examples so far have run on a single CPU. The multiprocessing module was designed to break down that barrier and run your code across multiple CPUs. It does this by creating a new instance of the Python interpreter to run on each CPU. Bringing up a separate Python interpreter is a heavyweight operation. Unlike the previous approaches, using multiprocessing allows you to take full advantage of the all CPUs that your computer has. Here‚Äôs the sample code:\n\n\n\nListing¬†4: io_processes.py\n\n\nimport atexit\nimport multiprocessing\nimport time\nfrom concurrent.futures import ProcessPoolExecutor\nimport requests\n\nsession: requests.Session\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with ProcessPoolExecutor(initializer=init_process) as executor:\n        executor.map(download_site, sites)\n\ndef download_site(url):\n    with session.get(url) as response:\n        name = multiprocessing.current_process().name\n        print(f\"{name}:Read {len(response.content)} bytes from {url}\")\n\ndef init_process():\n    global session\n    session = requests.Session()\n    atexit.register(session.close)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThis looks similar to the multi-threaded example.\nHere‚Äôs what this code does:\n\nA global variable is declared to hold the session object.\nThreadPoolExecutor is replaced with ProcessPoolExecutor from concurrent.futures and init_process() is passed.\nA custom initializer function is defined that each process will call shortly after starting.\nA cleanup function is registered with atexit.\n\nThe pool creates a number of separate Python interpreter processes. The communication between the main process and the other processes is handled. The pool instance doesn‚Äôt specify how many processes to create. By default, it‚Äôll determine the number of CPUs and match that. For an I/O-bound problem, increasing the number of processes won‚Äôt make things faster.\nNote: If you need to exchange data between your processes, then it requires expensive inter-process communication (IPC) and data serialization.\nEach process in the pool has its own memory space. They can‚Äôt easily share things like a session object. The initializer function parameter is built for this case. A global session variable can be initialized to hold the single session for each process.\nHere‚Äôs the output:\n$ python io_processes.py\nForkProcess-3:Read 10966 bytes from https://www.jython.org\nForkProcess-4:Read 276 bytes from http://olympus.realpython.org/dice\n ‚ãÆ\nDownloaded 160 sites in 3.428215079999063 seconds\nOn a computer with four CPU cores, it runs about four times faster than the synchronous version. It‚Äôs slower than the multi-threaded version and much slower than the asynchronous version.\nThe execution timing diagram for this code looks like this:\n\n\n\nTiming Diagram of a Multiprocessing Solution\n\n\nThere are separate processes executing in parallel. The diagrams of each one resemble the non-concurrent version. Multiprocessing is more useful for CPU-bound examples."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#async-and-await-keywords",
    "href": "posts/py-concurrency/py-concurrency.html#async-and-await-keywords",
    "title": "Python Concurrency",
    "section": "async and await Keywords",
    "text": "async and await Keywords\nThe async and await keywords are fundamental to asyncio.\n\nasync def: Defines a coroutine, a function that can be suspended and resumed.\nawait: Pauses execution inside a coroutine until an awaitable object (another coroutine or a Future) completes. When a coroutine encounters await, it yields control back to the event loop.\n\nasync def f(x):\n    y = await z(x)  # OK - `await` and `return` allowed in coroutines\n    return y\n\nasync def g(x):\n    yield x  # OK - this is an async generator\nA function defined with async def is a coroutine. It may use await, return, or yield, but all are optional.\nUsing await and/or return creates a coroutine function. To call a coroutine function, you must await it to get its results. It is less common to use yield in an async def block. This creates an asynchronous generator, iterated over with async for. Anything defined with async def may not use yield from, which will raise a SyntaxError.\nExample:\nimport asyncio\n\nasync def my_coroutine():\n    print(\"Coroutine started\")\n    await asyncio.sleep(1)  # Simulate an I/O operation\n    print(\"Coroutine finished\")\n\nasync def main():\n    await my_coroutine()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nThis code demonstrates:\n\nmy_coroutine is defined as an async function, making it a coroutine.\nawait asyncio.sleep(1) pauses the coroutine for 1 second, simulating an I/O operation. The event loop can schedule other coroutines.\nasyncio.run(main()) starts the event loop and runs the main coroutine."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asynchronous-vs.-synchronous-execution-in-python-a-comparison",
    "href": "posts/py-concurrency/py-concurrency.html#asynchronous-vs.-synchronous-execution-in-python-a-comparison",
    "title": "Python Concurrency",
    "section": "Asynchronous vs.¬†Synchronous Execution in Python: A Comparison",
    "text": "Asynchronous vs.¬†Synchronous Execution in Python: A Comparison\nThis section compares the execution time of asynchronous and synchronous code in Python using a simple example. The example consists of a count function that prints ‚ÄúOne‚Äù, waits for 1 second, and then prints ‚ÄúTwo‚Äù. We execute this function three times, once using asyncio for asynchronous execution and once using a simple for loop for synchronous execution.\nAsynchronous Example\n\n\n\nListing¬†5: countasync.py\n\n\nimport asyncio\nimport time\n\nasync def count():\n    print(\"One\")\n    await asyncio.sleep(1)\n    print(\"Two\")\n\nasync def main():\n    await asyncio.gather(count(), count(), count())\n\nif __name__ == \"__main__\":\n    s = time.perf_counter()\n    asyncio.run(main())\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\n\n\nExplanation:\n\nasync def count(): defines an asynchronous function. The await asyncio.sleep(1) line allows the event loop to switch to other tasks while waiting for the sleep to complete, enabling concurrency.\nasync def main(): defines another asynchronous function that uses asyncio.gather() to run three instances of count() concurrently. asyncio.gather() ensures that all provided awaitables complete before it returns.\nasyncio.run(main()) starts the asyncio event loop and runs the main() function.\n\nOutput:\n$ python countasync.py\nOne\nOne\nOne\nTwo\nTwo\nTwo\ncountasync.py executed in 1.01 seconds.\nAs observed, the execution time is approximately 1 second. This is because all three count() functions are executed concurrently. The asyncio.sleep(1) calls allow the event loop to switch between the functions, effectively overlapping the wait times. The small amount over 1 second likely represents overhead from the asyncio event loop and the print function.\nSynchronous Example\n\n\n\nListing¬†6: countsync.py\n\n\nimport time\n\ndef count():\n    print(\"One\")\n    time.sleep(1)\n    print(\"Two\")\n\ndef main():\n    for _ in range(3):\n        count()\n\nif __name__ == \"__main__\":\n    s = time.perf_counter()\n    main()\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\n\n\nExplanation:\n\ndef count(): defines a regular synchronous function.\ntime.sleep(1) pauses the execution of the current thread for 1 second. No other code can execute during this time.\ndef main(): calls the count() function three times in a loop. Each call blocks until it completes.\n\nOutput:\n$ python3 countsync.py\nOne\nTwo\nOne\nTwo\nOne\nTwo\ncountsync.py executed in 3.01 seconds.\nAs observed, the execution time is approximately 3 seconds. This is because each count() function call takes 1 second of sleep time, and the functions are executed sequentially."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#components-of-asyncio",
    "href": "posts/py-concurrency/py-concurrency.html#components-of-asyncio",
    "title": "Python Concurrency",
    "section": "Components of asyncio",
    "text": "Components of asyncio\n\nEvent Loop: Manages coroutine execution and I/O events. asyncio.run() creates and manages it.\nCoroutines: Functions defined with async def that can be suspended and resumed.\nTasks: Represent a coroutine scheduled to run in the event loop, created using asyncio.create_task(). Tasks are essential for running coroutines concurrently.\nFutures: Represent the result of an asynchronous operation. Coroutines may await Futures, allowing them to wait for the completion of an asynchronous operation without blocking the entire program.\nQueues: asyncio provides queue classes (asyncio.Queue) similar to the standard queue.Queue but designed for asynchronous use. Queues are useful for coordinating communication between coroutines.\n\nThe Event Loop\nThe event loop monitors coroutines, identifies idle coroutines, and schedules executable tasks. It wakes up idle coroutines when their awaited resources become available.\nasyncio.run() manages the event loop implicitly:\nasyncio.run() obtains the event loop, runs tasks until completion, and then closes the loop.\nKey points about the event loop:\n\nCoroutines require the event loop to execute.\nBy default, an AsyncIO event loop runs in a single thread and on a single CPU core, which is often sufficient.\n\nThere‚Äôs a more long-winded way of managing the asyncio event loop, with get_event_loop(). The typical pattern looks like this and is taken from llama-index async_utils.py.\n\n\n\nListing¬†7: asyncio_run\n\n\ndef asyncio_run(coro: Coroutine) -&gt; Any:\n    \"\"\"Gets an existing event loop to run the coroutine.\n\n    If there is no existing event loop, creates a new one.\n    \"\"\"\n    try:\n        # Check if there's an existing event loop\n        loop = asyncio.get_event_loop()\n\n        # If we're here, there's an existing loop but it's not running\n        return loop.run_until_complete(coro)\n\n    except RuntimeError as e:\n        # If we can't get the event loop, we're likely in a different thread, or its already running\n        # asyncio.get_event_loop() raises RuntimeError if there's no running loop in the current thread.\n        try:\n            # If the first attempt failed, try creating and running a new event loop.\n            return asyncio.run(coro)  # asyncio.run() creates a new event loop, runs the coroutine, and closes the loop.\n        except RuntimeError as e:\n            # If asyncio.run() also raises RuntimeError, it likely means there's a nested asyncio call.\n            raise RuntimeError(\n                \"Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.\"\n                \"Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\"\n            )\n\n\n\nThis function aims to run an asyncio coroutine. It first tries to get the current event loop. If one exists and is not running, it runs the coroutine in that loop. If getting the current loop fails (RuntimeError), it means there‚Äôs no running loop in the current thread. In that case, it tries to create a new loop using asyncio.run(). If creating a new loop also fails (another RuntimeError), it likely indicates a nested asyncio scenario (trying to run asyncio code within already running asyncio code). The error suggests using nest_asyncio or asynchronous alternatives to avoid nested loops. Nesting is generally discouraged in asyncio. nest_asyncio allows nesting, but it can lead to unexpected behavior. It is generally recommended to re-factor code that uses nested async calls."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asyncio-examples-in-practice",
    "href": "posts/py-concurrency/py-concurrency.html#asyncio-examples-in-practice",
    "title": "Python Concurrency",
    "section": "AsyncIO Examples in Practice",
    "text": "AsyncIO Examples in Practice\n\nChained Coroutines\n\n\n\nListing¬†8: chained.py\n\n\nimport asyncio\nimport random\nimport time\n\nasync def part1(n: int) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part1({n}) sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-1\"\n    print(f\"Returning part1({n}) == {result}.\")\n    return result\n\nasync def part2(n: int, arg: str) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part2{n, arg} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-2 derived from {arg}\"\n    print(f\"Returning part2{n, arg} == {result}.\")\n    return result\n\nasync def chain(n: int) -&gt; None:\n    start = time.perf_counter()\n    p1 = await part1(n)\n    p2 = await part2(n, p1)\n    end = time.perf_counter() - start\n    print(f\"--&gt;Chained result{n} =&gt; {p2} (took {end:0.2f} seconds).\")\n\nasync def main(*args):\n    await asyncio.gather(*(chain(n) for n in args))\n\nif __name__ == \"__main__\":\n    import sys\n    random.seed(444)\n    args = [1, 2, 3] if len(sys.argv) == 1 else map(int, sys.argv[1:])\n    start = time.perf_counter()\n    asyncio.run(main(*args))\n    end = time.perf_counter() - start\n    print(f\"Program finished in {end:0.2f} seconds.\")\n\n\n\nExample output:\n$ python3 chained.py 9 6 3\npart1(9) sleeping for 4 seconds.\npart1(6) sleeping for 4 seconds.\npart1(3) sleeping for 0 seconds.\nReturning part1(3) == result3-1.\npart2(3, 'result3-1') sleeping for 4 seconds.\nReturning part1(9) == result9-1.\npart2(9, 'result9-1') sleeping for 7 seconds.\nReturning part1(6) == result6-1.\npart2(6, 'result6-1') sleeping for 4 seconds.\nReturning part2(3, 'result3-1') == result3-2 derived from result3-1.\n--&gt;Chained result3 =&gt; result3-2 derived from result3-1 (took 4.00 seconds).\nReturning part2(6, 'result6-1') == result6-2 derived from result6-1.\n--&gt;Chained result6 =&gt; result6-2 derived from result6-1 (took 8.01 seconds).\nReturning part2(9, 'result9-1') == result9-2 derived from result9-1.\n--&gt;Chained result9 =&gt; result9-2 derived from result9-1 (took 11.01 seconds).\nProgram finished in 11.01 seconds.\npart1() sleeps for a variable time, and part2() starts as results become available. This example demonstrates how asyncio.gather can run multiple coroutines concurrently, and how await ensures that the second coroutine in each chain only runs after the first has completed. Each chain function represents an independent, sequential process. The final result showcases the execution time of individual chains and the total program execution time. Note that the total program time is roughly the longest individual chain time, demonstrating concurrency.\n\n\nProducer/Consumer Pattern with asyncio.Queue\n\n\n\nListing¬†9: asyncq.py\n\n\nimport asyncio\nimport itertools as it\nimport os\nimport random\nimport time\n\nasync def makeitem(size: int = 5) -&gt; str:\n    return os.urandom(size).hex()\n\nasync def randsleep(caller=None) -&gt; None:\n    i = random.randint(0, 10)\n    if caller:\n        print(f\"{caller} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n\nasync def produce(name: int, q: asyncio.Queue) -&gt; None:\n    n = random.randint(0, 10)\n    for _ in it.repeat(None, n):  # Synchronous loop for each single producer\n        await randsleep(caller=f\"Producer {name}\")\n        i = await makeitem()\n        t = time.perf_counter()\n        await q.put((i, t))\n        print(f\"Producer {name} added &lt;{i}&gt; to queue.\")\n\nasync def consume(name: int, q: asyncio.Queue) -&gt; None:\n    while True:\n        await randsleep(caller=f\"Consumer {name}\")\n        i, t = await q.get()\n        now = time.perf_counter()\n        print(f\"Consumer {name} got element &lt;{i}&gt;\"\n              f\" in {now-t:0.5f} seconds.\")\n        q.task_done()\n\nasync def main(nprod: int, ncon: int):\n    q = asyncio.Queue()\n    producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)]\n    consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)]\n    await asyncio.gather(*producers)\n    await q.join()  # Implicitly awaits consumers, too\n    for c in consumers:\n        c.cancel()\n\nif __name__ == \"__main__\":\n    import argparse\n    random.seed(444)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--nprod\", type=int, default=5)\n    parser.add_argument(\"-c\", \"--ncon\", type=int, default=10)\n    ns = parser.parse_args()\n    start = time.perf_counter()\n    asyncio.run(main(**ns.__dict__))\n    elapsed = time.perf_counter() - start\n    print(f\"Program completed in {elapsed:0.5f} seconds.\")\n\n\n\nOutput\n$ python3 asyncq.py -p 2 -c 2\nProducer 0 sleeping for 2 seconds.\nProducer 1 sleeping for 2 seconds.\nConsumer 0 sleeping for 2 seconds.\nConsumer 1 sleeping for 4 seconds.\nProducer 0 added &lt;50679576b9&gt; to queue.\nProducer 0 sleeping for 3 seconds.\nProducer 1 added &lt;413dd61b37&gt; to queue.\nConsumer 0 got element &lt;50679576b9&gt; in 0.00015 seconds.\nConsumer 0 sleeping for 1 seconds.\nProducer 1 added &lt;1b64e8bb2f&gt; to queue.\nConsumer 1 got element &lt;413dd61b37&gt; in 0.00012 seconds.\nConsumer 1 sleeping for 5 seconds.\nProducer 1 added &lt;413dd61b37&gt; to queue.\nConsumer 0 got element &lt;1b64e8bb2f&gt; in 0.00009 seconds.\nConsumer 0 sleeping for 5 seconds.\nProgram completed in 7.02420 seconds.\nThis example illustrates the producer-consumer pattern using asyncio.Queue. Multiple producers generate random items and place them in the queue, while multiple consumers retrieve items from the queue and process them. asyncio.Queue handles synchronization between producers and consumers, ensuring that consumers don‚Äôt try to retrieve items from an empty queue and that producers don‚Äôt overwhelm the consumers. The q.join() method ensures that the main function waits until all items in the queue have been processed before canceling the consumers and exiting. The producers are stopped when they are finished producing items, while the consumers are stopped after the queue is emptied, preventing an infinite loop. The time elapsed between item production and consumption is measured, demonstrating the efficiency of the asynchronous queue in handling concurrent operations.\n\n\nSome Utility Functions from Llama-Index for Handling Asynchronous Tasks\nThis section presents three utility functions from the llama-index library that help manage asynchronous tasks in different ways:\n\nA function to execute a list of async tasks (with optional progress bar)\nA function to run tasks in batches to avoid memory issues\nA function to run tasks with a limited number of workers\n\nThese functions build on the core asyncio concepts covered earlier and provide practical solutions for common async programming needs.\n\n\n\nListing¬†10: llamaindex_async_utils.py\n\n\ndef run_async_tasks(\n    tasks: List[Coroutine],\n    show_progress: bool = False,\n    progress_bar_desc: str = \"Running async tasks\",\n) -&gt; List[Any]:\n    \"\"\"Run a list of async tasks.\"\"\"\n    tasks_to_execute: List[Any] = tasks\n    if show_progress:\n        try:\n            import nest_asyncio\n            from tqdm.asyncio import tqdm\n\n            # jupyter notebooks already have an event loop running\n            # we need to reuse it instead of creating a new one\n            nest_asyncio.apply()\n            loop = asyncio.get_event_loop()\n\n            async def _tqdm_gather() -&gt; List[Any]:\n                return await tqdm.gather(*tasks_to_execute, desc=progress_bar_desc)\n\n            tqdm_outputs: List[Any] = loop.run_until_complete(_tqdm_gather())\n            return tqdm_outputs\n        # run the operation w/o tqdm on hitting a fatal\n        # may occur in some environments where tqdm.asyncio\n        # is not supported\n        except Exception:\n            pass\n\n    async def _gather() -&gt; List[Any]:\n        return await asyncio.gather(*tasks_to_execute)\n\n    outputs: List[Any] = asyncio_run(_gather())\n    return outputs\n\n\n\nThis function runs a list of asynchronous tasks concurrently using asyncio.gather().\n\nIt takes a list of coroutines (tasks), a boolean to show progress, and an optional progress bar description.\nIf show_progress is True, it attempts to use tqdm.asyncio to display a progress bar.\n\nIt uses nest_asyncio.apply() to handle cases where the code is running in an environment like Jupyter notebooks, which already have an event loop.\nIt defines an inner coroutine _tqdm_gather() that uses tqdm.gather() to run the tasks and display the progress.\nIt runs _tqdm_gather() using asyncio_run().\nIf any exception occurs during the tqdm process (e.g., tqdm.asyncio is not supported), it falls back to running the tasks without a progress bar.\n\nIf show_progress is False or the tqdm process fails, it defines an inner coroutine _gather() that uses asyncio.gather() to run the tasks concurrently.\nIt runs _gather() using asyncio_run().\nIt returns a list of the results from the completed tasks.\n\n\n\n\nListing¬†11: llamaindex_async_utils.py\n\n\ndef chunks(iterable: Iterable, size: int) -&gt; Iterable:\n    args = [iter(iterable)] * size\n    return zip_longest(*args, fillvalue=None)\n\nasync def batch_gather(\n    tasks: List[Coroutine], batch_size: int = 10, verbose: bool = False\n) -&gt; List[Any]:\n    output: List[Any] = []\n    for task_chunk in chunks(tasks, batch_size):\n        task_chunk = (task for task in task_chunk if task is not None)\n        output_chunk = await asyncio.gather(*task_chunk)\n        output.extend(output_chunk)\n        if verbose:\n            print(f\"Completed {len(output)} out of {len(tasks)} tasks\")\n    return output\n\n\n\nThe function chunks splits an iterable into chunks of a specified size.\n\nIt takes an iterable and the desired chunk size as input.\nIt creates multiple iterators from the input iterable using [iter(iterable)] * size.\nIt uses zip_longest() to group elements from these iterators into tuples, effectively creating the chunks.\nThe fillvalue=None argument ensures that shorter chunks are padded with None if the input iterable is not evenly divisible by the chunk size.\nThe function returns an iterator that yields the chunks. For example, chunks([1, 2, 3, 4, 5, 6, 7], 3) will return [(1, 2, 3), (4, 5, 6), (7, None, None)]\n\nThe function batch_gather runs a list of asynchronous tasks in batches.\n\nIt takes a list of coroutines (tasks), a batch size, and a boolean for verbose output.\nIt uses the chunks() function to split the tasks into batches.\nFor each batch (task_chunk), it filters out any None values (which might have been added by chunks() due to padding).\nIt uses asyncio.gather() to run the tasks in the current batch concurrently.\nThe results from each batch are added to the output list.\nIf verbose is True, it prints a message indicating how many tasks have been completed so far.\nThe function returns a list of the results from all the completed tasks. This is useful when you have a very large list of coroutines, as sending them all to asyncio.gather could result in memory errors.\n\n\n\n\nListing¬†12: llamaindex_async_utils.py\n\n\nasync def run_jobs(\n    jobs: List[Coroutine[Any, Any, T]],\n    show_progress: bool = False,\n    workers: int = DEFAULT_NUM_WORKERS,\n    desc: Optional[str] = None,\n) -&gt; List[T]:\n    \"\"\"Run jobs.\n\n    Args:\n        jobs (List[Coroutine]):\n            List of jobs to run.\n        show_progress (bool):\n            Whether to show progress bar.\n\n    Returns:\n        List[Any]:\n            List of results.\n    \"\"\"\n    semaphore = asyncio.Semaphore(workers)\n\n    async def worker(job: Coroutine) -&gt; Any:\n        async with semaphore:\n            return await job\n\n    pool_jobs = [worker(job) for job in jobs]\n\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n        results = await tqdm_asyncio.gather(*pool_jobs, desc=desc)\n    else:\n        results = await asyncio.gather(*pool_jobs)\n\n    return results\n\n\n\nThis function runs a list of asynchronous jobs (coroutines) with a specified number of workers and optional progress tracking.\n\nIt uses a semaphore to limit the number of concurrent workers.\nIt defines an inner coroutine worker() that acquires the semaphore before running a job and releases it afterward, ensuring that only a limited number of jobs run concurrently.\nIt creates a list of worker tasks (pool_jobs) by mapping each job to the worker() coroutine.\nIf show_progress is True, it uses tqdm_asyncio.gather() to run the worker tasks and display a progress bar.\nIf show_progress is False, it uses asyncio.gather() to run the worker tasks concurrently without a progress bar.\nThe function returns a list of the results from the completed jobs. This is useful when you want to limit the number of concurrent asyncio tasks being run, perhaps because of rate limiting by an API, or resource constraints on your system."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#considerations-for-using-asyncio",
    "href": "posts/py-concurrency/py-concurrency.html#considerations-for-using-asyncio",
    "title": "Python Concurrency",
    "section": "Considerations for Using asyncio",
    "text": "Considerations for Using asyncio\nasyncio is suitable for I/O-bound tasks:\n\nNetwork applications: web servers, chat applications, API clients.\nWeb scraping: fetching data from multiple websites concurrently.\nReal-time applications: handling asynchronous events and data streams.\n\nIt is not appropriate for CPU-bound tasks like complex calculations or image processing. For CPU-bound tasks, consider multiprocessing.\nAdvantages of asyncio\n\nEfficiency: Reduces blocking and increases resource utilization.\nScalability: Handles concurrent connections with limited overhead.\nConcurrency with a single thread: Reduces multithreading complexities."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html",
    "href": "posts/pdf-rag/pdf-rag.html",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "",
    "text": "In this blog post, we focus on RAG (Retrieval-Augmented Generation) specifically for PDF documents. We will study how PDFs can be processed for use with Language Models (LLMs). The code relies on llama-index package which is versatile but evolving rapidly and unstable.\nPDFs are ubiquitous in businesses and throughout the human world. What‚Äôs remarkable is the sheer diversity of PDF types we encounter. These range from reports of just a few pages to documents spanning hundreds of pages, generally including a table of contents. We also frequently deal with slide decks, or presentations, which are common in corporate settings. These can vary from just a few slides to dozens or even hundreds, often with their own unique tables of contents and organizational structures.\nIt‚Äôs fascinating to consider how differently humans and computers process this information. Humans, when facing lengthy documents, generally tend to look at the table of contents and assess the document‚Äôs overall structure to identify areas of interest.\nHowever, machines, especially with page-by-page parsing, often lose the document‚Äôs overall structure. One way to address this is to use embeddings and chunking, attempting to find similarities through embeddings or semantic similarity to identify potentially relevant chunks of information.\nThis highlights the core challenge of the method: the need to process data differently as a human than as a machine, particularly to gain a holistic view of the problem.\nIn Section¬†1, we delve into PDF metadata and structure extraction. We explore the challenges and limitations of raw conversion and the necessity of reformatting to improve document structure. We also discuss the parsing of document structure to create a hierarchical index for efficient document retrieval. In Section¬†2, we implement a multi-document PDF RAG system based on LlamaIndex, focusing on query-based document retrieval and generation.\nSome important resources for this blog post include:\nAll code paths referenced in this post are relative to the pdf-rag GitHub Repo."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-markdown",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-markdown",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF to Markdown Conversion",
    "text": "PDF to Markdown Conversion\nFor PDF to Markdown conversion, we refer to the related blog post PDF parsing for LLM Input which describes different methods to parse PDFs for LLM input.\nFor RAG applications with PDFs, we‚Äôll focus on using Vision Language Models (VLMs) for PDF-to-Markdown conversion. As discussed in the related blog post PDF parsing for LLM Input, VLMs offer a straightforward and effective solution. While the approach is more expensive than basic text extraction, it provides valuable capabilities like image description integration. Similar to LlamaParse but with more flexibility, our implementation allows:\n\nFull control over model selection\nChoice of different API providers\nEnd-to-end process management\n\nThe code implements a VLMPDFReader class (in src/pdf_rag/readers.py) that uses Vision Language Models (VLMs) to convert PDF documents to markdown format. Here‚Äôs how it works.\nKey Features\n\nUses both Gemini and Mistral models for processing.\nSupports both single PDFs and batch processing.\nIncludes a caching mechanism to avoid reprocessing.\nHandles API rate limiting and retries.\nProcesses PDFs page by page with parallel execution.\n\nArchitecture\nThe conversion process follows these steps:\n\nInitialization\n\nSets up API clients for Gemini and Mistral.\nValidates API keys and cache directory.\nLoads conversion template.\n\nPDF Processing\n\nLoads PDF file using PdfReader.\nProcesses each page individually.\nConverts PDF pages to images for VLM processing.\n\nVLM Processing Pipeline\n\nFirst attempts conversion with Gemini.\nFalls back to Mistral if Gemini returns ‚ÄúRECITATION‚Äù.\nUses a template to guide the conversion.\n\nOutput Generation\n\nCombines processed pages.\nAdds page markers for reference.\nCaches results to avoid reprocessing.\nReturns document with metadata.\n\n\nUsage Example\nreader = VLMPDFReader(\n    cache_dir=\"./cache\",\n    api_key_gemini=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    api_key_mistral=\"your_mistral_key\"  # or use env var MISTRAL_API_KEY\n)\n\n# Single file\ndoc = reader.load_data(\"path/to/document.pdf\")\n\n# Multiple files\ndocs = reader.load_data([\"doc1.pdf\", \"doc2.pdf\"])\nWhy do we use Gemini and Mistral models ?\nThe ‚ÄúRECITATION‚Äù Error in Gemini API\nThe RECITATION error in the Gemini API occurs when the model detects that it is generating content that closely resembles its training data, particularly copyrighted material. This error is designed to prevent the model from reproducing protected content verbatim. The issue arises unpredictably and can halt content generation mid-stream, leading to incomplete responses.\nIn this case of failure, we fall back to the Mistral model to parse the problematic pdf page.\nOverall, the RECITATION error is a significant challenge for developers using the Gemini API, highlighting the tension between content safety and usability\nVLMPDFReader is designed to be robust and efficient, with built-in caching and parallel processing capabilities to handle large documents or multiple files efficiently.\nHere is the associated template prompt for the VLM.\n\n\nClick to view template\n\nYou are a specialized document transcription assistant converting PDF documents to Markdown format.\nYour primary goal is to create an accurate, complete, and well-structured Markdown representation.\n\n&lt;instructions&gt;\n1. Language and Content:\n   - MAINTAIN the original document language throughout ALL content\n   - ALL elements (headings, tables, descriptions) must use source language\n   - Preserve language-specific formatting and punctuation\n   - Do NOT translate any content\n\n2. Text Content:\n   - Convert all text to proper Markdown syntax\n   - Use appropriate heading levels (# ## ###)\n   - Preserve emphasis (bold, italic, underline)\n   - Convert bullet points to Markdown lists (-, *, +)\n   - Maintain original document structure and hierarchy\n\n3. Visual Elements (CRITICAL):\n   a. Tables:\n      - MUST represent ALL data cells accurately in original language\n      - Use proper Markdown table syntax |---|\n      - Include header rows\n      - Add caption above table: [Table X: Description] in document language\n\n   b. Charts/Graphs:\n      - Create detailed tabular representation of ALL data points\n      - Include X/Y axis labels and units in original language\n      - List ALL data series names as written\n      - Add caption: [Graph X: Description] in document language\n\n   c. Images/Figures:\n      - Format as: ![Figure X: Detailed description](image_reference)\n      - Describe key visual elements in original language\n      - Include measurements/scales if present\n      - Note any text or labels within images\n\n4. Quality Requirements:\n   - NO content may be omitted\n   - Verify all numerical values are preserved\n   - Double-check table column/row counts match original\n   - Ensure all labels and legends are included\n   - Maintain document language consistently throughout\n\n5. Structure Check:\n   - Begin each section with clear heading\n   - Use consistent list formatting\n   - Add blank lines between elements\n   - Preserve original content order\n   - Verify language consistency across sections\n&lt;/instructions&gt;\n\n\nAn example of raw conversion is shown in Figure¬†1.\n\n\n\n\n\n\nFigure¬†1: Raw conversion from PDF to Markdown using VLM\n\n\n\nWe also have a PDFDirectoryReader class (in src/pdf_rag/readers.py) to provide batch processing capabilities for PDF files within a directory structure. It acts as a wrapper around the VLMPDFReader, adding directory handling and metadata extraction features.\nKey Features\n\nDirectory-based Processing: Handles both single PDF files and directories containing multiple PDFs\nMetadata Extraction: Automatically extracts and includes file metadata like:\n\nCreation date\nLast modified date\nFile size\nFile type\nRelative path information\n\nConfiguration Management:\n\nValidates directory paths and creates cache directories\nManages API keys for both Gemini and Mistral models\nSupports environment variable configuration\n\nParallel Processing: Configurable number of workers for concurrent processing\n\nUsage Example\nreader = PDFDirectoryReader(\n    root_dir=\"./documents\",\n    cache_dir=\"./cache\",\n    num_workers=4,\n    show_progress=True\n)\n\n# Process single PDF file\ndocs = reader.load_data(\"documents/sample.pdf\")\n\n# Process directory of PDFs\ndocs = reader.load_data(\"documents/reports/\")\n\n# Async processing\ndocs = await reader.aload_data(\"documents/reports/\")\nDifficulties and Drawbacks of Raw VLM Conversion\nThe raw conversion process has several limitations and drawbacks that can affect the quality and accuracy of the output. One of the main difficulties in PDF parsing is that it is generally done page by page, causing the loss of the overall document structure. We lose the appropriate headings that help retrieve the general structure of the document, and we end up with a very local and focused view. For example, when converting PDF to markdown, the heading structure specified by the markdown is generally incorrect - the headers and their levels don‚Äôt correspond to what is expected."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-reformatting",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-reformatting",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF Reformatting",
    "text": "PDF Reformatting\nThe loss of document structure during raw conversion necessitates reformatting to improve the overall document organization and coherence. Reformatting involves restructuring the content to ensure proper hierarchy, headings, and formatting. This step is crucial for enhancing readability and navigation within the document. In practice, we use a Large Language Model (LLM) to reprocess the document chunk by chunk, given that some documents can be very long. The aim is to provide a more comprehensive overview. We‚Äôre using Gemini, which has a very large context, which makes this approach quite helpful. We are observing an improvement in the markdown formatting, particularly with better title identification. It‚Äôs not always perfect, but there‚Äôs a more effective segmentation based on titles and markdown, resulting in better adherence to the overall document structure compared to what we had before. This makes it easier to have a structure a bit more coherent when you cut through markdowns, especially in reports.\nThe ReformatMarkdownComponent class (in src/pdf_rag/transforms.py) reformats markdown content using the Gemini API. Here‚Äôs a detailed breakdown of its functionalities:\n\nConfiguration Options\n\nmax_iters: Maximum iterations for reformatting (default: 50)\nin_place: Whether to modify nodes directly or create copies (default: True)\nnum_workers: Number of concurrent workers (default: 4)\nshow_progress: Display progress during processing (default: True)\n\nCaching Mechanism\n\nStores reformatted content in .reformatted.md files\nAvoids reprocessing previously reformatted content\nUses file path and cache directory from node metadata\n\nProcessing Pipeline\n\nHandles individual nodes asynchronously\nUses Jinja2 templates for content reformatting\nSupports both landscape and portrait formats\nAccumulates reformatted content iteratively\n\n\nUsage Example\ncomponent = ReformatMarkdownComponent(\n    api_key=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    num_workers=4,\n    show_progress=True\n)\n\n# Process nodes\nprocessed_nodes = component(nodes)\n\n# Async processing\nprocessed_nodes = await component.acall(nodes)\nThe component is designed to improve document structure through:\n\nHeading Hierarchies: Ensures proper nesting and levels (H1 -&gt; H2 -&gt; H3)\nConsistent Formatting: Standardizes markdown syntax, lists, and spacing\nContent Preservation: Maintains original language, technical details, and metadata\nFormat Handling: Supports both portrait (reports) and landscape (presentations) layouts\nQuality Checks: Validates completeness and structure accuracy\n\nHere is the associated template prompt for Gemini.\n\n\nClick to view template prompt for Gemini\n\n&lt;document&gt;\n{{ document }}\n&lt;/document&gt;\n\n{% if processed %}\n&lt;processed&gt;\n{{ processed }}\n&lt;/processed&gt;\n{% endif %}\n\nYou are a professional technical documentation editor specializing in markdown documents.\nYour task is to transform the document into a well-structured markdown document with clear hierarchy and organization.\n\n&lt;instructions&gt;\n1. Content Preservation (CRITICAL):\n    - PRESERVE ALL original content without exception\n    - Do not summarize or condense any information\n    - Maintain all technical details, examples, and code snippets\n    - Keep all original links, references, and citations\n    - Preserve all numerical data and specifications\n    {% if landscape %}\n    - Preserve `---end page ...` markers\n    {% endif %}\n\n2. Document Structure:\n    - Ensure exactly one H1 (#) title at the start\n    - Use maximum 3 levels of headers (H1 -&gt; H2 -&gt; H3)\n    - Avoid excessive nesting - prefer flatter structure\n    - Group related sections under appropriate headers\n    - If an existing TOC is present, maintain and update it\n    - Only create new TOC if none exists\n\n3. Formatting Standards:\n    - Use consistent bullet points/numbering\n    - Format code blocks with appropriate language tags\n    - Properly format links and references\n    - Use tables where data is tabular\n    - Include blank lines between sections\n\n4. Quality Checks:\n    - Compare final document with original for completeness\n    - Verify all technical information is preserved\n    - Ensure all examples remain intact\n    - Maintain all nuances and specific details\n\n5. Metadata & Front Matter:\n    - Include creation/update dates if present\n    - Preserve author information\n    - Maintain any existing tags/categories\n&lt;/instructions&gt;\n\n{% if processed %}\nPlease continue reformatting from where it was left off, maintaining consistency with the processed portion.\nEnsure NO content is omitted - preserve everything from the original document.\nAll sections should seamlessly integrate with the existing structure.\nEnd your response with &lt;end&gt;.\n{% else %}\nProvide the complete reformatted document following the above guidelines.\nWARNING: Do not omit ANY content - preserve everything from the original document.\nEnsure all sections are properly nested and formatted.\nEnd your response with &lt;end&gt;.\n{% endif %}\n\n\nPDFs can be either portrait-formatted reports or landscape-formatted slides. In both cases, the PDF is processed page by page, resulting in the loss of the overall document structure. However, the processing approach needs to differ depending on whether we‚Äôre dealing with slides or reports.\nFor slides (landscape format):\n\nHave a clear page-by-page structure\nEach slide is a discrete unit\nPage markers are essential for maintaining structure\n\nFor reports (portrait format):\n\nDo not rely on page breaks\nHave a clear heading hierarchy\nStructure comes from nested headers\n\nNote that the template prompt has a conditional part based on format:\n{% if landscape %}\n- Preserve `---end page ...` markers\n{% endif %}\nAn example of Markdown reformatting is shown in Figure¬†2.\n\n\n\n\n\n\nFigure¬†2: Markdown reformatting using LLM"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-metadata",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-metadata",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF Metadata Extraction",
    "text": "PDF Metadata Extraction\nOnce we‚Äôve converted and reformatted the PDF content into markdown format using Vision Language Models (VLMs), the next step is extracting valuable metadata from the documents. This section explores a series of specialized extractors built on the LlamaIndex framework, each designed to capture specific aspects of PDF documents:\n\nDocument context (author, date, language)\nTable of contents (both extraction and creation)\nPage structure (especially for presentations)\nDocument hierarchy and relationships\n\nThese extractors work together to create a comprehensive metadata layer that enhances document searchability and understanding.\nAll the extractors are defined in the file src/pdf_rag/extractors.py. Note that as experienced with reformatting markdown in Section¬†1.2, landscape-based vs portrait-based PDFs require different processing approaches. The extractors are designed to handle these differences effectively.\nThe GeminiBaseExtractor serves as the foundation for all extractors, providing:\n\nGemini API integration with configurable temperature and model selection\nAPI key validation and management\nAbstract interface for extraction operations\nParallel processing capabilities\n\n\n1. ContextExtractor\nExtracts contextual information from documents:\n\nProcesses document content using a specialized template\nReturns structured JSON with document context\nHandles parallel processing of multiple nodes\n\n\n\nClick to view template prompt for Gemini\n\n&lt;document&gt;\n{{ document }}\n&lt;/document&gt;\n\nPlease analyze the above document and provide output in the following JSON format:\n\n{\n    \"author\": \"detected_author\",\n    \"publication_date\": \"YYYY-MM-DD\",\n    \"language\": \"detected_language in ISO 639-1 language code format\",\n    \"document_type\": \"type_if_identifiable\"\n    \"themes\": [\n        {\n            \"name\": \"Theme name\",\n            \"description\": \"Brief explanation\"\n        }\n    ],\n    \"entities\": [\n            {\n                \"name\": \"Entity name\",\n                \"role\": \"Role/significance\"\n            }\n    ],\n    \"time_periods\": [\n        {\n            \"start_date\": \"YYYY-MM-DD\",\n            \"end_date\": \"YYYY-MM-DD\",\n            \"period_description\": \"Description of events/developments in this timeframe\",\n            \"is_approximate\": boolean,\n        }\n    ],\n    \"keywords\": [\n        \"keyword1\",\n        \"keyword2\"\n    ],\n    \"summary\": \"Concise summary text focusing on main points\"\n}\n\nNote: Keep descriptions concise and factual.\nIf an item is missing, answer with \"\".\nAnswer in the language of the document.\n\n\n\n\n2. TableOfContentsExtractor\nExtracts existing tables of contents from documents:\n\nFocuses on the first 10 pages by default\nSupports both portrait and landscape formats\nReturns an empty string if no TOC is found\nUses a templated approach for extraction\n\n\n\nClick to view template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\n{% if format == 'landscape' %}\nExtract the table of contents (TOC) from the document if present and specify the page number where the table of contents is located.\nThe table of contents, if it exists, is located on a single page.\nNote that each page in the document ends with a marker '--- end page n' where n is the page number.\n\nOutput format:\n- If a table of contents exists:\n  First line: \"Table of contents found on page X\"\n  Following lines: Complete TOC with its original structure and hierarchy\n- If no table of contents exists: Respond with exactly \"&lt;none&gt;\"\n{% else %}\nExtract the table of contents (TOC) from the document if it exists.\nIMPORTANT: the table of contents must be contained in consecutive lines in the source document itself.\n\nOutput format:\n- If a table of contents exists: complete TOC with its original structure and hierarchy\n- If no table of contents exists: Respond with exactly \"&lt;none&gt;\"\n{% endif %}\n\n\n\n\n3. TableOfContentsCreator\nCreates new tables of contents:\n\nPortrait Mode:\n\nAnalyzes markdown headers\nIncludes line numbers for reference\nRequires reformatted content\n\nLandscape Mode:\n\nGenerates TOC using Gemini\nIncludes a validation step\nTwo-phase process: draft and check\n\n\n\n\nClick to view draft template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\nGenerate a hierarchical table of contents for the slides deck above by:\n\n1. IDENTIFY SECTION BREAKS\n- Section breaks are marked by \"--- end page {n}\" where n is the page number\n\n2. EXTRACT SECTION INFO\n- Get the title text from each section break page\n- Record the corresponding page number\n- Validate that page numbers are unique and ascending\n\n3. FORMAT OUTPUT\nFormat each entry as:\n# {Section Title} (Page {n})\n\nExample output:\n# Introduction (Page 1)\n# Key Concepts (Page 5)\n# Implementation (Page 12)\n\nRequirements:\n- Page numbers must be unique and sequential\n- Ignore any formatting in the section titles\n\nThe TOC should help readers quickly navigate the main sections of the deck.\n\n\n\n\nClick to view check template prompt for Gemini\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\nGenerate a standardized table of contents following these rules:\n\n1. FORMAT REQUIREMENTS\n- Each entry: \"# {Title} (Page {n})\"\n- Page numbers must be integers in parentheses\n- One entry per line, no blank lines\n- Preserve original markdown formatting in titles\n- Page numbers ascending order\n\n2. VALIDATION\n- Reject duplicate page numbers\n- Reject duplicate titles\n- Page numbers must exist and be &gt; 0\n- Title cannot be empty\n\nExample valid output:\n# Executive Summary (Page 1)\n# Market Analysis (Page 3)\n# Financial Projections (Page 7)\n\n\n\n\n4. LandscapePagesExtractor\nSpecialized for landscape (presentation) documents:\n\nRequires an existing TOC (extracted or created)\nProcesses document content page by page\nUses template-based extraction\nReturns structured page information\n\n\n\nClick to view template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\nExtract and list all pages from the slides deck in &lt;doc&gt;, using the table of contents in &lt;toc&gt; as reference.\n\nRules:\n1. Format each line exactly as: Page N : [title]\n2. List pages in ascending numerical order (1, 2, 3...)\n3. When a page has no title, use the title from its preceding page\n4. Include all pages, even those without content\n\nExample:\nPage 1 : Introduction\nPage 2 : Market Analysis\nPage 3 : Market Analysis\nPage 4 : Key Findings\n\n\n\n\n5. StructureExtractor\nHandles document structure analysis:\n\nPortrait Mode:\n\nUses the created TOC for structure\nSimple structure representation\n\nLandscape Mode:\n\nCombines TOC and page information\nCreates a comprehensive structure\nRequires both TOC and page metadata\n\n\n\n\nClick to view template prompt for Gemini\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\n&lt;pages&gt;\n{{ pages }}\n&lt;/pages&gt;\n\nAnalyze the table of contents (TOC) in &lt;toc&gt; and the pages of the slides deck provided in &lt;pages&gt;.\nGroup the pages under their corresponding TOC sections using this format:\n\n# [TOC Section]\n- Page X : [Full Page Title]\n- Page Y : [Full Page Title]\n\nRules:\n- Each TOC section should have an appropriate level heading with #, ##, or ###.\n- List all pages that belong under each section\n- Maintain original page numbers and full titles\n- Include pages even if their titles are slightly different from TOC entries\n- Group subsections under their main section\n- List pages in numerical order within each section\n- Don't omit any pages\n- If a page doesn't clearly fit under a TOC section, place it under \"Other Pages\"\n\nExample:\n# Introduction\n- Page 1 : Welcome Slide\n- Page 2 : Project Overview\n\n# Key Findings\n- Page 3 : Financial Results\n- Page 4 : Market Analysis\n\n\nUsage Example\n# Initialize extractor\nextractor = TableOfContentsExtractor(\n    api_key=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    head_pages=10,\n    temperature=0.1\n)\n\n# Process nodes\nresults = await extractor.aextract(nodes)\nEach extractor is designed to work asynchronously and handle batch processing efficiently, with built-in error handling and progress reporting.\nTo visualize the metadata and structure of processed documents, you can:\n\nRun the ingestion pipeline using the script in src/scripts/metadata_structure.py (detailed in Section¬†1.5)\nView the results in the Python Shiny app at src/app/app-metadata.py\n\nA live demo is available on the HF Space Demo for PDF Metadata. The visualization interface is shown in Figure¬†3.\n\n\n\n\n\n\nFigure¬†3: HF space demo for metadata and structure extraction from PDF"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-parsing-structure",
    "href": "posts/pdf-rag/pdf-rag.html#sec-parsing-structure",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Parsing Structure",
    "text": "Parsing Structure\nIn this section, we focus on parsing the structure of extracted metadata to create a hierarchical index for efficient document retrieval. We distinguish between portrait (report) and landscape (presentation) structures and describe the benefits of each.\n\nParsing Structures from Extracted Metadata\nAt the end of the extraction, we have two types of parsed structure. The portrait (report) structure and the landscape (presentation) structure. We will describe the differences between the two and their benefits.\nPortrait (Report) Structure\n\nHierarchical Headers: Uses markdown heading levels (#, ##, ###, ####) to represent document hierarchy\nLine Numbers: Each heading includes line numbers [line X] for precise content location\nLinear Flow: Follows a traditional document structure with nested sections\nContent Focus: Emphasizes content hierarchy and relationships\nExample Use Case: Technical reports, research papers, documentation\n\n\n\n\nListing¬†1: Portrait structure example\n\n\n# Deloitte. [line 0]\n## Pushing through undercurrents [line 2]\n### Technology's impact on systemic risk: A look at banking [line 3]\n### Risk 1: Risk exposure from Banking as a Service offerings [line 15]\n#### Table 1: Risk exposure from Banking as a Service offerings [line 29]\n### Risk 2: Inadequate stability mechanisms for stablecoin arrangements [line 38]\n#### Table 1: Information about forces that could amplify the risk and how the industry mitigate it? [line 52]\n### Contacts [line 63]\n\n\n\nLandscape (Presentation) Structure\n\nPage-Based Organization: Groups content by pages under main sections\nExplicit Page Numbers: Each entry shows ‚ÄúPage X : Title‚Äù format\nFlat Section Hierarchy: Uses mainly top-level headings (#) for major sections\nSequential Listing: Lists pages sequentially within each section\nExample Use Case: Slide decks, presentations, visual-heavy documents\n\n\n\n\nListing¬†2: Landscape structure example\n\n\n# Everest Group¬Æ Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n- Page 1 : Everest Group¬Æ Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n\n# Introduction\n- Page 2 : Introduction\n\n# Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n- Page 3 : Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n- Page 4 : Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n- Page 12 : Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n- Page 13 : Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023\n\n# Deloitte profile\n- Page 5 : Deloitte profile (page 1 of 6)\n- Page 6 : Deloitte profile (page 2 of 6)\n- Page 7 : Deloitte profile (page 3 of 6)\n- Page 8 : Deloitte profile (page 4 of 6)\n- Page 9 : Deloitte profile (page 5 of 6)\n- Page 10 : Deloitte profile (page 6 of 6)\n\n# Appendix\n- Page 11 : Appendix\n\n# FAQs\n- Page 14 : FAQs\n\n# Everest Group¬Æ\n- Page 15 : Everest Group¬Æ\n\n\n\nBenefits\n\nDocument Navigation\n\nPortrait: Quick access to specific content sections via line numbers\nLandscape: Easy location of specific slides via page numbers\n\nContent Retrieval\n\nPortrait: Granular access to nested content hierarchies\nLandscape: Efficient slide/page-based content lookup\n\nStructure Preservation\n\nPortrait: Maintains detailed document hierarchy and relationships\nLandscape: Preserves presentation flow and slide organization\n\n\nWe then use distinct parsers to process these structures into formal tree representations. Here‚Äôs a detailed description of both parsing functions. The function parse_portrait_structure (in src/pdf_rag/structure_parsers.py) parses the structure of portrait-formatted documents (like reports) and creates a tree representation.\n\nTakes a BaseNode document as input\nCreates a hierarchical tree structure based on document headers\nHeaders must follow format: #{level} {title} [line {number}]\nValidates line numbers are in ascending order\nReturns a TreeNode representing the document structure\n\nThe function parse_landscape_structure (in src/pdf_rag/structure_parsers.py) parses the structure of landscape-formatted documents (like presentations) and creates a tree representation.\n\nTakes a BaseNode document as input\nHandles both section headers and page entries\nCreates a hierarchical structure with page numbers\nTracks missing or duplicate pages\nCreates an ‚ÄúUncategorized‚Äù section for orphaned pages\n\nBoth functions return a TreeNode object that can be traversed using breadth-first search (bfs) and supports standard tree operations like adding/removing children and setting parents.\nHere are the parsed structures for both document formats, shown in Listing¬†3 and Listing¬†4, which correspond to the raw structures in Listing¬†2 and Listing¬†1 respectively. These visualizations are also available in the HF Space Demo for PDF Metadata.\nIndentation shows parent/child relationships For landscape-oriented content, Numbers in [] represent:\n\nPositive numbers: page numbers\nNegative numbers: abstract nodes grouping related sections\n\n\n\n\nListing¬†3: Landscape structure parsed\n\n\nlife-sciences-smart-manufacturing-services-peak-matrix-assessment-2023 [-1]\n  Everest Group¬Æ Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [-2]\n    Everest Group¬Æ Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [1]\n  Introduction [-3]\n    Introduction [2]\n  Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [-4]\n    Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [3]\n    Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [4]\n    Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [12]\n    Life Sciences Smart Manufacturing Services PEAK Matrix¬Æ Assessment 2023 [13]\n  Deloitte profile [-5]\n    Deloitte profile (page 1 of 6) [5]\n    Deloitte profile (page 2 of 6) [6]\n    Deloitte profile (page 3 of 6) [7]\n    Deloitte profile (page 4 of 6) [8]\n    Deloitte profile (page 5 of 6) [9]\n    Deloitte profile (page 6 of 6) [10]\n  Appendix [-6]\n    Appendix [11]\n  FAQs [-7]\n    FAQs [14]\n  Everest Group¬Æ [-8]\n    Everest Group¬Æ [15]\n\n\n\nFor portrait-oriented content, Numbers in [] represent:\n\nPositive numbers: line numbers\nNegative numbers: abstract nodes grouping related sections\n\n\n\n\nListing¬†4: Portrait structure parsed\n\n\ndeloitte-tech-risk-sector-banking [-1]\n  Deloitte. [0]\n    Pushing through undercurrents [2]\n      Technology's impact on systemic risk: A look at banking [3]\n      Risk 1: Risk exposure from Banking as a Service offerings [15]\n        Table 1: Risk exposure from Banking as a Service offerings [29]\n      Risk 2: Inadequate stability mechanisms for stablecoin arrangements [38]\n        Table 1: Information about forces that could amplify the risk and how the industry mitigate it? [52]\n      Contacts [63]\n\n\n\n\n\nTree Index\nGiven these parsed structures, we now have access to a structured representation of the document content, divided by line numbers (portrait) or page numbers (landscape). The next step is to create a hierarchical index of the processed documents to support efficient traversal and querying. We need several components to build this index. First, we need two types of node parsing\n\nMarkdownLineNodeParser: For portrait-oriented content,\nMarkdownPageNodeParser: For landscape-oriented content\n\nHere‚Äôs a description of the two Markdown node parser classes.\nMarkdownLineNodeParser\n\nParses portrait-oriented documents (like reports) by splitting on headers\nTracks header hierarchy and line numbers\nPreserves code blocks by avoiding header parsing within them\nBuilds text nodes with header path and line number metadata\nUsed for documents where structure comes from markdown headers\n\nMarkdownPageNodeParser\n\nParses landscape-oriented documents (like presentations) by splitting on page markers\nSplits on --- end page N markers\nOptionally chunks content using a sentence splitter\nBuilds text nodes with page number metadata\nUsed for documents with clear page-based structure like slide decks\n\nThe key difference is that MarkdownLineNodeParser focuses on header-based structure while MarkdownPageNodeParser focuses on page-based structure, matching the different needs of reports versus presentations.\nUsing these parsers, we obtain document nodes that can be integrated into a hierarchical index. The TreeIndex class, located in src/pdf_rag/tree_index.py, extends BaseIndex to offer a hierarchical tree-based indexing structure for document nodes. It is specifically designed to manage both portrait (reports) and landscape (presentations) formatted documents. The TreeIndex class leverages the LlamaIndex framework, which provides a flexible and extensible indexing system for document nodes. Please note that this class is a work in progress; we have provided an initial draft of TreeIndex, but it is not yet fully implemented and should not be used in its current state.\nThe key features of the TreeIndex class (in src/pdf_rag/tree_index.py) include:\n\nData Structure\n\n\nUses IndexStructTree to maintain hierarchical relationships\nStores nodes in a tree structure with parent-child relationships\nSupports both root-level and nested nodes\nMaintains bidirectional references (parent-to-children and child-to-parent)\n\n\nNode Management\n\n\nNode insertion with automatic relationship building\nNode deletion with relationship cleanup\nReference document tracking\nSupport for abstract nodes (section headers) and content nodes (pages)\n\n\nNeo4j Integration\n\n\nExport functionality to Neo4j graph database\nCreates nodes and relationships in Neo4j\nMaintains document structure and metadata\nSupports constraints and cleanup operations\n\nUsage Example\n# Create index with nodes\nindex = TreeIndex(\n    nodes=document_nodes,\n    show_progress=True\n)\n\n# Export to Neo4j\nconfig = Neo4jConfig(\n    uri=\"neo4j://localhost:7687\",\n    username=\"neo4j\",\n    password=\"password\",\n    database=\"neo4j\"\n)\nindex.export_to_neo4j(config=config)\nFor now, we provide a basic implementation of the TreeIndex class to demonstrate the concept of hierarchical indexing. A visual representation of the index is shown in Figure¬†4.\n\n\n\n\n\n\nFigure¬†4: Neo4j visualisation of TreeIndex"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-ingestion-pipeline",
    "href": "posts/pdf-rag/pdf-rag.html#sec-ingestion-pipeline",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Script and Ingestion Pipeline",
    "text": "Script and Ingestion Pipeline\nThe script src/scripts/metadata_structure.py executes all the extractors in sequence through an ingestion pipeline. This process enriches documents with comprehensive metadata including author information, document context, table of contents, and structural relationships. The resulting metadata provides a rich foundation for document analysis and retrieval. It stores the processed data in both a local storage and optionally in a Neo4j database.\nTo execute the script, you need to provide a configuration file in YAML format. An example configuration file is provided in configs/metadata_structure_example.yaml. The configuration file includes settings for the pipeline, API keys, and file paths. The script supports three main modes:\n\ningest: Runs only the ingestion pipeline\ntree: Creates the tree index\nall: Runs both ingestion and indexing\n\nThe pipeline first ingests the PDF documents using PDFDirectoryReader to read PDF files. Then it processes the documents through the ingestion pipeline, which includes several transformations:\n\nReformatMarkdownComponent: Reformats content to markdown\nContextExtractor: Extracts contextual information\nTableOfContentsExtractor: Extracts table of contents\nTableOfContentsCreator: Creates structured TOC\nLandscapePagesExtractor: Handles landscape-oriented pages\nStructureExtractor: Extracts document structure\n\nIt stores the results in a SimpleDocumentStore for persistence and loading of processed data. Then, the script creates a hierarchical index of the processed documents using the TreeIndex class. The index is stored in a local directory and optionally exported to a Neo4j database.\nUsage\npython scripts/metadata_structure.py --config path/to/config.yaml\nThis script is particularly useful to process large collections of PDF documents and create searchable, structured knowledge bases from them. A Shiny app is provided to visualize and explore the metadata and structure of processed documents. The app allows you to:\n\nBrowse processed documents and their metadata\nView extracted table of contents\nExplore document structure trees\nCompare raw and reformatted markdown content\nVisualize document hierarchies\n\nYou can run the app locally using:\npython src/app/app-metadata.py\nA live demo showcasing these capabilities is available on the HF Space Demo for PDF Metadata. The visualization interface, shown in Figure¬†3, provides an interactive way to explore document metadata and structure."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-description",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-description",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Description",
    "text": "Description\nThe code src/pdf_rag/react_agent_multi_pdfs.py implements a ReActAgentMultiPdfs class designed for querying information from multiple PDF documents using a ReAct agent approach. The core idea is to build an agent for each PDF document, and then a ‚Äútop-level‚Äù agent orchestrates the use of these individual agents to answer complex queries. It leverages LlamaIndex for indexing, querying, and agent creation, and uses Gemini for LLM and embeddings. It includes both synchronous and asynchronous methods for building agents and processing queries to utilize concurrency.\nClass Definition:\nclass ReActAgentMultiPdfs:\n    def __init__(\n        self,\n        api_key_gemini: str,\n        api_key_mistral: str,\n        root_dir: Path,\n        pdfs_dir: Path,\n        cache_dir: Path,\n        storage_dir: Path,\n        num_workers: int = 16,\n        chunks_top_k: int = 5,\n        nodes_top_k: int = 10,\n        max_iterations: int = 20,\n        verbose: bool = True,\n    ) -&gt; None:\n        # ... initialization logic ...\nThe ReActAgentMultiPdfs class is initialized with several parameters:\n\napi_key_gemini: API key for the Google Gemini model.\napi_key_mistral: API key for the Mistral AI model.\nroot_dir: The root directory of the project.\npdfs_dir: The directory containing the PDF files to process.\ncache_dir: The directory to store cached data.\nstorage_dir: The directory to store the persisted indexes.\nnum_workers: Number of workers to use for parallel processing.\nchunks_top_k: Number of top chunks to retrieve from the vector index for each document.\nnodes_top_k: Number of top nodes to retrieve from the object index.\nmax_iterations: Maximum number of iterations for the ReAct agent.\nverbose: Whether to print verbose output.\n\nThe constructor initializes various components including:\n\nPDFDirectoryReader: Used to load data from PDF files.\nMarkdownPageNodeParser: Used to parse documents into nodes.\n\nKey Methods:\n\n_get_vector_summary_paths(self, pdf_file: Path) -&gt; tuple[str, Path, Path, Path]:\n\nThis helper function takes a pdf_file path and returns paths for storing the vector index, summary index, and summary text file, respectively.\nIt ensures that the paths are unique based on the PDF filename and are stored within the specified storage_dir.\nCritically it cleans the filenames, replacing spaces and parentheses with underscores, which is essential to avoid issues with LLM tool parsing.\n\nbuild_agent_per_doc(self, pdf_file: Path) -&gt; tuple[ReActAgent, str]:\n\nThis method is the heart of the per-document agent creation process.\nIt takes a pdf_file path as input.\nIt first checks if the vector and summary indexes already exist at the paths defined by _get_vector_summary_paths. If they do, it loads them from disk; otherwise, it creates them.\nIndex Creation: Loads the PDF data using self._pdf_reader, parses it into nodes using self._markdown_parser, and then builds both a VectorStoreIndex (for semantic search) and a SummaryIndex (for generating summaries). These are persisted to disk using storage_context.persist.\nQuery Engine Creation: Creates two query engines: vector_query_engine using the vector index and summary_query_engine using the summary index. The vector_query_engine uses FullPagePostprocessor to retrieve the full page for context.\nSummary Extraction: Extracts a summary of the document using the summary_query_engine and stores it in a text file.\nTool Definition: Defines two QueryEngineTools: one for the vector query engine and one for the summary query engine. These tools are used by the ReAct agent to answer questions. Each tool has a descriptive name and description that informs the agent when to use it.\nAgent Creation: Creates a ReActAgent from the defined tools and a system prompt that instructs the agent on how to use the tools. The system prompt emphasizes the need to use the tools and avoid relying on prior knowledge.\nReturns the created ReActAgent and the document summary.\n\nabuild_agent_per_doc(self, pdf_file: Path) -&gt; tuple[ReActAgent, str]:\n\nThis is the asynchronous version of build_agent_per_doc.\nIt uses async and await keywords to perform asynchronous operations, particularly when loading data using self._pdf_reader.aload_data and querying the summary engine.\nThe rest of the logic is very similar to the synchronous version.\n\nbuild_agents(self) -&gt; tuple[dict[str, ReActAgent], dict[str, dict]]:\n\nThis method iterates through all PDF files in the self._pdfs_dir directory.\nFor each PDF file, it calls self.build_agent_per_doc to create an agent and extract a summary.\nIt stores the agents in a dictionary agents_dict and the summaries in a dictionary extra_info_dict.\nReturns the dictionaries of agents and extra information.\n\nabuild_agents(self) -&gt; tuple[dict[str, ReActAgent], dict[str, dict]]:\n\nThis is the asynchronous version of build_agents.\nIt uses asyncio.gather (via run_jobs from LlamaIndex) to build the agents concurrently, improving performance.\nThe logic is otherwise similar to the synchronous version.\n\ntop_agent(self) -&gt; ReActAgent:\n\nThis method creates the ‚Äútop-level‚Äù ReAct agent that orchestrates the use of the per-document agents.\nIt first calls self.build_agents to create the per-document agents.\nIt then iterates through the agents and creates a QueryEngineTool for each agent, using the document summary as the tool description.\nObject Index: Creates an ObjectIndex from the QueryEngineTools, and then a VectorStoreIndex on these objects (tools).\nCustom Object Retriever: Creates a CustomObjectRetriever that wraps the vector store retriever from the object index. This CustomObjectRetriever is critical: it adds a query planning tool to the retrieval process. This query planning tool is a SubQuestionQueryEngine which is specifically designed for comparing multiple documents.\nTop Agent Creation: Creates the ReActAgent using the CustomObjectRetriever as the tool_retriever. The system prompt instructs the agent to always use the tools provided.\nIt is decorated with @cached_property so it‚Äôs only built once.\n\nabuild_top_agent(self) -&gt; ReActAgent:\n\nThis is the asynchronous version of top_agent.\nIt uses await self.abuild_agents() to build the agents asynchronously.\nThe rest of the logic is similar to the synchronous version.\n\nprocess_queries(self, queries: list[str]) -&gt; list[Any]:\n\nProcesses a list of queries using the synchronous top_agent.\n\naprocess_queries(self, queries: list[str]) -&gt; list[Any]:\n\nProcesses a list of queries using the asynchronous top_agent.\n\n\nCustom Object Retriever (CustomObjectRetriever)\nThe CustomObjectRetriever class inherits from ObjectRetriever and overrides the retrieve method. The important modification here is the addition of a SubQuestionQueryEngine as a tool. This tool is specifically designed for comparison queries across multiple documents.\nHere‚Äôs a breakdown of the critical part of CustomObjectRetriever.retrieve:\n        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=tools,\n            llm=self._llm,\n            question_gen=LLMQuestionGenerator.from_defaults(llm=self._llm),\n        )\n        sub_question_description = f\"\"\"\n        Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this\n        tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n        \"\"\"\n        sub_question_tool = QueryEngineTool(\n            query_engine=sub_question_engine,\n            metadata=ToolMetadata(name=\"compare_tool\", description=sub_question_description),\n        )\n        retrieved_tools = tools + [sub_question_tool]\n        return retrieved_tools\n\nA SubQuestionQueryEngine is created, using all the per-document tools. The LLMQuestionGenerator helps break down complex questions into sub-questions that can be answered by the individual tools.\nA QueryEngineTool is created for the SubQuestionQueryEngine, with a description emphasizing its use for comparison queries.\nThis sub_question_tool is added to the list of retrieved tools.\n\nKey Concepts:\n\nReAct Agent: A type of agent that interleaves reasoning and acting. It uses a language model to generate thoughts, actions, and observations.\nVector Store Index: An index that stores vector embeddings of text data. This allows for efficient semantic search.\nSummary Index: An index that stores summaries of text data. This allows for efficient summarization of documents.\nQuery Engine: A component that allows you to query an index.\nQueryEngineTool: A tool that allows an agent to query a query engine.\nObjectIndex: An index that stores objects (in this case, QueryEngineTools).\nObjectRetriever: A component that allows you to retrieve objects from an object index.\nSubQuestionQueryEngine: A query engine that breaks down complex questions into sub-questions and answers them using a set of tools. It excels at comparison queries.\nGemini: The LLM and embedding model used for this agent.\n\nOverall Functionality:\nThe ReActAgentMultiPdfs class provides a system for answering complex questions based on multiple PDF documents. It works by creating an agent for each document and then creating a top-level agent that can orchestrate the use of these individual agents. The CustomObjectRetriever plays a crucial role in enabling the top-level agent to handle comparison queries by including a sub-question query engine as a tool. The asynchronous methods offer a way to speed up processing when dealing with many documents. This architecture allows the system to answer questions that require information from multiple documents, such as ‚ÄúWhat are the key differences between document A and document B?‚Äù"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-example",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-example",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Example Usage",
    "text": "Example Usage\nWe provide an example usage in the notebook notebooks/query_multi_pdfs_example.ipynb. The notebook demonstrates how to use the ReActAgentMultiPdfs class to process multiple PDF documents and answer queries across them. It includes:\n\nSetting up the ReActAgent with multiple PDF documents\nProcessing queries about documents\nAnalyzing the agent‚Äôs reasoning process and tool selection\n\nThe complete example code and sample outputs are shown below, and the full notebook is available at ReActAgent: Querying Multiple PDFs Example.\n\n\nimport os\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nfrom omegaconf import OmegaConf, ValidationError\n\nfrom pdf_rag.react_agent_multi_pdfs import ReActAgentMultiPdfs\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@dataclass\nclass ReActAgentConfig:\n    data_dir: Path | str\n    api_key_gemini: str | None = None\n    api_key_mistral: str | None = None\n    num_workers: int = 16\n    chunks_top_k: int = 5\n    nodes_top_k: int = 10\n    max_iterations: int = 20\n    verbose: bool = True\n    queries: list[str] = field(default_factory=list)\n\n    def __post_init__(self):\n        self.data_dir = Path(self.data_dir)\n        self.root_dir = self.data_dir / \"pdfs\"\n        self.pdfs_dir = self.data_dir / \"pdfs\"\n        self.cache_dir = self.data_dir / \"cache\"\n        self.storage_dir = self.data_dir / \"storage_queries\"\n\n        self.api_key_gemini = self.api_key_gemini or os.environ.get(\"GEMINI_API_KEY\")\n        self.api_key_mistral = self.api_key_mistral or os.environ.get(\"MISTRAL_API_KEY\")\n        if not self.api_key_gemini:\n            raise ValueError(\n                \"Gemini API Key is required. Provide api_key_gemini or set GEMINI_API_KEY environment variable.\"\n            )\n        if not self.api_key_mistral:\n            raise ValueError(\n                \"Mistral API Key is required. Provide api_key_mistral or set MISTRAL_API_KEY environment variable.\"\n            )\n\n\ndef load_and_validate_config(config_path: str) -&gt; ReActAgentConfig:\n    try:\n        config = OmegaConf.load(config_path)\n        react_agent_schema = OmegaConf.structured(ReActAgentConfig)\n        react_agent_config = OmegaConf.merge(react_agent_schema, config)\n        react_agent_config = ReActAgentConfig(**react_agent_config)\n        print(\"Configuration loaded and validated successfully:\")\n        return react_agent_config\n    except ValidationError as e:\n        raise ValidationError(f\"Validation error: {e}\")\n    except Exception as e:\n        raise Exception(f\"Error loading configuration: {e}\")\n\n\nconfig_path = \"../configs/query_multi_pdfs_nicolas.yaml\"\nconfig = load_and_validate_config(config_path)\n\nreact_agent_multi_pdfs = ReActAgentMultiPdfs(\n    api_key_gemini=config.api_key_gemini,\n    api_key_mistral=config.api_key_mistral,\n    root_dir=config.root_dir,\n    pdfs_dir=config.pdfs_dir,\n    cache_dir=config.cache_dir,\n    storage_dir=config.storage_dir,\n    num_workers=config.num_workers,\n    chunks_top_k=config.chunks_top_k,\n    nodes_top_k=config.nodes_top_k,\n    max_iterations=config.max_iterations,\n    verbose=config.verbose,\n)\n\nresponses = react_agent_multi_pdfs.process_queries(queries=config.queries)\nfor q, r in zip(config.queries, responses):\n    print(30 * \"-\")\n    print(f\"Query: {q}\")\n    print(f\"Response: {r}\")\n\nConfiguration loaded and validated successfully:\n\n\n\n&gt; Running step 0dc45634-70c7-4656-9b12-fc2b086848ce. Step input: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'}\n\n&gt; Running step b574597e-591e-42f5-b2fe-075603369f1c. Step input: vulnerabilities of APIs in Banking as a Service (BaaS)\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'API vulnerabilities in Banking as a Service (BaaS)'}\n\nObservation: BaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\n\n\n&gt; Running step ab6397c0-d075-499f-8d74-ce5ef25de495. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\nObservation: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\n&gt; Running step 8b2c762a-4a21-47ad-81f9-5dba4bb5e0f3. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\n\n&gt; Running step 17a33cea-70cf-4150-98a9-73d9e08299cf. Step input: What mitigation opportunities are there to ensure strong security for BaaS platforms and API connectivity?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'mitigation strategies for BaaS platforms and API security'}\n\n&gt; Running step 0cde1ce4-d18b-4957-b0a9-0e8526141ad8. Step input: mitigation strategies for BaaS platforms and API security\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'mitigation strategies for BaaS platforms and API security'}\n\nObservation: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\n\n\n\n&gt; Running step 176b5919-f695-4091-8e2b-7ccc379b4dd0. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\n\nObservation: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\n\n&gt; Running step bf56f882-81b4-4053-9376-1dd9a231b405. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. Improving due diligence on BaaS providers and facilitating institutional knowledge transfer from banks to BaaS partners for better risk management and compliance are also beneficial mitigation strategies.\n\n&gt; Running step 69fc5519-d8d6-4ea3-ab8f-6ee9d32adcb7. Step input: How can the industry best improve due diligence on BaaS providers in this landscape?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'due diligence on BaaS providers'}\n\n&gt; Running step 57cce3c7-58b1-46a5-ae23-684437434197. Step input: due diligence on BaaS providers\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_deloitte-tech-risk-sector-banking\n\nAction Input: {'input': 'due diligence on BaaS providers'}\n\nObservation: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\n\n\n\n&gt; Running step fa50b051-1a5f-4068-bbe5-0ace2f8b0a20. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\n\nObservation: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\n\n&gt; Running step a835048a-e05e-4c02-b3bd-32f47024b7b4. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The industry can improve due diligence on BaaS providers by implementing strong security measures for BaaS platforms and thoroughly vetting BaaS partners.\n\n&gt; Running step 23143917-1992-4bf2-b67d-d81cb14b7fdc. Step input: What are the common objectives of the Open Data ecosystem?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_gx-iif-open-data\n\nAction Input: {'input': 'common objectives of Open Data ecosystem'}\n\n&gt; Running step ac79ff95-9ad1-4e03-a0ea-f75ff7b28194. Step input: common objectives of Open Data ecosystem\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_gx-iif-open-data\n\nAction Input: {'input': 'common objectives of Open Data ecosystem'}\n\nObservation: Open Data ecosystems share common objectives, including promoting innovation to increase consumer choice, creating more secure methods of data sharing, improving consumer data privacy and portability, and fostering cross-sectoral collaboration and interoperability.\n\n\n\n&gt; Running step 674ae599-801a-4725-8ea7-5ffe05a0a371. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Open Data ecosystems generally aim to promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n\nObservation: Open Data ecosystems generally aim to promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n\n&gt; Running step 46685aec-07d0-46f4-b8ad-4424cfc72ebd. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The common objectives of the Open Data ecosystem are to promote innovation, create secure data sharing methods, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n\n&gt; Running step 537f15c8-4be9-4f96-be10-6d63a2959001. Step input: What are key strategic decisions to be made by ecosystem participants?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_gx-iif-open-data\n\nAction Input: {'input': 'strategic decisions for open data ecosystem participants'}\n\n&gt; Running step 4f1f9e8c-aca3-4b46-8546-f4584318dc4c. Step input: strategic decisions for open data ecosystem participants\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: summary_tool_gx-iif-open-data\n\nAction Input: {'input': 'strategic decisions for open data ecosystem participants'}\n\nObservation: When looking at the key strategic decisions to be made by ecosystem participants, the following hypotheses are of particular interest:\n\n\n\n- Ecosystems are built on symbiotic relationships and generate value in standard ways;\n\n- Data sharing is (and will continue to be) critical to ecosystems, but client consent and permission are priorities;\n\n- Interoperability will likely be key to increasing the value capture in ecosystems;\n\n- Broadening the set of data used by Fls and non-Fls could realize significant benefits for consumers, authorities, and the private sector; and\n\n- Cyber-security and operational resilience will likely play a key role in building trust in Open Data ecosystems.\n\n\n\n&gt; Running step 3c89568b-6983-4a14-ac6b-bb1d53104325. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The key strategic decisions for open data ecosystem participants revolve around symbiotic relationships, data sharing with client consent, interoperability, broadening data usage, and ensuring cyber-security and operational resilience to build trust.\n\nObservation: The key strategic decisions for open data ecosystem participants revolve around symbiotic relationships, data sharing with client consent, interoperability, broadening data usage, and ensuring cyber-security and operational resilience to build trust.\n\n&gt; Running step 106b85da-5fee-4418-9a40-1bea7d6f56cc. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Key strategic decisions for ecosystem participants involve establishing symbiotic relationships, facilitating data sharing with client consent, ensuring interoperability, broadening data usage, and prioritizing cyber-security and operational resilience to foster trust.\n\n&gt; Running step 54f9c69d-a738-4028-bc14-10b1efec857a. Step input: How can the public and private sectors collaborate to promote innovation, secure data sharing, and data privacy within the Open Data ecosystem?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_gx-iif-open-data\n\nAction Input: {'input': 'collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy'}\n\n&gt; Running step c8953756-0575-4165-9a6e-a29c23a8bbd1. Step input: collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_gx-iif-open-data\n\nAction Input: {'input': 'collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy'}\n\nObservation: Open Data ecosystems promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability. To leverage the potential of Open Data, data from different industries can be merged through sector-specific approaches that bring together private entities, the public sector, and consumers to create common data spaces for certain industries, while a holistic approach to data would help develop more customer-centric solutions.\n\n\n\n&gt; Running step be48c51d-9b67-4dfe-adf7-11b0591c0d09. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Open Data ecosystems foster cross-sectoral collaboration between private entities, the public sector, and consumers to create common data spaces for certain industries, promoting innovation, secure data sharing, and data privacy.\n\nObservation: Open Data ecosystems foster cross-sectoral collaboration between private entities, the public sector, and consumers to create common data spaces for certain industries, promoting innovation, secure data sharing, and data privacy.\n\n&gt; Running step 5d9dd1d3-8e6e-4291-b0aa-f5ba27b3c62a. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The public and private sectors can collaborate within the Open Data ecosystem by fostering cross-sectoral partnerships to create common data spaces. This collaboration promotes innovation, secure data sharing methods, and improvements in consumer data privacy.\n\n&gt; Running step 54dc5bbe-979a-435f-871c-c2c4dc733b26. Step input: What are the key characteristics that define the Leaders, Major Contenders, and Aspirants within the Life Sciences Smart Manufacturing Services PEAK Matrix?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix'}\n\n&gt; Running step f6a15c99-4121-4b3d-a5ed-3024171d1e1e. Step input: key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix'}\n\nObservation: **Leaders:** They act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n\n\n**Major Contenders:** This group includes a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n\n\n**Aspirants:** Aspirants have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\n\n\n\n&gt; Running step 2b6e54f6-31a9-4178-92e2-1c4a3efb5561. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Here are the key characteristics of Leaders, Major Contenders, and Aspirants in the Life Sciences Smart Manufacturing Services PEAK Matrix:\n\n\n\n**Leaders:** Act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n\n\n**Major Contenders:** Include a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n\n\n**Aspirants:** Have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\n\nObservation: Here are the key characteristics of Leaders, Major Contenders, and Aspirants in the Life Sciences Smart Manufacturing Services PEAK Matrix:\n\n\n\n**Leaders:** Act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n\n\n**Major Contenders:** Include a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n\n\n**Aspirants:** Have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\n\n&gt; Running step 5d2cea8b-eee1-4ee4-a8ab-98b0d453474a. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The key characteristics are:\n\n\n\n*   **Leaders:** Act as digital transformation partners with end-to-end capabilities, balanced offerings, flexible engagement models, specialized talent, strong partnerships, and a clear roadmap for enhancing capabilities.\n\n*   **Major Contenders:** Have a less balanced portfolio, often specializing in specific areas, with limited solutions for high-growth areas. They leverage cross-industry resources and focus on the mid-tier client segment.\n\n*   **Aspirants:** Have a restricted focus, limited digital service capabilities and partnership ecosystem, and concentrate on capturing market share in the small and midsize buyer segment.\n\n&gt; Running step 423c986e-395b-403b-afe0-8acada1ae7f1. Step input: What are some of the solutions that can assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality'}\n\n&gt; Running step bccdeb0c-a589-460a-b810-59927dee9e0e. Step input: solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality\n\nThought: The current language of the user is: english. I need to use a tool to help me answer the question.\n\nAction: summary_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality'}\n\nObservation: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n\n&gt; Running step 2d287b00-2e7d-474e-94fe-75e04853131d. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n\nObservation: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n\n&gt; Running step a8339752-282e-4d96-861c-339fe81db351. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n\n&gt; Running step e3b4b9cd-9da2-4737-9c43-662051410718. Step input: How are different areas of a manufacturing line benefiting from AI?\n\nThought: The current language of the user is: English. I need to find information about how AI is benefiting manufacturing lines. I don't have a specific tool that directly addresses this, but the \"tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\" tool seems most relevant as it discusses smart manufacturing in the life sciences, which likely involves AI applications.\n\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'AI applications in life sciences manufacturing'}\n\n&gt; Running step 8285ebfe-60e4-4c07-b369-84bf63a817a8. Step input: AI applications in life sciences manufacturing\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\n\nAction Input: {'input': 'AI applications in life sciences manufacturing'}\n\nObservation: Life sciences enterprises are investing in critical use cases, including digital twins and predictive maintenance, and exploring opportunities such as sustainable and personalized medicine manufacturing. Service providers are becoming end-to-end digital transformation partners, co-developing solutions to assist enterprises in their digital journeys. One solution enables visibility of multi-tier supply networks and senses upcoming risks to mitigate disruptions. Another platform solution enables a comprehensive view of manufacturing operations across the organization with connected data, creating dynamic data visualizations that provide useful insights and recommended actions, assisting in predictive analytics and quality analytics.\n\n\n\n&gt; Running step 2dc34498-25fe-491a-9487-ea77a5562c2b. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Life sciences companies are investing in AI applications like digital twins and predictive maintenance, and exploring opportunities in sustainable and personalized medicine manufacturing. Service providers are offering end-to-end digital transformation partnerships, co-developing solutions for digital journeys. These solutions include providing visibility into multi-tier supply networks to mitigate disruptions and offering platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n\nObservation: Life sciences companies are investing in AI applications like digital twins and predictive maintenance, and exploring opportunities in sustainable and personalized medicine manufacturing. Service providers are offering end-to-end digital transformation partnerships, co-developing solutions for digital journeys. These solutions include providing visibility into multi-tier supply networks to mitigate disruptions and offering platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n\n&gt; Running step 3cd8ceec-02a0-4544-95a6-673923dca8b5. Step input: None\n\nThought: The current language of the user is: English. The tool provided some information about AI applications in life sciences manufacturing, including digital twins, predictive maintenance, supply chain visibility, and platforms for comprehensive views of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics. This gives a good overview of how AI is benefiting different areas of a manufacturing line.\n\nAnswer: AI is benefiting different areas of manufacturing lines through applications like digital twins and predictive maintenance. It also provides better visibility into supply chains and offers platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n\n&gt; Running step b23d31dd-3e78-4d90-bbe9-0c17280967db. Step input: What are ConocoPhillips' key financial priorities for the next 10 years?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': \"ConocoPhillips' 10-year financial plan\"}\n\n&gt; Running step c1216a19-59b1-4f3e-ba31-3e5bc2653a3c. Step input: ConocoPhillips' 10-year financial plan\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: summary_tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': \"ConocoPhillips' 10-year financial plan\"}\n\nObservation: ConocoPhillips has a compelling 10-year plan that includes the following:\n\n\n\n*   Peer-leading ROCE improving through time\n\n*   Top quartile ordinary dividend growth\n\n*   &gt;90% market cap distributed\n\n*   ~$35/BBL WTI FCF Breakeven\n\n*   ~6% CFO CAGR, ~11% FCF CAGR\n\n*   Unhedged for price upside\n\n\n\n&gt; Running step 85f186e1-6701-49c4-9ede-8877d99b5f7d. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips' 10-year financial plan aims for peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and being unhedged for price upside.\n\nObservation: ConocoPhillips' 10-year financial plan aims for peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and being unhedged for price upside.\n\n&gt; Running step ec8ff47a-d551-4907-93fb-d325380bc4ba. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips' key financial priorities for the next 10 years include: peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and remaining unhedged for price upside.\n\n&gt; Running step 296a9b6f-5116-4992-8417-a98b9ecf7e52. Step input: How does ConocoPhillips plan to meet it's Net-Zero and Emissions targets?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': 'ConocoPhillips Net-Zero and Emissions targets'}\n\n&gt; Running step b2a1e6ee-2120-498d-b83d-bfaa372b0646. Step input: ConocoPhillips Net-Zero and Emissions targets\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': 'ConocoPhillips Net-Zero and Emissions targets'}\n\nObservation: ConocoPhillips is progressing toward its 2050 net-zero ambition and accelerating its 2030 GHG emissions intensity reduction target. They plan to reduce GHG intensity 50-60% by 2030 and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They are also aiming for zero routine flaring by 2025.\n\n\n\n&gt; Running step 9b83e468-2f60-431e-940d-646ae3abca87. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips is working towards its 2050 net-zero ambition. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n\nObservation: ConocoPhillips is working towards its 2050 net-zero ambition. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n\n&gt; Running step 52cb3db4-88f0-4e6c-98a2-85289b0dd630. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips plans to meet its Net-Zero ambition by 2050. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n\n&gt; Running step a9a3e10f-d4aa-4e20-afd1-fb3b6a6f4cb2. Step input: What is ConocoPhillips strategy to grow production?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': 'ConocoPhillips production growth strategy'}\n\n&gt; Running step 379518d2-dcd9-4126-96b0-6b262201571f. Step input: ConocoPhillips production growth strategy\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_2023-conocophillips-aim-presentation\n\nAction Input: {'input': 'ConocoPhillips production growth strategy'}\n\nObservation: ConocoPhillips has a strategy for capital-efficient production growth that underpins growing distribution capacity. They have a material low cost of supply resource base, leveraging existing infrastructure, with an average cost of supply of ~$30/BBL. Their production mix consists of approximately 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They anticipate a 4% CAGR at ~40% reinvestment rate over the next 10 years at $60/BBL WTI.\n\n\n\n&gt; Running step 36765352-5b93-4eac-b04d-c74a685229f3. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips' production growth strategy focuses on capital-efficient growth to support increasing distribution capacity. They leverage a substantial, low-cost supply resource base, utilizing existing infrastructure, with an average supply cost of approximately $30 per barrel. Their production is composed of roughly 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They project a 4% compound annual growth rate (CAGR) with about a 40% reinvestment rate over the next decade, assuming a West Texas Intermediate (WTI) crude oil price of $60 per barrel.\n\nObservation: ConocoPhillips' production growth strategy focuses on capital-efficient growth to support increasing distribution capacity. They leverage a substantial, low-cost supply resource base, utilizing existing infrastructure, with an average supply cost of approximately $30 per barrel. Their production is composed of roughly 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They project a 4% compound annual growth rate (CAGR) with about a 40% reinvestment rate over the next decade, assuming a West Texas Intermediate (WTI) crude oil price of $60 per barrel.\n\n&gt; Running step 4fb25291-2915-4d64-b88a-2eba67a1f58d. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: ConocoPhillips' strategy to grow production involves capital-efficient growth supported by a substantial, low-cost resource base and existing infrastructure. They aim for a 4% compound annual growth rate (CAGR) over the next decade, with a reinvestment rate of about 40%, assuming a WTI crude oil price of $60 per barrel.\n\n&gt; Running step 271b97c5-318f-4709-9689-2efaa1692bfc. Step input: What are the key features and capabilities of the XC9500 In-System Programmable CPLD family?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'key features and capabilities of XC9500 CPLD family'}\n\n&gt; Running step 38da0c2b-6f77-4b3e-998d-22dec2f0435f. Step input: key features and capabilities of XC9500 CPLD family\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'key features and capabilities of XC9500 CPLD family'}\n\nObservation: The XC9500 CPLD family offers in-system programmability and testing for general purpose logic integration. Devices are in-system programmable for a minimum of 10,000 program/erase cycles and include IEEE 1149.1 (JTAG) boundary-scan support.\n\n\n\nKey features include:\n\n\n\n*   High-performance with 5 ns pin-to-pin logic delays and fCNT to 125 MHz.\n\n*   A density range of 36 to 288 macrocells, equating to 800 to 6,400 usable gates.\n\n*   5V in-system programmability with 10,000 program/erase cycle endurance.\n\n*   Enhanced pin-locking architecture.\n\n*   Flexible 36V18 Function Block with 90 product terms driving up to 18 macrocells.\n\n*   Global and product term clocks, output enables, set and reset signals.\n\n*   Extensive IEEE Std 1149.1 boundary-scan (JTAG) support.\n\n*   Programmable power reduction mode in each macrocell.\n\n*   Slew rate control on individual outputs.\n\n*   User programmable ground pin capability.\n\n*   Extended pattern security features for design protection.\n\n*   High-drive 24 mA outputs.\n\n*   3.3V or 5V I/O capability.\n\n*   Advanced CMOS 5V FastFLASH‚Ñ¢ technology.\n\n*   Support for parallel programming of multiple devices.\n\n\n\nThe architecture includes features such as enhanced pin-locking, an expanded JTAG instruction set, and in-system programming across the full device operating range. Additional system features include output slew rate control, user-programmable ground pins, and configurable I/Os for 3.3V or 5V operation. All outputs provide 24 mA drive. The devices also offer a low-power mode for individual or all macrocells.\n\n\n\n&gt; Running step ad6a2bf8-8a03-4f32-a6f9-fae02d7d9e56. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The XC9500 CPLD family provides in-system programmability and testing. Key features include high performance (5 ns pin-to-pin delays, 125 MHz fCNT), densities from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability (10,000 cycles), enhanced pin-locking, flexible function blocks, global/product term clocks, JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, and advanced CMOS FastFLASH technology. It also supports parallel programming, expanded JTAG, and configurable I/Os.\n\nObservation: The XC9500 CPLD family provides in-system programmability and testing. Key features include high performance (5 ns pin-to-pin delays, 125 MHz fCNT), densities from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability (10,000 cycles), enhanced pin-locking, flexible function blocks, global/product term clocks, JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, and advanced CMOS FastFLASH technology. It also supports parallel programming, expanded JTAG, and configurable I/Os.\n\n&gt; Running step 125f1de5-aa3e-48fa-b445-a0a836de904e. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The key features and capabilities of the XC9500 In-System Programmable CPLD family include: high performance with 5 ns pin-to-pin delays and 125 MHz fCNT, densities ranging from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability with 10,000 program/erase cycles, enhanced pin-locking architecture, flexible Function Blocks, global and product term clocks, extensive JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, advanced CMOS FastFLASH technology, support for parallel programming, expanded JTAG, and configurable I/Os.\n\n&gt; Running step 7813cb38-11bb-41dd-b88b-5c5b044ba777. Step input: How does the Fast CONNECT switch matrix enable flexible signal routing and logic implementation within the XC9500 devices?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'Fast CONNECT switch matrix'}\n\n&gt; Running step 5de85ea0-fe67-49a5-a791-1ee39c77be35. Step input: Fast CONNECT switch matrix\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'Fast CONNECT switch matrix'}\n\nObservation: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\n\n\n\n&gt; Running step 7e74704f-8e13-4efd-bf1d-d2d80fc9a54e. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\n\nObservation: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\n\n&gt; Running step 6bfc34d0-b396-4ee0-aafe-6d14f487229b. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The Fast CONNECT switch matrix enables flexible signal routing by allowing any I/O Block (IOB) output (corresponding to user pin inputs) and all Function Block (FB) outputs to drive the matrix. User programming can select up to 36 of these signals to drive each FB with a uniform delay. It also increases logic implementation flexibility by combining multiple internal connections into a single wired-AND output before driving the destination FB, increasing the effective logic fan-in of the destination FB without additional timing delay.\n\n&gt; Running step 1d68eac4-1bf1-439d-a3d5-ede073dd7907. Step input: What design security options are available in the XC9500 family, and how do they protect programming data?\n\nThought: The current language of the user is: English. I need to find information about design security options in the XC9500 family.\n\nAction: tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'design security'}\n\n&gt; Running step dca71de5-ce54-4035-81af-13b0d874fda6. Step input: design security\n\nThought: The current language of the user is: english. I need to use a tool to help me answer the question.\n\nAction: vector_tool_XC9500_CPLD_Family\n\nAction Input: {'input': 'design security'}\n\nObservation: XC9500 devices have advanced data security features that protect programming data against unauthorized reading or accidental erasure/reprogramming. There are four security settings available. The read security bits can be set to prevent the internal programming pattern from being read or copied, which also inhibits further program operations but allows device erasure. The write security bits provide added protection against accidental device erasure or reprogramming when the JTAG pins are subject to noise. Once set, the write-protection may be deactivated when the device needs to be reprogrammed with a valid pattern.\n\n\n\n&gt; Running step 8d883c8c-aaa2-4dc0-874e-625773179440. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: XC9500 devices offer advanced data security features to safeguard programming data from unauthorized access or accidental erasure/reprogramming. These devices provide four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n\nObservation: XC9500 devices offer advanced data security features to safeguard programming data from unauthorized access or accidental erasure/reprogramming. These devices provide four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n\n&gt; Running step d27647c5-3477-4090-82b9-09ce5c02f89f. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: The XC9500 family offers advanced data security features to protect programming data. It provides four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n\n&gt; Running step 10dc49fa-9fbd-40d4-a72e-1d84eda84d54. Step input: What strategies are companies implementing to address tax transformation in a data-driven world?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'strategies for tax transformation in a data-driven world'}\n\n&gt; Running step 9819b07e-37da-4fd9-9dac-a93d8b6e7e18. Step input: strategies for tax transformation in a data-driven world\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'strategies for tax transformation in a data-driven world'}\n\nObservation: To meet complex tax obligations, real-time reporting requirements, and increasing expectations of tax transparency, firms need immediate access to reliable, accurate, and fit-for-purpose tax data. Businesses should consider customizing existing ERP systems and identifying appropriate issues, involving the right stakeholders internally, obtaining budget when there are competing demands, and devising a robust schedule of maintenance. Tax departments are encouraged to develop a road map to use available technology, identify future needs, and then make build-or-buy decisions. Embedding tax into everyday processes and operations will lead to tax considerations in transformation efforts becoming \"business as usual,\" making building the business case for technology investment less onerous.\n\n\n\n&gt; Running step a498dc3f-6f71-44cc-9add-db894c222e83. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To transform tax in a data-driven world, businesses should focus on:\n\n\n\n*   Ensuring immediate access to reliable, accurate, and fit-for-purpose tax data to meet complex obligations, real-time reporting, and transparency expectations.\n\n*   Customizing existing ERP systems and addressing relevant issues.\n\n*   Involving the right internal stakeholders.\n\n*   Securing budget despite competing demands.\n\n*   Creating a robust maintenance schedule.\n\n*   Developing a technology roadmap to address current and future needs, guiding build-or-buy decisions.\n\n*   Embedding tax into everyday processes to make tax considerations routine in transformation efforts, simplifying the justification for technology investments.\n\nObservation: To transform tax in a data-driven world, businesses should focus on:\n\n\n\n*   Ensuring immediate access to reliable, accurate, and fit-for-purpose tax data to meet complex obligations, real-time reporting, and transparency expectations.\n\n*   Customizing existing ERP systems and addressing relevant issues.\n\n*   Involving the right internal stakeholders.\n\n*   Securing budget despite competing demands.\n\n*   Creating a robust maintenance schedule.\n\n*   Developing a technology roadmap to address current and future needs, guiding build-or-buy decisions.\n\n*   Embedding tax into everyday processes to make tax considerations routine in transformation efforts, simplifying the justification for technology investments.\n\n&gt; Running step f859adbd-5e63-4539-89f0-c13f01b9f2d2. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Companies are implementing strategies such as ensuring immediate access to reliable and accurate tax data, customizing ERP systems, involving internal stakeholders, securing budgets, creating maintenance schedules, developing technology roadmaps, and embedding tax into everyday processes to address tax transformation in a data-driven world.\n\n&gt; Running step 913a33a5-23de-43aa-a220-b71d419a6011. Step input: How can tax departments effectively invest in technology to meet compliance and strategic goals?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'How can tax departments effectively invest in technology to meet compliance and strategic goals?'}\n\n&gt; Running step 11574750-b6fa-48a0-b5c2-90b73fc01b01. Step input: How can tax departments effectively invest in technology to meet compliance and strategic goals?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: vector_tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'effective technology investment for tax compliance and strategic goals'}\n\nObservation: To effectively invest in technology, tax departments should first secure a budget and create a roadmap that leverages existing technology, identifies future needs, and guides build-or-buy decisions. Taking a top-down approach and explaining the impact of tax authority digitalization to the C-suite, as well as collaborating with the IT department, can help build a strong value case for budget. It's important to consider the long-term potential requirements, as finding a short-term solution without considering future needs may prevent the tax department from becoming a strategic advisor.\n\n\n\nWhen choosing technology, tax departments can implement using internal resources, appoint an implementation partner, or outsource the entire function requiring the technology. If using in-house resources, developing professional teams with data management and technology expertise is essential, especially in data analytics, strategic insights, and data management.\n\n\n\n&gt; Running step cf222cb7-a4c1-4012-9c72-6e32be1215b5. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To effectively invest in technology for tax compliance and strategic goals, tax departments should:\n\n\n\n1.  **Secure a budget and create a roadmap:** This roadmap should leverage existing technology, identify future needs, and guide build-or-buy decisions. A top-down approach, explaining the impact of tax authority digitalization to the C-suite, and collaboration with the IT department can help build a strong value case for budget.\n\n2.  **Consider long-term potential requirements:** Avoid short-term solutions that may hinder the tax department's ability to become a strategic advisor.\n\n3.  **Choose an implementation approach:** Options include using internal resources, appointing an implementation partner, or outsourcing the entire function.\n\n4.  **Develop professional teams:** If using in-house resources, create teams with data management and technology expertise, especially in data analytics, strategic insights, and data management.\n\nObservation: To effectively invest in technology for tax compliance and strategic goals, tax departments should:\n\n\n\n1.  **Secure a budget and create a roadmap:** This roadmap should leverage existing technology, identify future needs, and guide build-or-buy decisions. A top-down approach, explaining the impact of tax authority digitalization to the C-suite, and collaboration with the IT department can help build a strong value case for budget.\n\n2.  **Consider long-term potential requirements:** Avoid short-term solutions that may hinder the tax department's ability to become a strategic advisor.\n\n3.  **Choose an implementation approach:** Options include using internal resources, appointing an implementation partner, or outsourcing the entire function.\n\n4.  **Develop professional teams:** If using in-house resources, create teams with data management and technology expertise, especially in data analytics, strategic insights, and data management.\n\n&gt; Running step 62f00f19-9819-4183-a8d5-52fac1c5a514. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To effectively invest in technology for tax compliance and strategic goals, tax departments should secure a budget and create a roadmap that leverages existing technology and identifies future needs. They should also consider long-term potential requirements, choose an appropriate implementation approach (internal resources, partner, or outsourcing), and develop professional teams with data management and technology expertise.\n\n&gt; Running step cf14f83c-b67b-49a2-9429-4e9180ce5411. Step input: How can tax departments collaborate internally to secure budget and ensure technology alignment?\n\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n\nAction: tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'internal collaboration in tax departments to secure budget and ensure technology alignment'}\n\n&gt; Running step 7f6e4001-7126-4e9b-87fd-e2713553240b. Step input: internal collaboration in tax departments to secure budget and ensure technology alignment\n\nThought: The current language of the user is: English. I need to use a tool to find information about internal collaboration in tax departments to secure budget and ensure technology alignment.\n\nAction: vector_tool_dttl-tax-technology-report-2023\n\nAction Input: {'input': 'internal collaboration in tax departments to secure budget and ensure technology alignment'}\n\nObservation: To make a case for budget, tax departments need to show the value they bring to the company, explain how tax authority digitalization impacts the C-suite, and work with the IT department. Companies that have taken a top-down approach have been more successful in building this value case.\n\n\n\nWhen seeking technology, best practice includes securing a budget and creating a road map to utilize available technology, identify future needs, and then decide whether to build or buy. A holistic view of the tax department's operating model is useful at this point to increase speed and accuracy, and free up tax professionals for more strategic business activity.\n\n\n\nWhen asked about the greatest need for skills over the next three to five years, respondents most often named data analytics, data-driven strategic insights, and data management.\n\n\n\n&gt; Running step 55db2b52-becf-4e0f-a00e-299c4a5048bb. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: To secure budget for tax technology, tax departments need to demonstrate their value to the company, explain the impact of tax authority digitalization on the C-suite, and collaborate with the IT department. Companies adopting a top-down approach have seen more success in building this value case. Best practices include creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model.\n\nObservation: To secure budget for tax technology, tax departments need to demonstrate their value to the company, explain the impact of tax authority digitalization on the C-suite, and collaborate with the IT department. Companies adopting a top-down approach have seen more success in building this value case. Best practices include creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model.\n\n&gt; Running step e35d23b0-83b9-4d79-b225-c08ea46ecec1. Step input: None\n\nThought: I can answer without using any more tools. I'll use the user's language to answer\n\nAnswer: Tax departments can collaborate internally to secure budget and ensure technology alignment by demonstrating their value to the company, explaining the impact of tax authority digitalization to the C-suite, and collaborating with the IT department. Creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model are best practices. Companies adopting a top-down approach have seen more success in building this value case.\n\n------------------------------\n\nQuery: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n\nResponse: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\n\n------------------------------\n\nQuery: What mitigation opportunities are there to ensure strong security for BaaS platforms and API connectivity?\n\nResponse: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. Improving due diligence on BaaS providers and facilitating institutional knowledge transfer from banks to BaaS partners for better risk management and compliance are also beneficial mitigation strategies.\n\n------------------------------\n\nQuery: How can the industry best improve due diligence on BaaS providers in this landscape?\n\nResponse: The industry can improve due diligence on BaaS providers by implementing strong security measures for BaaS platforms and thoroughly vetting BaaS partners.\n\n------------------------------\n\nQuery: What are the common objectives of the Open Data ecosystem?\n\nResponse: The common objectives of the Open Data ecosystem are to promote innovation, create secure data sharing methods, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n\n------------------------------\n\nQuery: What are key strategic decisions to be made by ecosystem participants?\n\nResponse: Key strategic decisions for ecosystem participants involve establishing symbiotic relationships, facilitating data sharing with client consent, ensuring interoperability, broadening data usage, and prioritizing cyber-security and operational resilience to foster trust.\n\n------------------------------\n\nQuery: How can the public and private sectors collaborate to promote innovation, secure data sharing, and data privacy within the Open Data ecosystem?\n\nResponse: The public and private sectors can collaborate within the Open Data ecosystem by fostering cross-sectoral partnerships to create common data spaces. This collaboration promotes innovation, secure data sharing methods, and improvements in consumer data privacy.\n\n------------------------------\n\nQuery: What are the key characteristics that define the Leaders, Major Contenders, and Aspirants within the Life Sciences Smart Manufacturing Services PEAK Matrix?\n\nResponse: The key characteristics are:\n\n\n\n*   **Leaders:** Act as digital transformation partners with end-to-end capabilities, balanced offerings, flexible engagement models, specialized talent, strong partnerships, and a clear roadmap for enhancing capabilities.\n\n*   **Major Contenders:** Have a less balanced portfolio, often specializing in specific areas, with limited solutions for high-growth areas. They leverage cross-industry resources and focus on the mid-tier client segment.\n\n*   **Aspirants:** Have a restricted focus, limited digital service capabilities and partnership ecosystem, and concentrate on capturing market share in the small and midsize buyer segment.\n\n------------------------------\n\nQuery: What are some of the solutions that can assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality?\n\nResponse: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n\n------------------------------\n\nQuery: How are different areas of a manufacturing line benefiting from AI?\n\nResponse: AI is benefiting different areas of manufacturing lines through applications like digital twins and predictive maintenance. It also provides better visibility into supply chains and offers platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n\n------------------------------\n\nQuery: What are ConocoPhillips' key financial priorities for the next 10 years?\n\nResponse: ConocoPhillips' key financial priorities for the next 10 years include: peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and remaining unhedged for price upside.\n\n------------------------------\n\nQuery: How does ConocoPhillips plan to meet it's Net-Zero and Emissions targets?\n\nResponse: ConocoPhillips plans to meet its Net-Zero ambition by 2050. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n\n------------------------------\n\nQuery: What is ConocoPhillips strategy to grow production?\n\nResponse: ConocoPhillips' strategy to grow production involves capital-efficient growth supported by a substantial, low-cost resource base and existing infrastructure. They aim for a 4% compound annual growth rate (CAGR) over the next decade, with a reinvestment rate of about 40%, assuming a WTI crude oil price of $60 per barrel.\n\n------------------------------\n\nQuery: What are the key features and capabilities of the XC9500 In-System Programmable CPLD family?\n\nResponse: The key features and capabilities of the XC9500 In-System Programmable CPLD family include: high performance with 5 ns pin-to-pin delays and 125 MHz fCNT, densities ranging from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability with 10,000 program/erase cycles, enhanced pin-locking architecture, flexible Function Blocks, global and product term clocks, extensive JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, advanced CMOS FastFLASH technology, support for parallel programming, expanded JTAG, and configurable I/Os.\n\n------------------------------\n\nQuery: How does the Fast CONNECT switch matrix enable flexible signal routing and logic implementation within the XC9500 devices?\n\nResponse: The Fast CONNECT switch matrix enables flexible signal routing by allowing any I/O Block (IOB) output (corresponding to user pin inputs) and all Function Block (FB) outputs to drive the matrix. User programming can select up to 36 of these signals to drive each FB with a uniform delay. It also increases logic implementation flexibility by combining multiple internal connections into a single wired-AND output before driving the destination FB, increasing the effective logic fan-in of the destination FB without additional timing delay.\n\n------------------------------\n\nQuery: What design security options are available in the XC9500 family, and how do they protect programming data?\n\nResponse: The XC9500 family offers advanced data security features to protect programming data. It provides four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n\n------------------------------\n\nQuery: What strategies are companies implementing to address tax transformation in a data-driven world?\n\nResponse: Companies are implementing strategies such as ensuring immediate access to reliable and accurate tax data, customizing ERP systems, involving internal stakeholders, securing budgets, creating maintenance schedules, developing technology roadmaps, and embedding tax into everyday processes to address tax transformation in a data-driven world.\n\n------------------------------\n\nQuery: How can tax departments effectively invest in technology to meet compliance and strategic goals?\n\nResponse: To effectively invest in technology for tax compliance and strategic goals, tax departments should secure a budget and create a roadmap that leverages existing technology and identifies future needs. They should also consider long-term potential requirements, choose an appropriate implementation approach (internal resources, partner, or outsourcing), and develop professional teams with data management and technology expertise.\n\n------------------------------\n\nQuery: How can tax departments collaborate internally to secure budget and ensure technology alignment?\n\nResponse: Tax departments can collaborate internally to secure budget and ensure technology alignment by demonstrating their value to the company, explaining the impact of tax authority digitalization to the C-suite, and collaborating with the IT department. Creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model are best practices. Companies adopting a top-down approach have seen more success in building this value case.\n\n\n\n\nSource: ReActAgent: Querying Multiple PDFs Example\nLet‚Äôs analyze the output of the ReAct Agent for a specific query: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n&gt; Running step 0dc45634-70c7-4656-9b12-fc2b086848ce. Step input: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'}\n&gt; Running step b574597e-591e-42f5-b2fe-075603369f1c. Step input: vulnerabilities of APIs in Banking as a Service (BaaS)\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'API vulnerabilities in Banking as a Service (BaaS)'}\nObservation: BaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\n&gt; Running step ab6397c0-d075-499f-8d74-ce5ef25de495. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\nObservation: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n&gt; Running step 8b2c762a-4a21-47ad-81f9-5dba4bb5e0f3. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\nHere‚Äôs a step-by-step explanation:\nWhat are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n\nInitial Thought: The agent recognizes the need for a tool to answer the question.\nAction (tool selection): The agent selects the tool_deloitte-tech-risk-sector-banking. This indicates that the agent has identified the appropriate source document for answering the question. It has arrived at this decision by retrieving tools based on the tools metadata.\nAction Input: The agent provides {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'} as input to the chosen tool. This represents the refined query or search terms to be used within the selected document.\nSecondary Thought: The agent reiterates the need for a tool: either querying the Vector Index or the Summary Index.\nSecondary Action: The agent selects vector_tool_deloitte-tech-risk-sector-banking. This is the vector query engine associated with the deloitte document, meaning it‚Äôs going to use semantic search on the document‚Äôs content.\nObservation: The agent receives the following observation from the vector_tool_deloitte-tech-risk-sector-banking:\nBaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\nThought: The agent decides it has enough information to answer the question without further tool usage.\nAnswer: The agent provides a concise answer in English, summarizing the vulnerabilities: customer data/funds at risk, backdoor for hackers, and noncompliance with data privacy rules.\nObservation & Answer Redux: The agent repeats its reasoning and repeats the final answer\n\nKey Observations:\n\nReAct Loop: The agent follows the ReAct loop (Reason, Act, Observe). It reasons about the question, selects a tool (Action), observes the tool‚Äôs output (Observation), and then decides on the next step.\nTool Selection: The agent correctly identifies and uses the appropriate tool (document) for each question. This demonstrates the agent‚Äôs ability to choose the right knowledge source.\nSemantic Search: The use of vector_tool_deloitte-tech-risk-sector-banking indicates the agent is leveraging semantic search to retrieve relevant information from the document‚Äôs vector index.\nConcise Answers: The agent provides concise and relevant answers based on the retrieved information.\nRedundancy: There‚Äôs a bit of redundancy in the steps, particularly the repeated ‚ÄúThought‚Äù and ‚ÄúObservation‚Äù before the ‚ÄúAnswer.‚Äù This is due to the specific implementation of the ReAct loop in LlamaIndex.\nSingle Document Usage: The agent doesn‚Äôt seem to be using the ‚Äúcompare_tool‚Äù or querying multiple documents simultaneously in these examples. All the information is coming from the Deloitte Tech Risk Sector Banking document.\n\nFeel free to use this notebook as a starting point for building your own multi-document query system using ReAct agents. You can adapt the code to work with your own PDF documents and queries, and explore the capabilities of the ReAct framework for document analysis and retrieval."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html",
    "href": "posts/pdf-parsing/pdf-parsing.html",
    "title": "PDF Parsing for LLM Input",
    "section": "",
    "text": "This blog post explores the current landscape of PDF parsing for use as input to Large Language Models (LLMs). Extracting meaningful information from PDFs can be challenging due to their complex structure. This article examines several approaches, their strengths, and limitations, with a focus on their suitability for LLM integration (markdown output).\nWe begin with a detailed presentation of some open-source PDF parsing libraries: Docling, Marker-PDF, and MinerU in Section¬†1. In particular, we provide a comprehensive overview of the Docling pipeline, a modular and open-source PDF processing pipeline designed to transform PDFs into a structured representation (the DoclingDocument) in Section¬†1.1. We also discuss Marker-PDF and MinerU in Section¬†1.2 and Section¬†1.3. The objective is to gain an understanding of how modern PDF parsing libraries function, in particular the role of deep learning models in PDF parsing and extraction, focusing on layout analysis, table structure recognition, and Optical Character Recognition (OCR).\nFollowing this, we compare and evaluate various PDF parsing libraries and tools in Section¬†2, including open-source libraries (Docling, Marker, MinerU, PyMuPDF) and closed-source solutions (LlamaParse, Gemini, Mistral). We provide a detailed comparison based on a qualitative analysis using a diverse set of test PDFs (slides, reports, scanned documents, and documents with complex tables). The results of this analysis are available through an interactive demo that visualizes the differences between PDF input and Markdown output for each parsing solution."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-docling",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-docling",
    "title": "PDF Parsing for LLM Input",
    "section": "Docling: A Modular PDF Processing Pipeline",
    "text": "Docling: A Modular PDF Processing Pipeline\nOverview\nDocling is a modular and extensible pipeline designed to ingest various document formats, primarily PDFs (but also DOCX, HTML, etc.), and transform them into a unified, structured representation: the DoclingDocument. The core aim is to create a standardized representation suitable for downstream tasks, such as feeding data into an LLM. Docling is in active development and is completely open-source. It provides a comprehensive technical documentation (Auer et al. 2024), (Livathinos et al. 2025).\n\n\n\n\n\n\nFigure¬†1: Sketch of Docling‚Äôs default processing pipeline. Source: (Auer et al. 2024)\n\n\n\nThe Docling pipeline, as shown in Figure¬†1, consists of the following stages:\n\nPDF Backend (for raw PDF document processing)\nAI Models (Layout Analysis, Table Structure Recognition, OCR)\nAssembly and Post-processing\n\nPDF Backend\nThe PDF backend is responsible for:\n\nRetrieving all text content and their geometric coordinates on each page.\nRendering the visual representation of each page as it appears in a PDF viewer.\n\nThese capabilities are encapsulated in Docling‚Äôs PDF backend interface. Docling provides multiple backend choices, including a custom-built PDF parser based on the low-level qpdf library. This parser is open-sourced as a separate package, docling-parse, and powers Docling‚Äôs default PDF backend. Figure¬†2 illustrates the parsing process.\n\n\n\n\n\n\n\n\nOriginal PDF page\n\n\n\n\n\n\n\nParsed PDF page\n\n\n\n\n\n\nFigure¬†2: Illustration of docling parse from source.\n\n\n\nThe docling-parse package provides a Python interface to extract text content, images, and annotations from PDFs. It also supports rendering PDF pages as images. The extracted content is serialized to JSON Listing¬†1, which can be further processed by downstream components.\n\n\n\nListing¬†1: Example of docling-parse JSON output\n\n\n{'annotations': [{'/A': {'/IsMap': False,\n    '/S': '/URI',\n    '/URI': 'https://www.deloitte.com/global/en/Industries/financial-services/perspectives/pushing-through-undercurrents.html'},\n   '/BS': {'/S': '/S', '/Type': '/Border', '/W': 0},\n   '/Border': [0, 0, 0],\n   '/H': '/N',\n   '/Rect': [474.409, 580.322, 512.947, 569.083],\n   '/Subtype': '/Link',\n   '/Type': '/Annot'},\n  {'/A': {'/IsMap': False,\n    '/S': '/URI',\n    '/URI': 'https://www.deloitte.com/global/en/Industries/financial-services/perspectives/pushing-through-undercurrents.html'},\n   '/BS': {'/S': '/S', '/Type': '/Border', '/W': 0},\n   '/Border': [0, 0, 0],\n   '/H': '/N',\n   '/Rect': [67.9417, 568.322, 286.919, 557.22],\n   '/Subtype': '/Link',\n   '/Type': '/Annot'}],\n 'original': {'cells': {'data': [[36.142,\n     711.041,\n     54.862,\n     739.753,\n     36.142,\n     711.041,\n     54.862,\n     711.041,\n     54.862,\n     739.753,\n     36.142,\n     739.753,\n     'P',\n     -1,\n     8.32,\n     '/WinAnsiEncoding',\n     'WINANSI',\n     '/TT0',\n     '/FSUTKX+OpenSans-Light',\n     False,\n     True],\n    [54.542,\n     711.041,\n     73.422,\n     739.753,\n     54.542,\n     711.041,\n     73.422,\n     711.041,\n     73.422,\n     739.753,\n     54.542,\n     739.753,\n     'u',\n     -1,\n\n\n\nAI Models\nDocling integrates several AI models for layout analysis and table structure recognition (TableFormer (Nassar et al. 2022)). Pre-trained weights (hosted on Hugging Face) and a separate package for inference code (docling-ibm-models) are available.\nLayout Analysis Model\nThis model detects and classifies various elements on a page image by predicting bounding boxes. The architecture is based on RT-DETR and retrained on DocLayNet (Pfitzmann et al. 2022) and proprietary datasets. The Docling pipeline uses page images at 72 dpi resolution. Bounding box proposals are post-processed to remove overlaps based on confidence and size and then intersected with text tokens to group them into meaningful units (e.g., paragraphs, section titles, tables).\nRT-DETR (Real-Time DEtection TRansformer) is an object detection system using a hybrid encoder to process image features and IoU-aware query selection to focus on important parts of the image.\nTable Structure Recognition\n\n\n\n\n\n\nFigure¬†3: TableFormer architecture. Source: DS4SD/docling-ibm-models\n\n\n\nThe TableFormer model (a vision transformer) recovers table structure Figure¬†3. It predicts the logical row and column structure of a table based on an input image, determining which cells belong to column headers, row headers, or the table body. TableFormer handles tables with partial or no borderlines, empty cells, row or column spans, and other complexities.\nThe Docling pipeline feeds table objects detected in the layout analysis to the TableFormer model. TableFormer structure predictions are matched back to the PDF cells to avoid re-transcription of text in the table image.\nOCR (Optical Character Recognition)\nDocling optionally supports OCR for scanned PDFs or content in embedded bitmaps. Docling supports multiple OCR engines such as EasyOCR, Tesseract, RapidOCR, and OcrMac. By default, Docling feeds a high-resolution page image (216 dpi) to the OCR engine to capture small print details.\nAssembly and Post-processing\nIn the final stage, Docling assembles all prediction results into a DoclingDocument, defined in the docling-core package. This document object is then passed through a post-processing model that augments features, such as:\n\nDocument language detection\nReading order correction\nMatching figures with captions\nLabeling metadata (title, authors, references)\n\nThe final output can be serialized to JSON or transformed into Markdown.\nAdditional post-processing steps can include:\n\nClassification of figures.\nIdentification of code blocks or formulas.\nAnnotation of pictures with LLMs (example).\n\nThe DoclingDocument is a unified representation designed to encapsulate document structure and content in a standardized way. It‚Äôs a Pydantic datatype supporting text, tables, pictures, and more. It distinguishes between the main body and auxiliary elements (‚Äúfurniture‚Äù). It retains layout information (bounding boxes) and provenance information. The DoclingDocument structure is organized into content items (texts, tables, pictures) and content structure (body, furniture, groups) Figure¬†4.\n\n\n\n\n\n\nFigure¬†4: DoclingDocument structure. Source: DS4SD/docling documentation"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-marker-pdf",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-marker-pdf",
    "title": "PDF Parsing for LLM Input",
    "section": "Marker-PDF: Accurate PDF Conversion",
    "text": "Marker-PDF: Accurate PDF Conversion\nMarker-pdf pipeline is another interesting option for PDF parsing. Raw extraction is done by pdftext, which is based on pypdfium2. Then, it uses Surya Surya GitHub. Surya is a document OCR toolkit that performs:\n\nOCR in 90+ languages.\nLine-level text detection in any language.\nLayout analysis (table, image, header, etc. detection).\nReading order detection.\nTable recognition (detecting rows/columns).\nLaTeX OCR.\n\nThe JSON output format from Surya is illustrated in Listing¬†2. This structured output includes detailed information about page dimensions, blocks of text, and individual spans with their corresponding bounding boxes, font information, and text content. Marker itself converts PDFs and images to markdown, JSON, and HTML. The key features of Marker include:\n\nSupports a range of documents in all languages.\nFormats tables, forms, equations, links, references, and code blocks.\nExtracts and saves images along with the markdown.\nRemoves headers/footers/other artifacts.\nEasily extensible with custom formatting and logic.\nOptionally boosts accuracy with an LLM.\nWorks on GPU, CPU, or MPS.\n\n\n\n\nListing¬†2: Example of Surya JSON output\n\n\n[\n  {\n    \"page\": 0,\n    \"bbox\": [\n      0,\n      0,\n      595.2760009765625,\n      841.8900146484375\n    ],\n    \"width\": 596,\n    \"height\": 842,\n    \"rotation\": 0,\n    \"blocks\": [\n      {\n        \"lines\": [\n          {\n            \"spans\": [\n              {\n                \"bbox\": [\n                  36.14179992675781,\n                  99.6307373046875,\n                  481.22967529296875,\n                  131.6307373046875\n                ],\n                \"text\": \"Pushing through undercurrents\",\n                \"rotation\": 0,\n                \"font\": {\n                  \"name\": \"OpenSans-Light\",\n                  \"flags\": 524320,\n                  \"size\": 1,\n                  \"weight\": 240\n                },\n                \"char_start_idx\": 0,\n                \"char_end_idx\": 28,\n                \"url\": \"\"\n              },\n              {\n                \"bbox\": [\n                  466.78369140625,\n                  125.09466552734375,\n                  466.78369140625,\n                  125.09466552734375\n                ],\n                \"text\": \"\\n\",\n                \"rotation\": 0,\n                \"font\": {\n                  \"name\": \"\",\n                  \"flags\": 0,\n                  \"size\": 1,\n                  \"weight\": -1\n                },\n                \"char_start_idx\": 29,\n                \"char_end_idx\": 30,\n                \"url\": \"\"\n              }\n            ],\n            \"bbox\": [\n              36.14179992675781,\n              99.6307373046875,\n              481.22967529296875,\n              131.6307373046875\n            ]\n          }\n        ],\n..."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-mineru",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-mineru",
    "title": "PDF Parsing for LLM Input",
    "section": "MinerU: Multi-Module Document Parsing",
    "text": "MinerU: Multi-Module Document Parsing\n\n\n\n\n\n\nFigure¬†5: Overview of the MinerU framework processing workflow.\n\n\n\nMinerU is a multi-module document parsing framework that uses a multi-stage approach, employing various document parsing models to process document images. The code repository is available at the MinerU GitHub Repository, and technical details can be found in the reference paper (Wang et al. 2024). MinerU provides source code, models, and documentation for parsing various document formats efficiently.\nThe MinerU framework processing workflow (Figure¬†5) consists of four stages:\n\nDocument Preprocessing: Uses PyMuPDF to read PDF files, filters out unprocessable files, and extracts PDF metadata (parseability, language type, page dimensions).\nDocument Content Parsing: Employs the PDF-Extract-Kit library for layout analysis (layout and formula detection). Applies different recognizers to various regions: OCR for text and titles, formula recognition for formulas, and table recognition for tables.\nDocument Content Post-Processing: Removes invalid regions, stitches content according to regional positioning, and obtains positioning, content, and sorting information for different document regions.\nFormat Conversion: Generates user-required formats, such as Markdown, for subsequent use.\n\nDocument Preprocessing\nThis stage focuses on filtering unprocessable PDFs and obtaining PDF metadata:\n\nLanguage Identification: Currently processes Chinese and English documents.\nContent Garbled Detection: Identifies text-based PDFs with garbled text.\nScanned PDF Identification: Distinguishes between text-based and scanned PDFs.\nPage Metadata Extraction: Extracts document metadata such as total page count, page dimensions, and other attributes.\n\nDocument Content Parsing\nMinerU uses the PDF-Extract-Kit model library to detect different types of regions and recognize their content:\n\nLayout Analysis: Identifies different types of elements and their regions on a page.\nFormula Detection: Detects inline and displayed formulas.\nFormula Recognition: Recognizes formula images into LaTeX source code using the UniMERNet model.\nTable Recognition: Extracts tabular data from visual table images using TableMaster and StructEqTable.\nOCR: Applies Paddle-OCR to recognize text regions.\n\nDocument Content Post-Processing\nThis stage addresses content ordering by handling the relationships between Bounding Boxes (BBoxes):\n\nContainment Relationships: Removes formulas and text blocks contained within image and table regions.\nPartial Overlap Relationships: Shrinks partially overlapping text boxes and ensures the integrity of text when overlapping with tables/images.\nSegmentation Algorithm: Divides the page into regions based on human reading order (‚Äútop to bottom, left to right‚Äù).\n\nModels Overview\n\n\n\nTable¬†1: MinerU models overview\n\n\n\n\n\n\n\n\n\n\nTask Type\nDescription\nModels\n\n\n\n\nLayout Detection\nLocate different elements in a document: including images, tables, text, titles, formulas\nDocLayout-YOLO_ft, YOLO-v10_ft, LayoutLMv3_ft\n\n\nFormula Detection\nLocate formulas in documents: including inline and block formulas\nYOLOv8_ft\n\n\nFormula Recognition\nRecognize formula images into LaTeX source code\nUniMERNet\n\n\nOCR\nExtract text content from images (including location and recognition)\nPaddleOCR\n\n\nTable Recognition\nRecognize table images into corresponding source code (LaTeX/HTML/Markdown)\nPaddleOCR+TableMaster, StructEqTable"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#pdf-sample-dataset",
    "href": "posts/pdf-parsing/pdf-parsing.html#pdf-sample-dataset",
    "title": "PDF Parsing for LLM Input",
    "section": "PDF Sample Dataset",
    "text": "PDF Sample Dataset\nWe test the different PDF parsing options on a small but diverse dataset of PDFs. These PDFs are available at the following link: PDF Parsing Dataset. The dataset contains different types of PDFs to cover various difficulties faced by PDF parsers:\n\nSlides\nImage-only PDFs (requiring OCR)\nReports\nTables\n\nThe PDF files located in the pdfs directory were sourced from the following locations:\n\nXC9500_CPLD_Family-1-4.pdf: Downloaded from https://media.digikey.com/pdf/Data%20Sheets/AMD/XC9500_CPLD_Family.pdf\n2023-conocophillips-aim-presentation-1-7.pdf: Downloaded from https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf\n\nThe following four PDF files are sourced from the RAG blog benchmark, specifically from the associated Google Drive folder:\n\ngx-iif-open-data.pdf\ndeloitte-tech-risk-sector-banking.pdf\nlife-sciences-smart-manufacturing-services-peak-matrix-assessment-2023.pdf\ndttl-tax-technology-report-2023.pdf"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#qualitative-results",
    "href": "posts/pdf-parsing/pdf-parsing.html#qualitative-results",
    "title": "PDF Parsing for LLM Input",
    "section": "Qualitative Results",
    "text": "Qualitative Results\nNote: The following results are based on a limited set of tests and should be considered indicative rather than scientifically rigorous.\n\nYou can directly compare the output Markdown results at the Hugging Face Space demo: pdf-parsing-demo.\n\nDocling provides decent results for text extraction, layout analysis, and table recognition. The OCR support is also a valuable addition for scanned PDFs. Very recently, it provides support for image description example, which is a very promising feature.\n\n\n\n\n\n\nFigure¬†6: Docling parsing results\n\n\n\nLlamaParse in default mode also provides good results. It provides descriptions of charts and images and is able to extract tables. However, the quality of the chart and image descriptions is not top-tier. It is also a closed-source solution.\n\n\n\n\n\n\nFigure¬†7: LlamaParse parsing results\n\n\n\nMarker provides good results and a description of the images using Gemini.\n\n\n\n\n\n\nFigure¬†8: Marker parsing results\n\n\n\nPyMuPDF in default mode is a more raw extraction approach, parsing the raw content. The other libraries tend to build upon more fundamental extraction libraries such as PyMuPDF. The results generally require further refinement.\n\n\n\n\n\n\nFigure¬†9: Pymupdf parsing results\n\n\n\nGemini is very good at parsing the content and providing a description of the images. It is versatile and effective. However, it is closed source.\n\n\n\n\n\n\nFigure¬†10: Gemini parsing results\n\n\n\n\n\n\n\n\n\nFigure¬†11: Gemini parsing results (suppl.)\n\n\n\nMistral OCR is a closed-source solution that provides good results. It is particularly effective at extracting text and tables. However, it does not support image description by default.\n\n\n\n\n\n\nFigure¬†12: Mistral parsing results\n\n\n\nMinerU is not part of the app because it is not easy to split the output by page, and tables are under HTML format. However, the results are good, and the layout is very good. It doesn‚Äôt support image description. MinerU is a very promising library.\n\n\n\n\n\n\nFigure¬†13: MinerU parsing results\n\n\n\nWe tested unstructured (free version), which did not produce satisfying results.\nMarkitdown is a very new library that we did not test. It seems to rely on ad-hoc solutions and is simply an aggregator of solutions.\nWe also tested (not extensively) some cloud-based solutions for PDF parsing such as Adobe PDF Extract API, AWS Textract and Azure AI Document intelligence but we were not convinced by the first results. Apparently, they seem to be very good at extracting for large quantity of pdf following the same template.\nIn terms of table extraction capabilities, we tested Camelot, a specialized library specifically designed for extracting tables from PDFs. Camelot excels at handling both lattice and stream type tables, making it particularly effective for complex table extraction tasks. The library is well-documented and maintained, with comprehensive documentation available on their GitHub repository. The results were good for table extraction. Camelot is a dedicated library for tables.\nConclusion: The choice of PDF parsing library depends on the specific requirements of the task at hand. or general-purpose parsing with a strong emphasis on image understanding, consider Gemini. However, be aware that this is a closed-source solution. If an open-source solution is preferred, Docling, Marker and MinerU are strong contenders. For tasks requiring specialized table extraction, Camelot is a reliable choice."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why I Write\nWelcome to my blog, a personal space where I share notes on my work and explorations in science, machine learning, and the broader tech landscape. These posts serve as a journal for my thoughts and discoveries, but I hope they also provide value and inspiration to anyone interested in these dynamic fields. Whether you‚Äôre a fellow researcher, a tech enthusiast, or simply curious, I invite you to explore and engage with the ideas I present here. Feel free to reach out if you‚Äôd like to connect or collaborate on exciting projects !\nIf you find my work interesting, please consider citing it in your research.\nShort Bio\nI am an AI Research Scientist working at the intersection of academic research and industry. I am currently a postdoctoral researcher at CREST, supervised by Arnak Dalalyan, where my research focuses on generative modeling, with particular interest in diffusion models and flow matching. This work builds on a PhD in statistics and over six years of experience in data science and artificial intelligence.\nMy career spans both applied research and real-world deployment. I began at Thales, working on time series analysis, signal processing, object detection, and anomaly detection. I later joined Iktos, where I designed deep learning models for drug discovery, before contributing to foundational biological models at MBZUAI and GenBio AI.\nIn parallel with research, I maintain strong ties to industry and entrepreneurship. In 2025, I explored venture creation by developing proofs of concept for AI-driven decision-making tools, while freelancing on projects ranging from LLM observability platforms (Rollstack) to improving Retrieval-Augmented Generation systems (Clarifeye). I am driven by the challenge of translating cutting-edge theory into impactful, scalable AI systems."
  },
  {
    "objectID": "posts/raman/raman.html",
    "href": "posts/raman/raman.html",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "",
    "text": "This blog post is inspired by an interview homework and a Kaggle competition. The objective is to provide an overview of Raman spectroscopy and provide some ML approaches to analyze Raman spectra. The associated code is available on GitHub at raman-spectra.\nIn the first part, Section¬†1, we review the physical principles underlying Raman scattering, the distinction between Stokes and anti-Stokes lines, the relationship between Raman and IR selection rules, and the main components of a Raman spectrometer.\nThe second part, Section¬†2, uses the Dig4Bio Raman Transfer Learning Challenge dataset as a concrete example. Through the accompanying notebook, we perform a brief exploratory data analysis and motivate the need for a robust preprocessing pipeline when working with real-world spectra from multiple instruments. The dedicated section Section¬†2.3 then details the individual steps of this pipeline‚Äîspectral cropping, Whitaker‚ÄìHayes cosmic-ray removal, Savitzky‚ÄìGolay denoising, and ASPLS baseline correction‚Äîboth conceptually and mathematically, following their implementation in RamanSPy.\nThe third part Section¬†3, focuses on predicting analyte concentrations from preprocessed spectra. We start with a classical chemometrics baseline using PLS Canonical (PLSW2A), reviewing the underlying algorithm, the geometry of scores, loadings, and rotation matrices, and how these pieces assemble into a linear map from spectra to concentrations. The goal is to provide a transparent, end-to-end example of a supervised learning pipeline‚Äîfrom raw spectra, through preprocessing, to a calibrated regression model.\nFinally, the accompanying codebase extends beyond traditional PLS models and includes an experimental transformer-based toolkit, SpecBERT, for spectral analysis. SpecBERT adapts BERT/ViT-style encoders to 1D Raman spectra, using Masked Spectral Modeling (MSM) for self-supervised pretraining on large unlabeled spectral collections, followed by supervised fine-tuning for regression or classification. This implementation is intentionally labeled as experimental: it was largely generated with AI assistance as a proof of concept and should be treated as a starting point rather than production-ready software. Nevertheless, it illustrates how modern representation-learning techniques can complement classical chemometric methods when spectral data are abundant but accurate concentration labels are scarce."
  },
  {
    "objectID": "posts/raman/raman.html#introduction-what-is-raman-spectroscopy",
    "href": "posts/raman/raman.html#introduction-what-is-raman-spectroscopy",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Introduction: What is Raman Spectroscopy?",
    "text": "Introduction: What is Raman Spectroscopy?\nRaman Spectroscopy is a non-destructive analytical technique used to observe vibrational, rotational, and other low-frequency modes in a system. It provides a structural ‚Äúfingerprint‚Äù by which molecules can be identified and characterized.\nThe technique is named after Indian physicist Sir C. V. Raman, who, along with his student K. S. Krishnan, discovered the phenomenon in 1928, earning Raman the Nobel Prize in Physics in 1930.\nAt its core, Raman spectroscopy relies on the inelastic scattering of monochromatic light, usually from a laser. When light interacts with a molecule, most of it is scattered at the same energy (and wavelength) as the incident light. This is called Rayleigh Scattering. However, a tiny fraction of the light (about 1 in 10 million photons) is scattered at a different energy. This inelastic scattering is the Raman Effect, and the energy difference provides the chemical and structural information.\nWe illustrate a Raman spectrum in Figure¬†1.\n\n\n\n\n\n\nFigure¬†1: Example Raman spectrum from the Kaiser dataset showing characteristic peaks for glucose, sodium acetate, and magnesium sulfate. Peak locations are based on literature values. Sample concentrations: glucose 10.59 g/L, sodium acetate 1.11 g/L, and magnesium sulfate 3.48 g/L. (Lange et al. 2025, fig. 1)"
  },
  {
    "objectID": "posts/raman/raman.html#the-theoretical-principle-of-raman-spectroscopy",
    "href": "posts/raman/raman.html#the-theoretical-principle-of-raman-spectroscopy",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "The Theoretical Principle of Raman Spectroscopy",
    "text": "The Theoretical Principle of Raman Spectroscopy\n\nMolecular Vibrations & Degrees of Freedom\nMolecules are not static. Their atoms are constantly in motion, and these motions can be categorized. Raman spectroscopy is a tool to measure the energy of one specific type of motion: vibration.\n\nDegrees of Freedom (\\(3N\\)): For a molecule with \\(N\\) atoms, each atom can move independently in three dimensions \\((x, y, z)\\). This gives a total of \\(3N\\) possible movements for the entire molecule.\nPartitioning the Motion: These \\(3N\\) motions are not all vibrations. They are a combination of:\n\nTranslational Motion (3 degrees): The entire molecule moving as a single unit through space (up/down, left/right, forward/backward). This accounts for \\(3\\) of the \\(3N\\) motions.\nRotational Motion (2 or 3 degrees): The entire molecule spinning around its center of mass. For a non-linear molecule it accounts for another \\(3\\) motions, and for a linear molecule it accounts for \\(2\\) motions.\nVibrational Motion (The Remainder): Any motion that is left over is an internal vibration‚Äîthe stretching, bending, twisting, or rocking of the chemical bonds themselves.\n\nNumber of Vibrational Modes for a non-linear molecule = \\(3N - 6\\)\nNumber of Vibrational Modes for a linear molecule = \\(3N - 5\\)\n\n\n\nThis simple calculation tells a scientist the theoretical number of fundamental vibrations a molecule has, which helps in interpreting a complex spectrum by knowing how many peaks to look for.\n\n\nVibrational Energy\nThe energy associated with molecular vibrations is quantized. A vibrating bond cannot have any arbitrary amount of energy. It can only exist at discrete energy levels, like the rungs of a ladder.\n\nQuantum Harmonic Oscillator (QHO): This is a simplified physical model used to describe these energy levels. The formula \\(E_n = h(n + 1/2)\\nu\\) describes the energy of each level, where \\(n\\) is the quantum number (0, 1, 2‚Ä¶), \\(h\\) is Planck‚Äôs constant, and \\(\\nu\\) is the natural frequency of the vibration. This frequency depends on the strength of the bond and the mass of the atoms.\nEnergy Change: Raman spectroscopy doesn‚Äôt measure the absolute energy of a level (\\(E_n\\)), but rather the energy difference required to jump from one level to another (typically from \\(n=0\\) to \\(n=1\\)). This energy difference, \\(\\Delta E = h\\nu\\), is what we see as a peak in the spectrum, reported as a Raman Shift in wavenumbers (\\(cm^{-1}\\)).\n\n\n\nRaman Scattering\nThis is the core of the Raman effect, explaining the interaction between light and the molecule that produces the signal. When a laser photon strikes a molecule, it pushes the molecule into an unstable, extremely short-lived ‚Äúvirtual state.‚Äù The molecule immediately relaxes from this state by emitting a new photon.\nThree things can happen:\n\nRayleigh Scattering (Elastic): The molecule relaxes back to the exact same vibrational energy level it started from. The emitted photon has the exact same energy as the incident laser photon. This is the most common event by far and contains no chemical information.\nStokes Raman Scattering (Inelastic): The molecule starts in the ground state (\\(n=0\\)), is excited to the virtual state, and then relaxes to a higher vibrational level (\\(n=1\\)). Because the molecule has absorbed energy, the emitted photon must have less energy than the incident photon. The energy difference corresponds exactly to the vibrational energy \\(h\\nu\\). This is the signal we typically measure as it is most intense.\nAnti-Stokes Raman Scattering (Inelastic): The molecule starts in an already excited vibrational state (\\(n=1\\)), is excited to the virtual state, and then relaxes down to the ground state (\\(n=0\\)). The molecule has lost energy, so the emitted photon has more energy than the incident photon.\n\nAt room temperature, the vast majority of molecules are in the ground state, as described by the Boltzmann distribution. Therefore, Stokes peaks are always significantly more intense (stronger) than Anti-Stokes peaks.\n\n\nIR and Raman Active Vibrations\nThis section explains the ‚Äúrules‚Äù that determine whether a specific vibration will produce a Raman signal. This is the critical difference between Raman and Infrared (IR) spectroscopy.\n\nThe Raman Rule - Change in Polarizability: A vibration is ‚ÄúRaman active‚Äù only if it causes a change in the molecule‚Äôs polarizability. Polarizability is a measure of how easily the electron cloud of a molecule can be distorted by an external electric field (like that of the laser).\nThe IR Rule - Change in Dipole Moment: A vibration is ‚ÄúIR active‚Äù only if it causes a change in the molecule‚Äôs dipole moment.\n\nThis difference leads to complementarity:\n\nSymmetric vibrations (e.g., C-C, S-S) often cause large changes in polarizability. They are strong in Raman but weak or absent in IR.\nAsymmetric vibrations involving polar bonds (e.g., C=O) cause large changes in dipole moment. They are strong in IR but may be weak in Raman.\nFor molecules with a center of symmetry (like CO‚ÇÇ), the Rule of Mutual Exclusion states that a vibration cannot be both IR and Raman active.\n\nTo fully grasp this, we summarize the differences between dipole moment and polarizability in the table below.\n\n\n\n\n\n\n\n\nFeature\nDipole Moment (Œº)\nPolarizability (Œ±)\n\n\n\n\nNature\nIntrinsic, permanent property\nInduced, response property\n\n\nOrigin\nUnequal sharing of electrons (electronegativity difference)\nDistortion of electron cloud by an external electric field\n\n\nExistence\nExists even without an external field\nDescribes the potential to be distorted; distortion occurs only in a field\n\n\nAnalogy\nA permanent magnet\nThe ‚Äúsquishiness‚Äù of a ball\n\n\nKey Example\nH‚ÇÇO is polar (large dipole moment).\nBenzene is non-polar (zero dipole moment) but highly polarizable.\n\n\nSpectroscopy Rule\nChange in Dipole Moment during vibration -&gt; IR Active\nChange in Polarizability during vibration -&gt; Raman Active\n\n\n\n\n\nFluorescence and Raman Scattering\nFluorescence and Raman scattering are both optical phenomena that involve the interaction of light with matter, but they differ fundamentally in their mechanisms and the information they provide.\n\n\n\n\n\n\n\n\nFeature\nRaman Scattering\nFluorescence\n\n\n\n\nMechanism\nScattering process involving a ‚Äúvirtual state.‚Äù\nAbsorption-Emission process involving real electronic states.\n\n\nTimescale\nVirtually instantaneous (~femtoseconds)\nDelayed by nanoseconds (fluorescence lifetime)\n\n\nEnergy Shift\nSmall, specific shifts (Raman Shifts) corresponding to vibrational energies.\nLarge, broad shift (Stokes Shift) corresponding to the energy gap between electronic states.\n\n\nInformation Provided\nA sharp ‚Äúfingerprint‚Äù of specific molecular vibrations.\nA broad signal indicating the presence of a fluorescent molecule.\n\n\nEfficiency / Intensity\nExtremely Inefficient (1 in 10‚Å∑ photons) -&gt; Very Weak Signal\nVery Efficient (can be &gt;90%) -&gt; Extremely Strong Signal"
  },
  {
    "objectID": "posts/raman/raman.html#instrumentation-a-raman-spectrometer",
    "href": "posts/raman/raman.html#instrumentation-a-raman-spectrometer",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Instrumentation: A Raman Spectrometer",
    "text": "Instrumentation: A Raman Spectrometer\nA Raman spectrometer is designed to isolate and detect the extremely weak Raman scattered light from the overwhelmingly strong Rayleigh scattered light.\nThe essential components are:\n\nLight Source (Laser): A powerful, stable, monochromatic light source. Common lasers include Visible (e.g., 532 nm, 633 nm) and Near-Infrared (NIR) (e.g., 785 nm, 1064 nm) which reduces fluorescence.\nSample Illumination and Collection Optics: Lenses or microscope objectives focus the laser onto the sample and collect the scattered light. In micro-Raman spectroscopy, this allows analysis of areas down to ~1 micron.\nOptical Filter (Notch or Edge Filter): The most critical component. It blocks the intense Rayleigh scattered light while allowing the Stokes and/or Anti-Stokes light to pass through.\nDispersive Element (Spectrograph): A diffraction grating separates the Raman signal into its constituent wavelengths.\nDetector: A highly sensitive detector, almost universally a Charge-Coupled Device (CCD) camera, is used to detect the weak signal and capture the entire spectrum at once."
  },
  {
    "objectID": "posts/raman/raman.html#the-raman-spectrum-interpretation",
    "href": "posts/raman/raman.html#the-raman-spectrum-interpretation",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "The Raman Spectrum: Interpretation",
    "text": "The Raman Spectrum: Interpretation\nThe output from the spectrometer is a Raman spectrum, see for example the spectrum in Figure¬†1.\n\nX-axis: Raman Shift (\\(cm^{-1}\\)). This corresponds to the vibrational frequency (energy) of molecular bonds. A key advantage is that the Raman shift is independent of the laser wavelength used.\nY-axis: Intensity (Arbitrary Units). This is proportional to the concentration of the molecule and how ‚ÄúRaman active‚Äù that particular vibration is.\n\nEach peak in the spectrum corresponds to a specific molecular vibration.\n\nFingerprint Region (approx. 400 - 1800 \\(cm^{-1}\\)): This region is rich with peaks from various bending and stretching modes. The complex pattern is unique to each molecule, making it a ‚Äúmolecular fingerprint‚Äù for identification.\nFunctional Group Region (approx. 1800 - 4000 \\(cm^{-1}\\)): This region contains peaks from specific functional groups, such as C-H stretches (~2800-3100 \\(cm^{-1}\\)), C‚â°C triple bonds (~2100-2300 \\(cm^{-1}\\)), and O-H stretches (~3200-3600 \\(cm^{-1}\\))."
  },
  {
    "objectID": "posts/raman/raman.html#advantages-and-limitations",
    "href": "posts/raman/raman.html#advantages-and-limitations",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Advantages and Limitations",
    "text": "Advantages and Limitations\n\nAdvantages:\n\nNon-Destructive: Uses low-power light and typically does not damage the sample.\nMinimal Sample Preparation: Can analyze solids, liquids, and gases directly, often through containers.\nWater is a Weak Scatterer: Excellent for analyzing biological or aqueous samples, a major advantage over IR.\nHigh Spatial Resolution: Micro-Raman can provide chemical information on a micron scale.\nSpecificity: Provides a sharp, well-resolved ‚Äúfingerprint‚Äù spectrum for unambiguous identification.\nRemote & In-situ Analysis: Can be coupled with fiber optics for remote monitoring.\n\n\n\nLimitations:\n\nWeak Signal: The Raman effect is inherently inefficient, sometimes requiring long acquisition times.\nFluorescence Interference: The most significant challenge. Fluorescence from the sample can overwhelm the weak Raman signal. This is often mitigated by using NIR lasers (e.g., 785 nm).\nSample Heating: High-intensity lasers can heat or burn sensitive samples.\nNot Ideal for Metals: Cannot be used for elemental analysis of metals as they lack molecular bonds."
  },
  {
    "objectID": "posts/raman/raman.html#raman-challenge-dataset-analysis",
    "href": "posts/raman/raman.html#raman-challenge-dataset-analysis",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Raman Challenge Dataset Analysis",
    "text": "Raman Challenge Dataset Analysis\nThis notebook analyzes the dig-4-bio-raman transfer learning challenge dataset and demonstrates how to load and work with the multi-instrument Raman spectroscopy data.\n\nfrom pathlib import Path\n\nfrom typing import Literal\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport ramanspy as rp\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n\n\"\"\"\nDefine common paths and preprocessing pipelines.\n\nThis section sets up:\n- Data directory path\n- Standard preprocessing pipeline (without normalization)\n  - Cropping to fingerprint region (300-1942 cm‚Åª¬π)\n  - Cosmic ray removal (Whitaker-Hayes)\n  - Denoising (Savitzky-Golay filter)\n  - Baseline correction (ASPLS)\n  \nNote: Normalization is commented out as it can destroy concentration information\nfor regression tasks.\n\"\"\"\n\n# Data directory path\ndata_path = Path(\"../data/dig-4-bio-raman-transfer-learning-challenge\")\n\n# Fingerprint region\nMIN_WAVENUMBER = 300\nMAX_WAVENUMBER = 1942\n\n# Standard preprocessing pipeline without normalization\n# This pipeline is suitable for regression tasks where we want to preserve\n# concentration-related intensity information\nstandard_pipeline_without_normalisation = rp.preprocessing.Pipeline([\n    rp.preprocessing.misc.Cropper(region=(MIN_WAVENUMBER, MAX_WAVENUMBER)),  # Fingerprint region\n    rp.preprocessing.despike.WhitakerHayes(),          # Remove cosmic rays\n    rp.preprocessing.denoise.SavGol(window_length=9, polyorder=3),  # Smooth noise\n    rp.preprocessing.baseline.ASPLS(),                 # Remove baseline drift\n    # rp.preprocessing.normalise.MinMax()  # Excluded for regression tasks\n])\n\n\n\"\"\"\nUtility functions\n\"\"\"\n\ndef find_spectral_columns(df: pd.DataFrame) -&gt; tuple[list[str], list[str], np.ndarray]:\n    \"\"\"\n    Identifies spectral data columns by checking if the column name can be converted to a float.\n    This is a robust way to separate metadata from spectral data.\n    \"\"\"\n    spectral_cols = []\n    metadata_cols = []\n    for col in df.columns:\n        try:\n            float(col)\n            spectral_cols.append(col)\n        except (ValueError, TypeError):\n            metadata_cols.append(col)\n\n    wavenumbers = pd.to_numeric(spectral_cols)\n    return metadata_cols, spectral_cols, wavenumbers\n\n\ndef build_background_subtraction_pipeline(background: rp.Spectrum) -&gt; rp.preprocessing.Pipeline:\n    \"\"\"\n    Builds a background subtraction pipeline with a specified region for cropping.\n    \"\"\"\n    background = rp.preprocessing.misc.Cropper(region=(MIN_WAVENUMBER, MAX_WAVENUMBER)).apply(background)\n    return rp.preprocessing.Pipeline([\n        rp.preprocessing.misc.Cropper(region=(MIN_WAVENUMBER, MAX_WAVENUMBER)),\n        rp.preprocessing.misc.BackgroundSubtractor(background=background)\n    ])\n\n\nDataset Description\nThis dataset contains Raman spectra from 8 different instruments measuring samples with known concentrations of three analytes:\n\nGlucose (g/L)\nSodium Acetate (Na_acetate, g/L)\nMagnesium Sulfate (Mg_SO4, g/L)\n\nThe instruments included are: Anton 532, Anton 785, Kaiser, Metrohm, Mettler Toledo, Tec 5, Timegate, and Tornado.\nMeasurement Protocol:\n\nAll samples were measured at room temperature with instrument-specific exposure times\nMost instruments acquired 5 replicate spectra per sample; Kaiser and Timegate recorded a single spectrum per sample\nDue to varying instrument availability, the sample sets differ across spectrometers\n\nInstrument Characteristics: Each instrument exhibits unique properties that must be accounted for in analysis:\n\nWavenumber ranges: Vary from ~200 to ~3500 cm‚Åª¬π across instruments\nSpectral resolution: Differs between instruments\n\nLet‚Äôs start by loading data from one instrument (anton_532) to understand the data structure and visualize a sample spectrum.\n\n\"\"\"\nLoad and visualize a sample spectrum from the anton_532 instrument.\n\nThis demonstrates:\n1. Loading CSV data with spectral columns (wavenumbers) and metadata\n2. Separating spectral data from metadata\n3. Creating a RamanSPy SpectralContainer for analysis\n4. Visualizing a raw spectrum with its concentration information\n\"\"\"\n\n# Load data from anton_532 instrument\ndf = pd.read_csv(data_path / \"anton_532.csv\").drop(columns=[\"MSM_present\", \"fold_idx\"])\n\n# Separate spectral columns (numeric column names = wavenumbers) from metadata\nmetadata_cols, spectral_cols, wavenumbers = find_spectral_columns(df)\n\n# Create RamanSPy SpectralContainer for easier manipulation\nspectral_container = rp.SpectralContainer(\n    spectral_data=df[spectral_cols].values,\n    spectral_axis=wavenumbers\n)\n\n# Select first spectrum for visualization\nspectrum = spectral_container[0]\nmetadata = df[metadata_cols].iloc[0].to_dict()\n\n# Create informative title with concentration information\ntitle = (\n    f\"Original Spectrum - \"\n    f\"glucose {metadata['glucose']:.4f} g/L - \"\n    f\"Na_acetate {metadata['Na_acetate']:.4f} g/L - \"\n    f\"Mg_SO4 {metadata['Mg_SO4']:.4f} g/L\"\n)\n\n# Plot the spectrum\nrp.plot.spectra(spectra=[spectrum], plot_type=\"single\", title=title)\nrp.plot.show()\n\n\n\n\n\n\n\n\n\n\nData Structure Overview\nLet‚Äôs examine the structure of the loaded dataset to understand how the data is organized. The DataFrame contains:\n\nMetadata columns: Concentration values (glucose, Na_acetate, Mg_SO4) and other sample information\nSpectral columns: Numeric column names representing wavenumber values (cm‚Åª¬π)\nEach row represents a single spectrum measurement with its associated concentration metadata\n\n\n# Display the first few rows of the dataset\ndf.head(5)\n\n\n\n\n\n\n\n\n200.0\n202.0\n204.0\n206.0\n208.0\n210.0\n212.0\n214.0\n216.0\n218.0\n...\n3488.0\n3490.0\n3492.0\n3494.0\n3496.0\n3498.0\n3500.0\nglucose\nNa_acetate\nMg_SO4\n\n\n\n\n0\n7519.06\n7582.15\n7379.73\n7054.04\n6818.64\n6684.24\n6562.20\n6398.03\n6256.17\n6135.72\n...\n6539.65\n6475.20\n6573.45\n6488.44\n6284.99\n6216.51\n6409.21\n0.26335\n1.4357\n1.44101\n\n\n1\n7414.24\n7502.98\n7327.93\n7002.97\n6760.01\n6638.17\n6539.84\n6360.47\n6200.67\n6080.42\n...\n6507.23\n6451.34\n6564.97\n6465.79\n6241.37\n6171.89\n6358.46\n0.26335\n1.4357\n1.44101\n\n\n2\n7376.45\n7474.90\n7304.00\n6961.46\n6691.31\n6557.30\n6470.64\n6309.66\n6177.40\n6090.05\n...\n6501.41\n6432.22\n6521.16\n6452.89\n6248.68\n6137.06\n6318.74\n0.26335\n1.4357\n1.44101\n\n\n3\n7383.60\n7456.90\n7264.59\n6938.71\n6707.66\n6594.61\n6493.20\n6307.50\n6144.74\n6033.89\n...\n6498.85\n6412.12\n6532.58\n6459.36\n6240.35\n6146.15\n6325.52\n0.26335\n1.4357\n1.44101\n\n\n4\n7345.77\n7435.40\n7263.58\n6929.82\n6671.51\n6544.19\n6459.28\n6302.71\n6158.40\n6043.02\n...\n6506.29\n6414.26\n6530.51\n6435.62\n6214.08\n6156.26\n6348.95\n0.26335\n1.4357\n1.44101\n\n\n\n\n5 rows √ó 1654 columns\n\n\n\n\n\nStep-by-Step Preprocessing Visualization\nPreprocessing is crucial for Raman spectroscopy data analysis. Raw spectra contain:\n\nBaseline drift: Broad fluorescence background\nCosmic rays: Sharp spikes from detector artifacts\nNoise: Random measurement noise\nEdge effects: Instrument-specific artifacts at spectral edges\n\nThis section visualizes how each preprocessing step transforms the spectrum, helping us understand the impact of each operation. We apply the following steps:\n\nCropping: Remove edge artifacts and focus on fingerprint region (300-1942 cm‚Åª¬π)\nCosmic Ray Removal: Remove sharp spikes (Whitaker-Hayes algorithm)\nDenoising: Smooth the spectrum (Savitzky-Golay filter)\nBaseline Correction: Remove broad fluorescence background (ASPLS)\n\nThis visualization helps understand how each preprocessing step improves signal quality and prepares the data for analysis.\n\ndf = pd.read_csv(data_path / \"anton_532.csv\").drop( columns=[\"MSM_present\", \"fold_idx\"])\nmetadata_cols, spectral_cols, wavenumbers = find_spectral_columns(df)\nspectral_container = rp.SpectralContainer(\n    spectral_data=df[spectral_cols].values,\n    spectral_axis=wavenumbers\n)\nspectrum = spectral_container[0]\nmetadata = df[metadata_cols].iloc[0].to_dict()\ntitle = f\"Original Spectrum - glucose {metadata['glucose']:.4f} gL - Na_acetate {metadata['Na_acetate']:.4f} gL - Mg_SO4 {metadata['Mg_SO4']:.4f} gL\"\nrp.plot.spectra(spectra=[spectrum], plot_type=\"single\", title=title)\nrp.plot.show()\nfor step in standard_pipeline_without_normalisation:\n    spectrum = step.apply(spectrum)\n    title = f\"{step.__class__.__name__}\"\n    rp.plot.spectra([spectrum], plot_type=\"single\", title=title)\n    rp.plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Spectral Cropping\nFrom the visualization above, we can observe:\n\nWavenumber range: Approximately 300‚Äì3600 cm‚Åª¬π\nStrong baseline/fluorescence: Broad curvature from background fluorescence\nEdge artifacts:\n\nBelow ~450 cm‚Åª¬π: Edge effects, Rayleigh tail, optical artifacts\nAbove ~3000 cm‚Åª¬π: OH stretch from water dominates the signal\n\nChemically informative region: The fingerprint region (400‚Äì1800 cm‚Åª¬π) contains the most relevant information\n\nThe reference paper Comparing machine learning methods on Raman spectra from eight different spectrometers recommends:\n\nCrop to 300‚Äì1942 cm‚Åª¬π: The largest common range across all 8 instruments\nRationale:\n\nPreserves glucose, acetate, and MgSO‚ÇÑ peaks\nRemoves high-wavenumber water OH stretch\nEliminates instrument-specific edge artifacts\nImproves cross-device robustness for transfer learning\n\n\nFor this dataset, cropping to 300‚Äì1942 cm‚Åª¬π is justified because:\n\nGlucose peaks: ~800‚Äì1150 cm‚Åª¬π ‚úì\nAcetate peak: ~920 cm‚Åª¬π ‚úì\nMgSO‚ÇÑ strong peak: ~980 cm‚Åª¬π ‚úì\nMinimal water interference: Removes dominant OH stretch region\nBetter signal-to-noise: Focuses on chemically informative region\n\nThis region contains the most relevant information for quantitative analysis while removing noise and artifacts.\n\n\nSample Structure and Organization\nThe dataset is organized around unique concentration triplets defined by the three analytes:\n\nGlucose (g/L)\nSodium Acetate (Na_acetate, g/L)\nMagnesium Sulfate (Mg_SO4, g/L)\n\nReplication Structure:\n\nMost instruments (Anton 532, Anton 785, Metrohm, Mettler Toledo, Tec 5, Tornado) recorded at least 5 replicate spectra per unique concentration triplet\nKaiser and Timegate instruments recorded only 1 spectrum per concentration triplet (no replicates)\n\nSample Numbering: To facilitate analysis and visualization, we assign a unique sample_number to each distinct concentration triplet. This allows us to:\n\nGroup spectra by their concentration profile\nSelect representative spectra from each unique sample\nTrack how preprocessing affects spectra from the same sample across different replicates\n\n\n\nExploring Concentration Distribution\nLet‚Äôs examine the distribution of unique concentration triplets in the dataset. This helps us understand:\n\nHow many unique samples (concentration combinations) are present\nThe frequency of each concentration triplet\nThe overall structure of the experimental design\n\n\ndf_metadata = df[metadata_cols]\ndf_metadata[[\"glucose\", \"Na_acetate\", \"Mg_SO4\"]].value_counts()\n\nglucose   Na_acetate  Mg_SO4  \n0.00000   0.00000     0.000000    10\n                      0.022899     5\n                      1.483750     5\n                      3.490220     5\n          0.01612     0.005016     5\n          0.38593     0.023827     5\n          0.65286     0.046091     5\n          0.71544     1.567450     5\n          0.75931     0.017697     5\n          0.76773     3.945410     5\n          1.29106     0.000000     5\n          1.37442     1.355800     5\n          1.39750     3.308900     5\n0.24955   0.08245     3.454390     5\n0.25369   0.00000     0.022609     5\n0.25636   0.67294     0.005511     5\n0.25649   0.74089     3.345560     5\n0.25787   0.00000     0.000000     5\n0.26229   0.03709     1.494600     5\n0.26335   1.43570     1.441010     5\n0.27710   0.69733     1.441180     5\n0.28256   1.33788     0.035751     5\n0.29784   1.56128     3.348590     5\n0.49945   1.33009     0.003688     5\n0.51645   1.02701     1.328620     5\n0.52096   0.00000     0.015037     5\n0.52616   0.00000     1.396560     5\n0.53269   0.68172     0.056479     5\n0.56078   1.07925     3.139890     5\n0.79631   1.06674     3.090570     5\n1.02342   0.71756     1.557450     5\n1.18537   0.00000     2.948570     5\n4.56573   0.69828     1.541520     5\n4.62097   0.97254     0.014210     5\n4.63261   0.69969     0.022235     5\n4.73407   0.67310     0.003497     5\n4.77883   0.00000     0.023272     5\n4.78962   1.03913     1.497530     5\n4.87666   0.72635     3.073870     5\n4.91288   1.03532     3.302040     5\n5.12832   0.02430     1.715070     5\n5.24672   0.02440     3.361270     5\n6.39311   0.00000     0.024451     5\n10.16120  0.00000     1.477840     5\n10.28120  0.66554     0.000000     5\n10.35390  0.00000     0.000000     5\n10.39640  0.68902     1.602520     5\n10.59310  1.11418     3.484130     5\n10.65220  0.00000     2.620210     5\n10.66050  1.13599     1.624690     5\n10.71080  0.74408     3.480790     5\n10.74480  1.05836     0.005669     5\n16.73440  0.00000     0.022216     5\nName: count, dtype: int64\n\n\n\n\nAssigning Sample Numbers\nTo facilitate analysis, we assign a unique sample_number to each distinct concentration triplet. This allows us to:\n\nGroup all replicate spectra from the same sample together\nEasily select representative spectra for visualization\nTrack samples across different preprocessing steps\n\nAfter assignment, we sort the DataFrame by sample_number to organize the data for easier analysis.\n\ndf['sample_number'] = df.groupby(['glucose', 'Na_acetate', 'Mg_SO4']).ngroup()\n\n\ndf = df.sort_values(by=\"sample_number\")\ndf\n\n\n\n\n\n\n\n\n200.0\n202.0\n204.0\n206.0\n208.0\n210.0\n212.0\n214.0\n216.0\n218.0\n...\n3490.0\n3492.0\n3494.0\n3496.0\n3498.0\n3500.0\nglucose\nNa_acetate\nMg_SO4\nsample_number\n\n\n\n\n48\n7872.51\n7949.13\n7756.47\n7430.07\n7191.07\n7054.33\n6923.01\n6728.96\n6566.42\n6446.71\n...\n8358.85\n8459.03\n8365.43\n8120.00\n7994.58\n8164.45\n0.0000\n0.0\n0.000000\n0\n\n\n47\n7855.57\n7937.75\n7754.15\n7431.91\n7196.21\n7069.05\n6947.50\n6745.40\n6582.47\n6474.62\n...\n8294.69\n8415.91\n8343.10\n8109.79\n7992.89\n8171.26\n0.0000\n0.0\n0.000000\n0\n\n\n46\n7880.69\n7968.58\n7784.68\n7442.57\n7186.43\n7063.44\n6968.17\n6777.16\n6612.35\n6503.68\n...\n8341.36\n8440.34\n8340.85\n8101.21\n7993.53\n8176.15\n0.0000\n0.0\n0.000000\n0\n\n\n49\n7816.74\n7903.64\n7718.65\n7384.09\n7142.29\n7031.14\n6940.11\n6749.42\n6575.63\n6450.89\n...\n8321.98\n8416.93\n8310.16\n8071.10\n7973.64\n8140.15\n0.0000\n0.0\n0.000000\n0\n\n\n45\n7927.66\n8014.66\n7817.07\n7476.43\n7234.11\n7111.39\n6999.47\n6808.78\n6644.33\n6525.25\n...\n8359.07\n8461.60\n8376.35\n8146.79\n8041.58\n8224.00\n0.0000\n0.0\n0.000000\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n197\n8207.04\n8318.43\n8149.32\n7811.73\n7548.74\n7407.86\n7296.73\n7115.31\n6970.95\n6873.25\n...\n9045.37\n9158.60\n9078.45\n8841.36\n8711.15\n8854.53\n16.7344\n0.0\n0.022216\n52\n\n\n196\n8297.09\n8374.05\n8181.89\n7849.53\n7608.91\n7486.01\n7376.52\n7186.54\n7024.39\n6912.22\n...\n9154.26\n9271.12\n9179.31\n8934.16\n8822.59\n8995.83\n16.7344\n0.0\n0.022216\n52\n\n\n195\n8478.47\n8586.03\n8417.81\n8090.71\n7841.44\n7706.42\n7591.60\n7406.22\n7241.26\n7116.70\n...\n9541.84\n9634.93\n9526.42\n9289.84\n9193.64\n9367.68\n16.7344\n0.0\n0.022216\n52\n\n\n198\n8144.21\n8233.26\n8056.15\n7729.10\n7491.60\n7380.43\n7282.35\n7076.46\n6901.83\n6794.68\n...\n8967.69\n9041.05\n8962.62\n8754.40\n8638.11\n8801.52\n16.7344\n0.0\n0.022216\n52\n\n\n199\n8084.45\n8189.22\n8040.56\n7728.67\n7486.21\n7363.75\n7264.74\n7068.97\n6883.15\n6744.28\n...\n8949.49\n9057.16\n8963.22\n8726.82\n8633.93\n8811.21\n16.7344\n0.0\n0.022216\n52\n\n\n\n\n270 rows √ó 1655 columns"
  },
  {
    "objectID": "posts/raman/raman.html#comparing-preprocessing-pipelines-standard-vs.-background-subtraction",
    "href": "posts/raman/raman.html#comparing-preprocessing-pipelines-standard-vs.-background-subtraction",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Comparing Preprocessing Pipelines: Standard vs.¬†Background Subtraction",
    "text": "Comparing Preprocessing Pipelines: Standard vs.¬†Background Subtraction\nThis section visualizes the effects of different preprocessing pipelines on Raman spectra from various concentrations. We compare two preprocessing approaches:\n\nStandard Pipeline: Applies cropping, cosmic ray removal, denoising, and baseline correction\nBackground Subtraction Pipeline: Crops the spectrum and subtracts a reference background spectrum\n\nKey Objectives:\n\nUnderstand how different preprocessing methods affect spectral appearance\nCompare pipeline performance on spectra from different concentrations\nAssess pipeline behavior on replicate spectra from the same concentration\n\nExpected Observations:\n\nBackground Subtraction Pipeline: When applied to spectra from different concentrations, this pipeline produces distinct spectral profiles that clearly differentiate between concentration levels. This demonstrates that the method effectively preserves concentration-dependent signal differences.\nStandard Pipeline: In contrast, the standard pipeline tends to produce more similar-looking spectra across different concentrations, as it focuses on removing artifacts and noise while preserving the overall spectral shape. This can be beneficial for certain types of analysis but may reduce concentration-dependent signal differences.\n\nImportant Note: The background subtraction pipeline uses the first spectrum in the dataset as the reference background. For spectra from the same sample (same concentration), this is the first replicate. For spectra from different samples (different concentrations), this corresponds to the first sample in the dataset, which is the zero concentration sample.\n\n# Note: Required for exporting static images in Quarto documents.\n# (Comment out these lines to enable interactive Plotly plots in Jupyter or other interactive environments.)\nimport plotly.io as pio\npio.renderers.default = 'png'\n\ndef plot_spectra_interactive(\n    df: pd.DataFrame,\n    pipeline_type: Literal[\"standard\", \"background\"] = \"standard\",\n    samples_type: Literal[\"same\", \"different\"] = \"same\",\n    height: int = 800,\n) -&gt; None:\n    metadata_cols, spectral_cols, wavenumbers = find_spectral_columns(df)\n    spectral_container = rp.SpectralContainer(\n        spectral_data=df[spectral_cols].values,\n        spectral_axis=wavenumbers,\n    )\n\n    fig_title = f\"Plot of {pipeline_type} pipeline with {samples_type} samples\"\n\n    match pipeline_type:\n        case \"standard\":\n            spectra_list = standard_pipeline_without_normalisation.apply(spectral_container)\n        case \"background\":\n            spectra_list = build_background_subtraction_pipeline(background=spectral_container[0]).apply(spectral_container)\n\n    # Series labels\n    match samples_type:\n        case \"same\":\n            series_labels = list(range(len(df)))\n        case \"different\":\n            series_labels = df[[\"glucose\", \"Na_acetate\", \"Mg_SO4\"]].apply(lambda x: f\"glucose {x['glucose']:.2f} gL - Na_acetate {x['Na_acetate']:.2f} gL - Mg_SO4 {x['Mg_SO4']:.2f} gL\", axis=1).tolist()\n\n    all_spectra_dfs = []\n    for i, spectrum in enumerate(spectra_list):\n        temp_df = pd.DataFrame({\n            'Wavenumber': spectrum.spectral_axis,\n            'Intensity': spectrum.spectral_data\n        })\n        temp_df['Series'] = series_labels[i]\n        all_spectra_dfs.append(temp_df)\n\n    # Combine all the small DataFrames into one large, tidy DataFrame\n    plot_df = pd.concat(all_spectra_dfs, ignore_index=True)\n\n    # Create the interactive plot\n    fig = px.line(\n        plot_df,\n        x='Wavenumber',\n        y='Intensity',\n        color='Series',          # Each series gets a different color line\n        title=fig_title,\n        labels={                 # Custom labels for axes\n            \"Wavenumber\": \"Wavenumber (cm‚Åª¬π)\",\n            \"Intensity\": \"Intensity (a.u.)\"\n        },\n        template='plotly_white',  # A clean template with grids\n        height=height,\n    )\n\n    # Update subplot titles to be cleaner (optional)\n    # fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[1]))\n\n    # Display the figure in the Jupyter Notebook\n    fig.show()\n\n\nComparing Pipelines on Different Concentrations\nWe first examine how each preprocessing pipeline handles spectra from different concentration levels. This helps us understand:\n\nHow well each pipeline preserves concentration-dependent spectral features\nThe ability to distinguish between different analyte concentrations\nThe impact of preprocessing on quantitative signal information\n\nWe select one representative spectrum from each unique concentration triplet to compare the pipelines on distinct samples.\n\ndifferent_samples_df = (\n    df.groupby(\"sample_number\", as_index=False)\n    .nth(0)\n)\ndifferent_samples_df.head(5)\n\n\n\n\n\n\n\n\n200.0\n202.0\n204.0\n206.0\n208.0\n210.0\n212.0\n214.0\n216.0\n218.0\n...\n3490.0\n3492.0\n3494.0\n3496.0\n3498.0\n3500.0\nglucose\nNa_acetate\nMg_SO4\nsample_number\n\n\n\n\n48\n7872.51\n7949.13\n7756.47\n7430.07\n7191.07\n7054.33\n6923.01\n6728.96\n6566.42\n6446.71\n...\n8358.85\n8459.03\n8365.43\n8120.00\n7994.58\n8164.45\n0.0\n0.00000\n0.000000\n0\n\n\n27\n7867.47\n7945.25\n7751.45\n7424.17\n7188.73\n7061.26\n6942.80\n6761.27\n6611.74\n6500.41\n...\n8623.98\n8714.13\n8628.46\n8391.91\n8265.12\n8434.56\n0.0\n0.00000\n0.022899\n1\n\n\n207\n7342.39\n7399.16\n7207.68\n6881.40\n6641.70\n6524.32\n6434.70\n6271.56\n6124.36\n6011.24\n...\n7215.12\n7326.84\n7226.49\n6984.68\n6873.00\n7048.59\n0.0\n0.00000\n1.483750\n2\n\n\n84\n7405.46\n7469.17\n7290.53\n6987.57\n6773.68\n6670.61\n6572.76\n6380.83\n6213.01\n6102.20\n...\n7914.66\n8002.84\n7926.06\n7731.41\n7653.32\n7804.36\n0.0\n0.00000\n3.490220\n3\n\n\n190\n7344.06\n7410.08\n7226.07\n6909.59\n6673.01\n6540.97\n6424.29\n6245.51\n6085.45\n5962.36\n...\n6989.71\n7082.35\n6989.41\n6777.86\n6697.66\n6873.22\n0.0\n0.01612\n0.005016\n4\n\n\n\n\n5 rows √ó 1655 columns\n\n\n\n\nplot_spectra_interactive(df=different_samples_df, pipeline_type=\"standard\", samples_type=\"different\")\n\n\n\n\n\n\n\n\n\nplot_spectra_interactive(df=different_samples_df, pipeline_type=\"background\", samples_type=\"different\")\n\n\n\n\n\n\n\n\n\n\nComparing Pipelines on Replicate Spectra\nNext, we examine how each preprocessing pipeline handles replicate spectra from the same concentration. This analysis reveals:\n\nMeasurement reproducibility: How consistent are replicate measurements after preprocessing?\nNoise characteristics: What residual variability remains after preprocessing?\nPipeline stability: How do different preprocessing methods affect replicate consistency?\n\nWe randomly select one sample (concentration triplet) and visualize all its replicate spectra to assess within-sample variability.\n\n# Draw a random sample number\nunique_sample_numbers = df[\"sample_number\"].unique()\nrandom_sample_number = np.random.choice(unique_sample_numbers)\nsame_samples_df = df[df[\"sample_number\"] == random_sample_number]\nsame_samples_df\n\n\n\n\n\n\n\n\n200.0\n202.0\n204.0\n206.0\n208.0\n210.0\n212.0\n214.0\n216.0\n218.0\n...\n3490.0\n3492.0\n3494.0\n3496.0\n3498.0\n3500.0\nglucose\nNa_acetate\nMg_SO4\nsample_number\n\n\n\n\n39\n7320.48\n7419.34\n7262.94\n6932.90\n6659.87\n6511.08\n6412.69\n6254.91\n6114.20\n6003.54\n...\n6505.33\n6628.53\n6562.14\n6346.97\n6249.52\n6424.52\n0.26229\n0.03709\n1.4946\n18\n\n\n37\n7406.77\n7464.21\n7264.81\n6934.42\n6697.93\n6580.23\n6468.39\n6254.99\n6088.88\n6006.50\n...\n6514.41\n6650.11\n6582.10\n6355.25\n6250.23\n6432.15\n0.26229\n0.03709\n1.4946\n18\n\n\n35\n7400.97\n7517.01\n7365.01\n7025.82\n6752.42\n6622.32\n6541.35\n6361.16\n6198.60\n6089.23\n...\n6604.90\n6699.57\n6597.09\n6374.15\n6294.01\n6489.00\n0.26229\n0.03709\n1.4946\n18\n\n\n38\n7331.09\n7410.65\n7238.04\n6920.44\n6677.50\n6547.06\n6443.18\n6276.78\n6130.41\n6014.91\n...\n6526.00\n6659.57\n6572.41\n6339.60\n6255.41\n6434.59\n0.26229\n0.03709\n1.4946\n18\n\n\n36\n7386.98\n7465.57\n7288.87\n6969.29\n6732.37\n6613.17\n6507.77\n6306.82\n6140.97\n6039.23\n...\n6593.39\n6661.59\n6571.50\n6375.59\n6293.98\n6460.65\n0.26229\n0.03709\n1.4946\n18\n\n\n\n\n5 rows √ó 1655 columns\n\n\n\n\nplot_spectra_interactive(df=same_samples_df, pipeline_type=\"standard\", samples_type=\"same\")\n\n\n\n\n\n\n\n\n\nplot_spectra_interactive(df=same_samples_df, pipeline_type=\"background\", samples_type=\"same\")\n\n\n\n\n\n\n\n\n\n\nUnderstanding Measurement Variability\nThe visualization above reveals important insights about measurement variability:\nKey Observations:\n\nIntrinsic Experimental Variability: Even with identical sample concentrations, replicate spectra show some variation. This reflects:\n\nNatural measurement noise inherent in Raman spectroscopy\nSlight variations in sample positioning or laser focus\nInstrument-specific measurement characteristics\nEnvironmental factors (temperature, humidity, etc.)\n\nPipeline Impact on Variability:\n\nThe standard pipeline typically reduces variability by removing noise and artifacts, making replicates appear more similar\nThe background subtraction pipeline may preserve or even amplify certain types of variability, depending on the reference background used\n\nImplications for Analysis:\n\nUnderstanding this variability is crucial for setting appropriate model expectations\nThe residual variability after preprocessing represents the lower bound of prediction uncertainty\nThis information helps in designing robust models that can handle real-world measurement conditions\n\n\nThis variability analysis is essential for developing reliable quantitative models that can generalize to new measurements."
  },
  {
    "objectID": "posts/raman/raman.html#sec-preprocessing-raman",
    "href": "posts/raman/raman.html#sec-preprocessing-raman",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Preprocessing Algorithms",
    "text": "Preprocessing Algorithms\nRaw Raman spectra often contain artifacts and noise that must be addressed before meaningful analysis can be performed. These preprocessing steps are essential for accurate peak identification, quantitative analysis, and machine learning applications. The standard preprocessing pipeline for Raman spectroscopy typically includes: spectral cropping to focus on regions of interest, cosmic ray removal to eliminate detector artifacts, denoising to reduce random noise, and baseline correction to remove fluorescence and drift effects. We describe each of these algorithms below, following the implementations available in RamanSPy (Georgiev et al. 2024).\n\nSpectral Cropping\nThe first step in preprocessing is often to reduce the spectral range to the region of interest. For Raman spectroscopy, this typically means focusing on the ‚Äúfingerprint region‚Äù (approximately 400-1800 \\(cm^{-1}\\)) where most characteristic molecular vibrations appear. Cropping serves multiple purposes: it removes noisy edge regions that may contain artifacts, focuses computational resources on the relevant spectral features, and reduces the data size for downstream processing.\nThe algorithm is straightforward: given a wavenumber axis \\(w\\) and spectrum \\(s\\), we create a boolean mask and apply it to both:\n\\[\nw_{\\text{cropped}} = w[w_{\\text{min}} \\leq w \\leq w_{\\text{max}}]\n\\]\n\\[\ns_{\\text{cropped}} = s[w_{\\text{min}} \\leq w \\leq w_{\\text{max}}]\n\\]\nThis operation has linear computational complexity \\(O(n)\\) where \\(n\\) is the spectrum length, making it an efficient first step in any preprocessing pipeline.\n\n\nCosmic Ray Removal (Whitaker-Hayes)\nCosmic rays striking the CCD detector during spectral acquisition can produce sharp intensity spikes that are easily mistaken for genuine Raman peaks. The Whitaker-Hayes algorithm (Whitaker and Hayes 2018) provides a robust method for identifying and removing these artifacts.\nThe algorithm identifies spikes using modified z-scores computed from the first differences of the spectrum. The modified z-score is more robust to outliers than the standard z-score because it uses the median absolute deviation (MAD) instead of the standard deviation:\n\\[\n\\text{MAD} = \\text{median}(|x_i - \\text{median}(x)|)\n\\]\n\\[\n\\text{Modified Z-score} = 0.6745 \\times \\frac{x_i - \\text{median}(x)}{\\text{MAD}}\n\\]\nThe constant 0.6745 makes the MAD consistent with the standard deviation for normal distributions. The algorithm examines the first differences of the spectrum:\n\\[\n\\Delta s_i = s_{i+1} - s_i\n\\]\nBecause cosmic rays cause sharp discontinuities, examining differences amplifies these sudden changes while smoothing gradual variations, making spike detection more sensitive than direct intensity analysis.\nFor each detected spike at position \\(i\\) (where \\(|z_{\\Delta s_i}| &gt; \\tau\\), typically \\(\\tau = 8\\)), the algorithm iteratively replaces it with the local mean of non-spike neighbors within a kernel of size \\(k\\):\n\\[\ns'_i =\n\\begin{cases}\n\\text{mean}(s_j : j \\in N(i), \\text{spike}_j = \\text{False}) & \\text{if spike}_i = \\text{True} \\\\\ns_i & \\text{otherwise}\n\\end{cases}\n\\]\nwhere \\(N(i) = \\{\\max(0, i-k), \\ldots, \\min(n-1, i+k)\\}\\) is the neighborhood around position \\(i\\).\nThe algorithm‚Äôs robustness comes from its use of median-based statistics, which are resistant to outliers. The iterative nature allows it to handle multiple adjacent spikes, and the local mean replacement preserves peak shapes while removing artifacts.\n\n\nDenoising (Savitzky-Golay)\nRandom noise in Raman spectra can obscure genuine peaks and complicate analysis. The Savitzky-Golay filter (Savitzky and Golay 1964) reduces noise while preserving peak shapes and positions through local polynomial regression, making it superior to simple moving averages for spectroscopic data.\nFor each point in the spectrum, the algorithm:\n\nConsiders a window of window_length points centered at that position\nFits a polynomial of degree polyorder to these points using least squares\nReplaces the center point with the polynomial‚Äôs predicted value\n\nMathematically, for a window centered at position \\(i\\) with \\(m = (\\text{window\\_length} - 1) / 2\\):\nThe polynomial fit is:\n\\[\nP(x) = a_0 + a_1 x + a_2 x^2 + \\ldots + a_p x^p\n\\]\nwhere \\(p = \\text{polyorder}\\). The least squares objective minimizes:\n\\[\n\\sum_{j=i-m}^{i+m} [y_j - P(x_j)]^2\n\\]\nThe smoothed value is then \\(y'_i = P(x_i)\\).\nThe Savitzky-Golay filter can be efficiently implemented as a convolution with precomputed coefficients that depend only on window_length and polyorder, not on the data itself, giving it \\(O(n)\\) complexity.\nWhy it works: Averaging over the window reduces random noise by a factor of approximately \\(\\sqrt{\\text{window\\_length}}\\), while polynomial fitting preserves local trends better than a simple moving average. This means narrow peaks are not significantly broadened, and the filter can even compute derivatives simultaneously, which is useful for peak finding algorithms.\nParameter selection: The window_length must be odd and greater than polyorder. RamanSPy defaults to window_length = 9 and polyorder = 3 (cubic polynomial), which provides a good balance between noise reduction and peak preservation. Smaller windows (5-7) preserve narrow peaks better but provide less noise reduction, while larger windows (11-15) provide stronger noise reduction but may broaden narrow peaks.\n\n\nBaseline Correction (ASPLS)\nBaseline drift in Raman spectra arises from fluorescence, sample holder interference, or optical effects. This slowly-varying background can obscure peaks and complicate quantitative analysis. The ASPLS (Adaptive Smoothness Penalized Least Squares) method (Zhang, Chen, and Liang 2010) is an advanced variant of asymmetric least squares (Eilers and Boelens 2005) that adapts the smoothness constraint based on local signal characteristics.\nThe goal is to find a baseline \\(z\\) that minimizes:\n\\[\nQ = \\sum_i w_i(y_i - z_i)^2 + \\lambda \\sum_j (\\Delta^d z_j)^2\n\\]\nwhere:\n\n\\(y_i\\) = observed spectrum values\n\\(z_i\\) = baseline values\n\\(w_i\\) = asymmetric weights\n\\(\\lambda\\) = smoothness penalty parameter\n\\(\\Delta^d\\) = \\(d\\)-th order difference operator (typically \\(d=2\\))\n\nThis formulation balances two competing objectives: fidelity (baseline close to data) and smoothness (baseline is smooth).\nAsymmetric weighting is the key innovation that prevents the baseline from following peaks. Unlike symmetric least squares, ASPLS uses:\n\\[\nw_i =\n\\begin{cases}\np & \\text{if } y_i &gt; z_i \\text{ (points above baseline)} \\\\\n1-p & \\text{if } y_i \\leq z_i \\text{ (points below baseline)}\n\\end{cases}\n\\]\nwhere \\(p \\ll 0.5\\) (typically \\(p = 0.01\\)). Since peaks are above the baseline, they receive low weight, while baseline regions receive high weight. This forces the fitted baseline to pass under the peaks.\nSmoothness penalty: The difference operator \\(\\Delta^d\\) penalizes roughness. For second-order differences (\\(d=2\\), the default):\n\\[\n\\Delta^2 z = [z_2 - 2z_1 + z_0, z_3 - 2z_2 + z_1, \\ldots]\n\\]\nThis penalizes curvature changes (second derivative), encouraging a smooth baseline.\nAdaptive smoothness is the method‚Äôs key advantage: it varies \\(\\lambda\\) locally based on signal characteristics:\n\\[\n\\lambda_i^{\\text{adaptive}} = \\frac{\\lambda}{1 + \\alpha \\times \\text{feature}_i}\n\\]\nwhere \\(\\alpha\\) is an adaptation strength parameter and \\(\\text{feature}_i\\) represents local signal characteristics (e.g., magnitude of residual). This means:\n\nStrong peaks (large residuals) ‚Üí \\(\\lambda\\) decreases ‚Üí more flexible baseline\nFlat regions ‚Üí \\(\\lambda\\) remains high ‚Üí smooth baseline\n\nThis prevents the baseline from ‚Äúchasing‚Äù strong peaks while maintaining smoothness in flat regions.\nThe algorithm iteratively solves the weighted penalized least squares problem, updating the asymmetric weights based on whether points are above or below the current baseline estimate, until convergence. RamanSPy defaults to \\(\\lambda = 10^5\\) and diff_order = 2, which work well for most Raman spectra.\n\n\nComplete Preprocessing Pipeline\nThe standard RamanSPy preprocessing workflow (Georgiev et al. 2024) applies these algorithms in a specific order, as the sequence of operations is critical for optimal results:\n\nCropping - Focus on region of interest, reduce data size\nWhitaker-Hayes Despiking - Remove cosmic rays\nSavitzky-Golay Smoothing - Reduce noise\nASPLS Baseline Correction - Remove baseline drift\nNormalization (optional) - Scale for comparison\n\nThe default parameters recommended by RamanSPy provide a good starting point for most applications: cropping to 600-1800 \\(cm^{-1}\\) (or full fingerprint region 400-1800 \\(cm^{-1}\\)), despiking with kernel_size=3 and threshold=8.0, smoothing with window_length=9 and polyorder=3, and baseline correction with \\(\\lambda = 10^5\\) and diff_order=2. These parameters should be kept consistent across all spectra within a study to ensure reproducibility and comparability."
  },
  {
    "objectID": "posts/raman/raman.html#raman-challenge-machine-learning-model-for-concentration-prediction",
    "href": "posts/raman/raman.html#raman-challenge-machine-learning-model-for-concentration-prediction",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Raman Challenge: Machine Learning Model for Concentration Prediction",
    "text": "Raman Challenge: Machine Learning Model for Concentration Prediction\nThis notebook demonstrates the development of Partial Least Squares (PLS) regression models for predicting concentrations of three compounds from Raman spectra:\n\nGlucose (C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ)\nSodium acetate (Na_acetate)\nMagnesium sulfate (Mg_SO4)\n\nThe analysis is performed one instrument at a time to account for instrument-specific variations in spectral characteristics. This approach ensures that models are tailored to the unique spectral response and calibration of each instrument, improving prediction accuracy and model reliability.\n\nfrom pathlib import Path\nfrom typing import Literal\nimport pandas as pd\nimport numpy as np\nimport ramanspy as rp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GroupKFold, cross_val_predict\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.metrics import r2_score, root_mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n\n\n# Data directory path\ndata_path = Path(\"../data/dig-4-bio-raman-transfer-learning-challenge\")\n\n# Fingerprint region\nMIN_WAVENUMBER = 300\nMAX_WAVENUMBER = 1942\n\n# Standard preprocessing pipeline without normalization\nstandard_pipeline_without_normalisation = rp.preprocessing.Pipeline([\n    rp.preprocessing.misc.Cropper(region=(MIN_WAVENUMBER, MAX_WAVENUMBER)),  # Fingerprint region\n    rp.preprocessing.despike.WhitakerHayes(),          # Remove cosmic rays\n    rp.preprocessing.denoise.SavGol(window_length=9, polyorder=3),  # Smooth noise\n    rp.preprocessing.baseline.ASPLS(),                 # Remove baseline drift\n    # rp.preprocessing.normalise.MinMax()  # Excluded for regression tasks\n])\n\n# Standard preprocessing pipeline with normalization\nstandard_pipeline = rp.preprocessing.Pipeline([\n    rp.preprocessing.misc.Cropper(region=(MIN_WAVENUMBER, MAX_WAVENUMBER)),\n    rp.preprocessing.despike.WhitakerHayes(),\n    rp.preprocessing.denoise.SavGol(window_length=9, polyorder=3),\n    rp.preprocessing.baseline.ASPLS(),\n    rp.preprocessing.normalise.MinMax()\n])\n\n\n\"\"\"\nUtility functions\n\"\"\"\n\ndef find_spectral_columns(df: pd.DataFrame) -&gt; tuple[list[str], list[str], np.ndarray]:\n    \"\"\"\n    Identifies spectral data columns by checking if the column name can be converted to a float.\n    This is a robust way to separate metadata from spectral data.\n    \"\"\"\n    spectral_cols = []\n    metadata_cols = []\n    for col in df.columns:\n        try:\n            float(col)\n            spectral_cols.append(col)\n        except (ValueError, TypeError):\n            metadata_cols.append(col)\n\n    wavenumbers = pd.to_numeric(spectral_cols)\n    return metadata_cols, spectral_cols, wavenumbers\n\n\nNormalization: A Critical Preprocessing Decision\nA fundamental preprocessing decision in quantitative Raman spectroscopy is whether to normalize the spectra. This choice involves an important trade-off:\n\nNormalize: Ensures the model learns from chemical differences rather than measurement artifacts (laser power fluctuations, integration time variations, focusing differences, etc.). This helps remove instrumental effects that could confound the analysis.\nDon‚Äôt normalize: Preserves the relationship between peak height and concentration, which is essential for quantitative regression. Absolute intensity information can be directly related to analyte concentration in ideal conditions.\n\nObjective: Display some random spectra from the dataset to visually assess the intensity variations and determine whether normalization is necessary for this analysis.\n\n\"\"\"\nDisplay some random spectra from the dataset\n\"\"\"\n\n# Load data from anton_532 instrument\ndf = pd.read_csv(data_path / \"anton_532.csv\").drop(columns=[\"MSM_present\", \"fold_idx\"])\n\n# Separate spectral columns (numeric column names = wavenumbers) from metadata\nmetadata_cols, spectral_cols, wavenumbers = find_spectral_columns(df)\n\nspectral_container = rp.SpectralContainer(\n    spectral_data=df[spectral_cols].values,\n    spectral_axis=wavenumbers\n)\n\nspectral_container = standard_pipeline_without_normalisation.apply(spectral_container)\n\n# Select 2 random spectra\nrandom_indices = np.random.choice(len(df), size=2, replace=False)\n\nfor i in random_indices:\n    spectrum = spectral_container[i]\n    metadata = df[metadata_cols].iloc[i].to_dict()\n\n    # Create informative title with concentration information\n    title = (\n        f\"Spectrum (preprocessed without normalization) - \"\n        f\"glucose {metadata['glucose']:.4f} g/L - \"\n        f\"Na_acetate {metadata['Na_acetate']:.4f} g/L - \"\n        f\"Mg_SO4 {metadata['Mg_SO4']:.4f} g/L\"\n    )\n\n    # Plot the spectrum\n    rp.plot.spectra(spectra=[spectrum], plot_type=\"single\", title=title)\n    rp.plot.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussion: Normalization in the Preprocessing Pipeline\nWe observe that without normalization, spectral intensities can reach very high values (e.g., ~350 arbitrary units), which is problematic for machine learning models that expect scaled data. However, choosing the appropriate normalization method is challenging due to noise and subtle spectral variations that must be preserved for quantitative analysis.\nVariations in overall spectral intensity can result from several instrumental factors:\n\nLaser power fluctuations: Changes in laser output power between measurements\nFocusing/sampling position: Variations in sample positioning and laser focus\nIntegration time: Differences in measurement duration\nDetector response: Variations in detector sensitivity or calibration\n\nNormalization corrects for these instrumental effects, ensuring that differences between spectra reflect chemical composition rather than measurement artifacts.\n\nCommon Normalization Methods\n1. Area Under the Curve (AUC) Scaling\n\nWhat it does: Divides each spectrum by its total area, making all spectra have area = 1.\nWhy it‚Äôs problematic for regression: Destroys concentration information. Stronger peaks (indicating higher concentration) lead to a larger total area, but AUC scaling erases this difference by forcing all spectra to have the same area.\nBest for: Classification tasks (focus on spectral shape, not absolute intensity). Avoid for regression.\n\n2. MinMax Scaling\n\nWhat it does: Scales each spectrum so its minimum is 0 and maximum is 1.\nProblems for regression:\n\nScaling depends on the highest point, which may be a compound peak, solvent peak, or noise artifact.\nInconsistent scaling between samples distorts the linear relationship between peak height and concentration.\nDifferent samples may have different scaling factors, making quantitative comparisons difficult.\n\nBest for: Some ML classification tasks. Use with caution for regression.\n\n3. Internal Standard Normalization (Preferred for Quantitative Analysis)\n\nWhat it is: Normalize by dividing the analyte peak(s) by a stable reference peak (from a constant component or added internal standard).\nWhy it‚Äôs best: Corrects for experimental variations (laser power, integration time, focus, detector response) while preserving the linear relationship needed for regression.\nFormula:\nNormalized Signal = (Analyte Peak Intensity) / (Standard Peak Intensity)\nBest for: Quantitative analysis and regression.\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\nMethod\nWhat it Does\nPro\nCon\nBest for\n\n\n\n\nAUC Scaling\nArea = 1 for all spectra\nFocuses on shape\nDestroys intensity info\nClassification\n\n\nMinMax Scaling\nScales to [0, 1]\nPreserves relative peak heights\nInconsistent; distorts quantitative info\nSome classification\n\n\nInternal Standard\nRatio to stable peak\nCorrects for variations; preserves linearity\nRequires suitable standard/peak\nRegression, quantitative\n\n\n\n\n\nDecision\nUnfortunately, internal standard normalization is difficult to implement in this context due to the lack of a suitable reference peak or internal standard in the dataset. Since we need to normalize the spectra due to very high intensity values that would otherwise cause numerical issues in the models, we choose to apply MinMax scaling while being aware of its limitations for quantitative regression tasks. This represents a pragmatic compromise between correcting for instrumental variations and preserving as much quantitative information as possible.\n\n\n\nMachine Learning Approach\nWe now train PLS regression models for concentration prediction, analyzing each instrument separately to account for instrument-specific spectral characteristics.\n\n1. Quantitative Modeling\nModel Choice:\n\nPartial Least Squares (PLS) Regression is the industry standard for spectroscopic data analysis. It effectively handles:\n\nHigh dimensionality: Many wavenumbers (features) relative to the number of samples\nSevere multicollinearity: Highly correlated spectral features (adjacent wavenumbers are typically very similar)\nNoise reduction: Projects data onto a lower-dimensional space of latent variables\n\nPLS finds latent variables (components) that maximize covariance between spectra (X) and concentrations (Y), making it ideal for quantitative prediction tasks.\n\nValidation Strategy:\n\nUse GroupKFold cross-validation with the sample fold_idx ID as the grouping variable.\nThis ensures that spectra from the same sample are not split across training and validation sets, providing a more realistic performance estimate that accounts for sample-to-sample variation.\nThis approach prevents data leakage and gives a better estimate of how the model will perform on truly unseen samples.\n\nHyperparameter Tuning:\n\nThe key parameter for PLS is the number of latent variables (components).\nTrain models with increasing numbers of components (1-20) and select the number that yields the lowest Root Mean Squared Error of Cross-Validation (RMSECV).\nToo few components may underfit the data, while too many may overfit to noise.\n\n\n\n2. Model Evaluation & Interpretation\nPerformance Metrics:\n\nEvaluate the final model using the optimal number of components.\nCalculate R¬≤ (coefficient of determination) and RMSE (Root Mean Squared Error) from cross-validated predictions.\nR¬≤ indicates the proportion of variance explained by the model (closer to 1 is better).\nRMSE indicates the average prediction error in concentration units (g/L).\n\nVisualization:\n\nGenerate Predicted vs.¬†Actual plots to provide a clear visual assessment of model performance.\nInspect PLS loadings to identify which spectral regions (wavenumbers) the model uses for predictions, enabling chemical interpretation and validation of the model‚Äôs behavior.\n\n\n# Loading the data\n\ninstrument: Literal[\"anton_532\", \"anton_785\", \"kaiser\", \"metrohm\", \"mettler_toledo\", \"tec5\", \"timegate\", \"tornado\"] = \"anton_532\"\n\ndf = pd.read_csv(data_path / f\"{instrument}.csv\").drop(columns=[\"MSM_present\"])\ndf\n\n\n\n\n\n\n\n\n200.0\n202.0\n204.0\n206.0\n208.0\n210.0\n212.0\n214.0\n216.0\n218.0\n...\n3490.0\n3492.0\n3494.0\n3496.0\n3498.0\n3500.0\nglucose\nNa_acetate\nMg_SO4\nfold_idx\n\n\n\n\n0\n7519.06\n7582.15\n7379.73\n7054.04\n6818.64\n6684.24\n6562.20\n6398.03\n6256.17\n6135.72\n...\n6475.20\n6573.45\n6488.44\n6284.99\n6216.51\n6409.21\n0.26335\n1.43570\n1.44101\n0\n\n\n1\n7414.24\n7502.98\n7327.93\n7002.97\n6760.01\n6638.17\n6539.84\n6360.47\n6200.67\n6080.42\n...\n6451.34\n6564.97\n6465.79\n6241.37\n6171.89\n6358.46\n0.26335\n1.43570\n1.44101\n0\n\n\n2\n7376.45\n7474.90\n7304.00\n6961.46\n6691.31\n6557.30\n6470.64\n6309.66\n6177.40\n6090.05\n...\n6432.22\n6521.16\n6452.89\n6248.68\n6137.06\n6318.74\n0.26335\n1.43570\n1.44101\n0\n\n\n3\n7383.60\n7456.90\n7264.59\n6938.71\n6707.66\n6594.61\n6493.20\n6307.50\n6144.74\n6033.89\n...\n6412.12\n6532.58\n6459.36\n6240.35\n6146.15\n6325.52\n0.26335\n1.43570\n1.44101\n0\n\n\n4\n7345.77\n7435.40\n7263.58\n6929.82\n6671.51\n6544.19\n6459.28\n6302.71\n6158.40\n6043.02\n...\n6414.26\n6530.51\n6435.62\n6214.08\n6156.26\n6348.95\n0.26335\n1.43570\n1.44101\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n265\n7832.00\n7938.65\n7758.85\n7401.53\n7122.56\n6982.76\n6884.94\n6702.59\n6537.42\n6417.34\n...\n7790.07\n7890.47\n7786.45\n7544.57\n7448.89\n7652.59\n10.71080\n0.74408\n3.48079\n4\n\n\n266\n7799.25\n7874.64\n7683.97\n7335.51\n7069.06\n6933.33\n6827.90\n6633.28\n6466.11\n6358.28\n...\n7735.00\n7846.83\n7756.49\n7530.37\n7427.52\n7587.48\n10.71080\n0.74408\n3.48079\n4\n\n\n267\n7752.49\n7823.53\n7631.56\n7311.92\n7087.74\n6973.90\n6867.04\n6686.11\n6522.25\n6391.02\n...\n7713.69\n7774.98\n7694.02\n7490.46\n7377.11\n7541.42\n10.71080\n0.74408\n3.48079\n4\n\n\n268\n7757.55\n7803.91\n7606.17\n7295.83\n7082.62\n6976.21\n6870.14\n6678.14\n6495.80\n6352.90\n...\n7702.41\n7801.14\n7693.90\n7459.89\n7372.70\n7559.06\n10.71080\n0.74408\n3.48079\n4\n\n\n269\n7726.52\n7799.61\n7607.11\n7281.07\n7047.06\n6924.05\n6810.98\n6627.21\n6470.48\n6357.52\n...\n7624.59\n7756.07\n7675.02\n7459.47\n7398.34\n7572.52\n10.71080\n0.74408\n3.48079\n4\n\n\n\n\n270 rows √ó 1655 columns\n\n\n\n\n# Processing the data\n\nmetadata_cols, spectral_cols, wavenumbers = find_spectral_columns(df)\ngroups = df[\"fold_idx\"].to_numpy()\nspectra_raw_df = df[spectral_cols]\nY = df[[\"glucose\", \"Na_acetate\", \"Mg_SO4\"]].to_numpy()\n\nspectral_container = rp.SpectralContainer(\n    spectral_data=spectra_raw_df.values,\n    spectral_axis=wavenumbers,\n)\n\nprocessed_spectra = standard_pipeline.apply(spectral_container)\nX_processed = processed_spectra.spectral_data\n\n\n# We will test a range of components (latent variables)\n\nn_components_range = np.arange(1, 21)\nrmsecv_scores = []\nr2cv_scores = []\n\n# Use GroupKFold to ensure spectra from the same sample are not split across folds\n# This gives a more realistic performance estimate. We use fold_idx as the group.\ncv_splitter = GroupKFold(n_splits=5) # 5 groups for 5 folds\n\nfor n_comp in n_components_range:\n    pls = PLSRegression(n_components=n_comp)\n    # Predict using cross-validation\n    y_pred_cv = cross_val_predict(pls, X_processed, Y, cv=cv_splitter, groups=groups)\n    rmsecv = root_mean_squared_error(Y, y_pred_cv)\n    r2cv = r2_score(Y, y_pred_cv)\n    rmsecv_scores.append(rmsecv)\n    r2cv_scores.append(r2cv)\n    print(f\"RMSECV with {n_comp} components: {rmsecv:.4f}\")\n    print(f\"R2CV with {n_comp} components: {r2cv:.4f}\")\n\n# Find the optimal number of components (the one with the minimum RMSECV)\noptimal_n_components = n_components_range[np.argmin(rmsecv_scores)]\nprint(f\"Optimal number of PLS components found: {optimal_n_components}\")\n\n# --- Plot RMSECV vs. Number of Components, and R2 vs. Number of Components ---\nfig, ax1 = plt.subplots(figsize=(10, 6))\ncolor_rmse = 'tab:red'\ncolor_r2 = 'tab:blue'\nax1.set_xlabel('Number of Components', fontsize=12)\nax1.set_ylabel('RMSE of Cross-Validation (g/L)', color=color_rmse, fontsize=12)\nax1.plot(n_components_range, rmsecv_scores, 'o-', mfc='w', color=color_rmse, label='RMSECV')\nax1.axvline(x=optimal_n_components, color='r', linestyle='--', label=f'Optimal Components = {optimal_n_components}')\nax1.tick_params(axis='y', labelcolor=color_rmse)\nax1.legend(loc='upper left')\nax2 = ax1.twinx()\nax2.set_ylabel('R¬≤ of Cross-Validation', color=color_r2, fontsize=12)\nax2.plot(n_components_range, r2cv_scores, '-', color=color_r2, label='R¬≤')\nax2.tick_params(axis='y', labelcolor=color_r2)\nax2.legend(loc='upper right')\nplt.title('RMSECV and R¬≤ vs. Number of PLS Components', fontsize=16)\nfig.tight_layout()\nplt.show()\n\nRMSECV with 1 components: 2.0614\nR2CV with 1 components: 0.0021\nRMSECV with 2 components: 1.9507\nR2CV with 2 components: 0.1411\nRMSECV with 3 components: 2.0019\nR2CV with 3 components: 0.1405\nRMSECV with 4 components: 1.8235\nR2CV with 4 components: 0.2169\nRMSECV with 5 components: 1.6823\nR2CV with 5 components: 0.3021\nRMSECV with 6 components: 1.6804\nR2CV with 6 components: 0.2938\nRMSECV with 7 components: 1.5609\nR2CV with 7 components: 0.3664\nRMSECV with 8 components: 1.5313\nR2CV with 8 components: 0.3533\nRMSECV with 9 components: 1.5394\nR2CV with 9 components: 0.3469\nRMSECV with 10 components: 1.5132\nR2CV with 10 components: 0.3592\nRMSECV with 11 components: 1.5052\nR2CV with 11 components: 0.3569\nRMSECV with 12 components: 1.5003\nR2CV with 12 components: 0.3398\nRMSECV with 13 components: 1.5246\nR2CV with 13 components: 0.3177\nRMSECV with 14 components: 1.5137\nR2CV with 14 components: 0.3010\nRMSECV with 15 components: 1.5298\nR2CV with 15 components: 0.2747\nRMSECV with 16 components: 1.5465\nR2CV with 16 components: 0.2485\nRMSECV with 17 components: 1.5418\nR2CV with 17 components: 0.2396\nRMSECV with 18 components: 1.5498\nR2CV with 18 components: 0.2290\nRMSECV with 19 components: 1.5826\nR2CV with 19 components: 0.1800\nRMSECV with 20 components: 1.5891\nR2CV with 20 components: 0.1416\nOptimal number of PLS components found: 12\n\n\n\n\n\n\n\n\n\n\n# =============================================================================\n# Final Model Training and Evaluation\n# =============================================================================\n\n# Create the final PLS model with the optimal number of components\nfinal_pls_model = PLSRegression(n_components=optimal_n_components)\n\n# Get cross-validated predictions for the final model\ny_pred_final = cross_val_predict(final_pls_model, X_processed, Y, cv=cv_splitter, groups=groups)\n\n# Calculate final performance metrics (overall)\nfinal_rmse = root_mean_squared_error(Y, y_pred_final)\nfinal_r2 = r2_score(Y, y_pred_final)\n\nprint(f\"Final Cross-Validated Model Performance (Overall):\")\nprint(f\"R¬≤: {final_r2:.4f}\")\nprint(f\"RMSE: {final_rmse:.4f} g/L\")\n\n# Calculate individual metrics for each target\ntarget_names = [\"Glucose\", \"Na_acetate\", \"Mg_SO4\"]\ntarget_metrics = []\n\nfor i, target_name in enumerate(target_names):\n    Y_target = Y[:, i]\n    y_pred_target = y_pred_final[:, i]\n    r2_target = r2_score(Y_target, y_pred_target)\n    rmse_target = root_mean_squared_error(Y_target, y_pred_target)\n    target_metrics.append((r2_target, rmse_target))\n    print(f\"{target_name}:\")\n    print(f\"  R¬≤: {r2_target:.4f}\")\n    print(f\"  RMSE: {rmse_target:.4f} g/L\")\n\n# --- Predicted vs. Actual Plots for All Targets ---\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor i, (target_name, ax) in enumerate(zip(target_names, axes)):\n    Y_target = Y[:, i]\n    y_pred_target = y_pred_final[:, i]\n    r2_target, rmse_target = target_metrics[i]\n    \n    sns.scatterplot(x=Y_target, y=y_pred_target, alpha=0.7, ax=ax)\n    ax.plot([Y_target.min(), Y_target.max()], [Y_target.min(), Y_target.max()], \n            'r--', lw=2, label='1:1 Line')\n    ax.set_title(f'Predicted vs. Actual {target_name}', fontsize=14)\n    ax.set_xlabel('Actual Concentration (g/L)', fontsize=11)\n    ax.set_ylabel('Predicted Concentration (g/L)', fontsize=11)\n    ax.text(0.05, 0.95, f'R¬≤ = {r2_target:.4f}\\nRMSE = {rmse_target:.4f} g/L', \n            transform=ax.transAxes, fontsize=10, \n            bbox=dict(facecolor='white', alpha=0.8), verticalalignment='top')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect('equal', adjustable='box')\n\nplt.tight_layout()\nplt.show()\n\nFinal Cross-Validated Model Performance (Overall):\nR¬≤: 0.3398\nRMSE: 1.5003 g/L\nGlucose:\n  R¬≤: 0.3915\n  RMSE: 3.3367 g/L\nNa_acetate:\n  R¬≤: -0.1697\n  RMSE: 0.5485 g/L\nMg_SO4:\n  R¬≤: 0.7974\n  RMSE: 0.6159 g/L\n\n\n\n\n\n\n\n\n\n\n# =============================================================================\n# Model Interpretation - Inspecting PLS Loadings\n# =============================================================================\n\n# To interpret the model, we fit it on all data and inspect the loadings.\n# The loadings show which variables (wavenumbers) are important for each component.\nfinal_pls_model.fit(X_processed, Y)\nloadings = final_pls_model.x_loadings_\n\n# --- Plot Loadings for the first few components ---\nplt.figure(figsize=(12, 6))\n# Ensure wavenumbers and loadings have matching dimensions\nwavenumbers_plot = wavenumbers\nif hasattr(wavenumbers, 'values'):\n    wavenumbers_plot = wavenumbers.values\nif len(wavenumbers_plot) != loadings.shape[0]:\n    # Try to slice wavenumbers to match loadings shape\n    wavenumbers_plot = wavenumbers_plot[:loadings.shape[0]]\nfor i in range(min(3, optimal_n_components)): # Plot up to the first 3 components\n    plt.plot(wavenumbers_plot, loadings[:, i], label=f'Component {i+1}')\n\nplt.title('PLS Loadings', fontsize=16)\nplt.xlabel('Raman Shift (cm‚Åª¬π)', fontsize=12)\nplt.ylabel('Loading Weight', fontsize=12)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# =============================================================================\n# Separate PLS Models Per Target\n# =============================================================================\n# \n# Training one PLS model per target allows:\n# - Better interpretability: each target has its own loadings showing which \n#   spectral regions are important for that specific compound\n# - Independent optimization: each target can have its own optimal number of \n#   components\n# - Potentially better performance: models can focus on the specific spectral \n#   regions relevant to each compound\n\ntarget_names = [\"glucose\", \"Na_acetate\", \"Mg_SO4\"]\ntarget_models = {}\ntarget_optimal_components = {}\ntarget_metrics_separate = {}\ntarget_predictions_separate = {}\n\n# Store single model metrics for comparison (from cell 11)\ntarget_metrics_single = {}\nfor i, target_name in enumerate(target_names):\n    Y_target = Y[:, i]\n    y_pred_target = y_pred_final[:, i]\n    r2_target = r2_score(Y_target, y_pred_target)\n    rmse_target = root_mean_squared_error(Y_target, y_pred_target)\n    target_metrics_single[target_name] = (r2_target, rmse_target)\n\n# Train separate PLS model for each target\nfor target_idx, target_name in enumerate(target_names):\n    print(f\"\\n{'='*60}\")\n    print(f\"Training PLS model for {target_name}\")\n    print(f\"{'='*60}\")\n    \n    # Extract single target\n    Y_target = Y[:, target_idx].reshape(-1, 1)  # Reshape to (n_samples, 1)\n    \n    # Find optimal number of components for this target\n    n_components_range = np.arange(1, 21)\n    rmsecv_scores = []\n    r2cv_scores = []\n    \n    for n_comp in n_components_range:\n        pls = PLSRegression(n_components=n_comp)\n        y_pred_cv = cross_val_predict(pls, X_processed, Y_target, cv=cv_splitter, groups=groups)\n        rmsecv = root_mean_squared_error(Y_target, y_pred_cv)\n        r2cv = r2_score(Y_target, y_pred_cv)\n        rmsecv_scores.append(rmsecv)\n        r2cv_scores.append(r2cv)\n        print(f\"  {n_comp:2d} components: RMSECV={rmsecv:.4f}, R¬≤CV={r2cv:.4f}\")\n    \n    # Find optimal number of components\n    optimal_n_components = n_components_range[np.argmin(rmsecv_scores)]\n    target_optimal_components[target_name] = optimal_n_components\n    print(f\"\\n  Optimal components for {target_name}: {optimal_n_components}\")\n    \n    # Train final model and get cross-validated predictions\n    final_pls = PLSRegression(n_components=optimal_n_components)\n    y_pred_final_separate = cross_val_predict(final_pls, X_processed, Y_target, cv=cv_splitter, groups=groups)\n    \n    # Calculate metrics\n    r2_final = r2_score(Y_target, y_pred_final_separate)\n    rmse_final = root_mean_squared_error(Y_target, y_pred_final_separate)\n    target_metrics_separate[target_name] = (r2_final, rmse_final)\n    target_models[target_name] = final_pls\n    target_predictions_separate[target_name] = y_pred_final_separate\n    \n    print(f\"  Final R¬≤: {r2_final:.4f}, RMSE: {rmse_final:.4f} g/L\")\n\n# =============================================================================\n# Plot Loadings for Each Target (Much More Interpretable!)\n# =============================================================================\n\nfig, axes = plt.subplots(len(target_names), 1, figsize=(14, 5*len(target_names)))\n\n# Handle case where we have only one target (axes would be 1D, not 2D)\nif len(target_names) == 1:\n    axes = [axes]\n\nfor idx, target_name in enumerate(target_names):\n    model = target_models[target_name]\n    # Fit the model on all data to get loadings\n    model.fit(X_processed, Y[:, idx].reshape(-1, 1))\n    loadings = model.x_loadings_\n    \n    # Use wavenumbers from processed spectra to match loadings dimensions\n    wavenumbers_processed = processed_spectra.spectral_axis\n    if hasattr(wavenumbers_processed, 'values'):\n        wavenumbers_processed = wavenumbers_processed.values\n    \n    ax = axes[idx]\n    n_comp_to_plot = min(3, target_optimal_components[target_name])\n    for comp_idx in range(n_comp_to_plot):\n        ax.plot(wavenumbers_processed, loadings[:, comp_idx], \n                label=f'Component {comp_idx+1}', alpha=0.7, linewidth=1.5)\n    \n    ax.set_title(f'PLS Loadings for {target_name} (Optimal: {target_optimal_components[target_name]} components)', \n                 fontsize=14, fontweight='bold')\n    ax.set_xlabel('Raman Shift (cm‚Åª¬π)', fontsize=12)\n    ax.set_ylabel('Loading Weight', fontsize=12)\n    ax.legend(loc='best')\n    ax.grid(True, alpha=0.3)\n    ax.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# =============================================================================\n# Performance Comparison: Single vs. Separate Models\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"Performance Comparison: Single Multi-Output Model vs. Separate Models\")\nprint(\"=\"*80)\nprint(f\"{'Target':&lt;15} {'Single Model R¬≤':&lt;18} {'Separate Model R¬≤':&lt;18} {'R¬≤ Improvement':&lt;15} {'Single RMSE':&lt;15} {'Separate RMSE':&lt;15} {'RMSE Improvement':&lt;15}\")\nprint(\"-\"*80)\n\nfor target_name in target_names:\n    single_r2, single_rmse = target_metrics_single[target_name]\n    separate_r2, separate_rmse = target_metrics_separate[target_name]\n    r2_improvement = separate_r2 - single_r2\n    rmse_improvement = single_rmse - separate_rmse  # Positive = better (lower RMSE)\n    \n    print(f\"{target_name:&lt;15} {single_r2:&gt;17.4f} {separate_r2:&gt;17.4f} {r2_improvement:&gt;+14.4f} {single_rmse:&gt;14.4f} g/L {separate_rmse:&gt;14.4f} g/L {rmse_improvement:&gt;+14.4f} g/L\")\n\nprint(\"=\"*80)\n\n# =============================================================================\n# Predicted vs. Actual Plots for Separate Models\n# =============================================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, (target_name, ax) in enumerate(zip(target_names, axes)):\n    Y_target = Y[:, idx]\n    y_pred_target = target_predictions_separate[target_name].flatten()\n    r2_target, rmse_target = target_metrics_separate[target_name]\n    \n    sns.scatterplot(x=Y_target, y=y_pred_target, alpha=0.7, ax=ax)\n    ax.plot([Y_target.min(), Y_target.max()], [Y_target.min(), Y_target.max()], \n            'r--', lw=2, label='1:1 Line')\n    ax.set_title(f'Predicted vs. Actual {target_name} (Separate Model)', fontsize=14)\n    ax.set_xlabel('Actual Concentration (g/L)', fontsize=11)\n    ax.set_ylabel('Predicted Concentration (g/L)', fontsize=11)\n    ax.text(0.05, 0.95, f'R¬≤ = {r2_target:.4f}\\nRMSE = {rmse_target:.4f} g/L\\nComponents = {target_optimal_components[target_name]}', \n            transform=ax.transAxes, fontsize=10, \n            bbox=dict(facecolor='white', alpha=0.8), verticalalignment='top')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_aspect('equal', adjustable='box')\n\nplt.tight_layout()\nplt.show()\n\n\n============================================================\nTraining PLS model for glucose\n============================================================\n   1 components: RMSECV=4.3714, R¬≤CV=-0.0445\n   2 components: RMSECV=4.6011, R¬≤CV=-0.1571\n   3 components: RMSECV=3.8873, R¬≤CV=0.1740\n   4 components: RMSECV=3.6280, R¬≤CV=0.2806\n   5 components: RMSECV=3.3775, R¬≤CV=0.3765\n   6 components: RMSECV=3.3941, R¬≤CV=0.3703\n   7 components: RMSECV=3.3577, R¬≤CV=0.3838\n   8 components: RMSECV=3.3525, R¬≤CV=0.3857\n   9 components: RMSECV=3.4223, R¬≤CV=0.3598\n  10 components: RMSECV=3.4784, R¬≤CV=0.3387\n  11 components: RMSECV=3.5761, R¬≤CV=0.3010\n  12 components: RMSECV=3.6763, R¬≤CV=0.2613\n  13 components: RMSECV=3.7758, R¬≤CV=0.2207\n  14 components: RMSECV=3.8433, R¬≤CV=0.1926\n  15 components: RMSECV=3.8770, R¬≤CV=0.1784\n  16 components: RMSECV=3.9075, R¬≤CV=0.1655\n  17 components: RMSECV=3.9259, R¬≤CV=0.1576\n  18 components: RMSECV=3.9506, R¬≤CV=0.1469\n  19 components: RMSECV=3.9550, R¬≤CV=0.1450\n  20 components: RMSECV=3.9898, R¬≤CV=0.1299\n\n  Optimal components for glucose: 8\n  Final R¬≤: 0.3857, RMSE: 3.3525 g/L\n\n============================================================\nTraining PLS model for Na_acetate\n============================================================\n   1 components: RMSECV=0.5332, R¬≤CV=-0.1053\n   2 components: RMSECV=0.5376, R¬≤CV=-0.1238\n   3 components: RMSECV=0.5270, R¬≤CV=-0.0800\n   4 components: RMSECV=0.5032, R¬≤CV=0.0156\n   5 components: RMSECV=0.5282, R¬≤CV=-0.0846\n   6 components: RMSECV=0.5444, R¬≤CV=-0.1522\n   7 components: RMSECV=0.5339, R¬≤CV=-0.1083\n   8 components: RMSECV=0.5567, R¬≤CV=-0.2051\n   9 components: RMSECV=0.5732, R¬≤CV=-0.2773\n  10 components: RMSECV=0.5949, R¬≤CV=-0.3761\n  11 components: RMSECV=0.6134, R¬≤CV=-0.4629\n  12 components: RMSECV=0.6396, R¬≤CV=-0.5904\n  13 components: RMSECV=0.6623, R¬≤CV=-0.7056\n  14 components: RMSECV=0.6793, R¬≤CV=-0.7939\n  15 components: RMSECV=0.6882, R¬≤CV=-0.8415\n  16 components: RMSECV=0.6992, R¬≤CV=-0.9010\n  17 components: RMSECV=0.7049, R¬≤CV=-0.9317\n  18 components: RMSECV=0.7113, R¬≤CV=-0.9673\n  19 components: RMSECV=0.7204, R¬≤CV=-1.0177\n  20 components: RMSECV=0.7285, R¬≤CV=-1.0634\n\n  Optimal components for Na_acetate: 4\n  Final R¬≤: 0.0156, RMSE: 0.5032 g/L\n\n============================================================\nTraining PLS model for Mg_SO4\n============================================================\n   1 components: RMSECV=1.2813, R¬≤CV=0.1234\n   2 components: RMSECV=0.8643, R¬≤CV=0.6011\n   3 components: RMSECV=0.6878, R¬≤CV=0.7474\n   4 components: RMSECV=0.5916, R¬≤CV=0.8131\n   5 components: RMSECV=0.6017, R¬≤CV=0.8067\n   6 components: RMSECV=0.6490, R¬≤CV=0.7751\n   7 components: RMSECV=0.6623, R¬≤CV=0.7658\n   8 components: RMSECV=0.6818, R¬≤CV=0.7518\n   9 components: RMSECV=0.7159, R¬≤CV=0.7263\n  10 components: RMSECV=0.7463, R¬≤CV=0.7026\n  11 components: RMSECV=0.7748, R¬≤CV=0.6794\n  12 components: RMSECV=0.7860, R¬≤CV=0.6701\n  13 components: RMSECV=0.8074, R¬≤CV=0.6519\n  14 components: RMSECV=0.8163, R¬≤CV=0.6441\n  15 components: RMSECV=0.8323, R¬≤CV=0.6301\n  16 components: RMSECV=0.8413, R¬≤CV=0.6220\n  17 components: RMSECV=0.8559, R¬≤CV=0.6088\n  18 components: RMSECV=0.8697, R¬≤CV=0.5961\n  19 components: RMSECV=0.8806, R¬≤CV=0.5859\n  20 components: RMSECV=0.8913, R¬≤CV=0.5758\n\n  Optimal components for Mg_SO4: 4\n  Final R¬≤: 0.8131, RMSE: 0.5916 g/L\n\n\n\n\n\n\n\n\n\n\n================================================================================\nPerformance Comparison: Single Multi-Output Model vs. Separate Models\n================================================================================\nTarget          Single Model R¬≤    Separate Model R¬≤  R¬≤ Improvement  Single RMSE     Separate RMSE   RMSE Improvement\n--------------------------------------------------------------------------------\nglucose                    0.3915            0.3857        -0.0058         3.3367 g/L         3.3525 g/L        -0.0159 g/L\nNa_acetate                -0.1697            0.0156        +0.1853         0.5485 g/L         0.5032 g/L        +0.0453 g/L\nMg_SO4                     0.7974            0.8131        +0.0157         0.6159 g/L         0.5916 g/L        +0.0243 g/L\n================================================================================"
  },
  {
    "objectID": "posts/raman/raman.html#conclusion",
    "href": "posts/raman/raman.html#conclusion",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook demonstrates the development of Partial Least Squares (PLS) regression models for predicting concentrations of glucose, Na_acetate, and Mg_SO4 from Raman spectra. The following summarizes key findings and decisions:\n\nPreprocessing Pipeline\nSpectral Preprocessing Steps:\n\nCropping to the fingerprint region (300-1942 cm‚Åª¬π) to focus on the most informative spectral region\nCosmic ray removal using the Whitaker-Hayes algorithm to eliminate spurious high-intensity spikes\nSavitzky-Golay smoothing (window length=9, polynomial order=3) to reduce high-frequency noise\nBaseline correction using Adaptive Smoothing Parameter Least Squares (ASPLS) to remove fluorescence background\n\nNormalization:\n\nMinMax normalization was applied despite its limitations for regression tasks. This choice was necessary to handle high intensity variations (values reaching ~350 arbitrary units), as internal standard normalization was not feasible in this context due to the lack of a suitable reference peak.\n\n\n\nModeling Approach\nTwo modeling strategies were compared:\n1. Single Multi-Output PLS Model\n\nOne PLS model predicting all three targets simultaneously\nOptimal components: 12\nOverall R¬≤: 0.34, RMSE: 1.50 g/L\nIndividual performance varied significantly across targets\n\n2. Separate PLS Models Per Target\n\nIndependent PLS models for each compound with individually optimized component counts\nGlucose: 8 components, R¬≤: 0.39, RMSE: 3.35 g/L\nNa_acetate: 4 components, R¬≤: 0.02, RMSE: 0.50 g/L\n\nMg_SO4: 4 components, R¬≤: 0.81, RMSE: 0.59 g/L\n\n\n\nKey Insights\n\nSeparate models provide superior interpretability: Each target has its own loadings that clearly show which spectral regions are important for that specific compound, making the models more interpretable and chemically meaningful.\nIndependent optimization is beneficial: Different targets require different numbers of components (8 for glucose vs.¬†4 for Na_acetate and Mg_SO4), demonstrating that separate models allow for better hyperparameter tuning tailored to each compound‚Äôs spectral characteristics.\nPerformance varies significantly by compound:\n\nMg_SO4 shows excellent predictive performance (R¬≤ = 0.81), indicating strong, distinct spectral signatures that are easily captured by the model.\nGlucose shows moderate performance (R¬≤ = 0.39), suggesting some predictive capability but with room for improvement through feature engineering or alternative preprocessing.\nNa_acetate shows poor performance (R¬≤ ‚âà 0.02), indicating weak or overlapping spectral features that may require additional preprocessing, feature selection, or alternative modeling approaches.\n\nModel selection recommendation: For production use, separate PLS models per target are recommended due to their:\n\nImproved interpretability through target-specific loadings\nIndependent optimization capabilities for each compound\nAbility to focus on compound-specific spectral regions"
  },
  {
    "objectID": "posts/raman/raman.html#sec-pls-canonical",
    "href": "posts/raman/raman.html#sec-pls-canonical",
    "title": "Analyzing Raman Spectra with Machine Learning",
    "section": "PLS Canonical (PLSW2A) Algorithm",
    "text": "PLS Canonical (PLSW2A) Algorithm\n\nOverview\nPLS Canonical (also known as PLSW2A) is a method for finding low-dimensional projections of two data matrices \\(X\\) and \\(Y\\) that maximize the covariance between their projected representations. Given two centered data matrices \\(X \\in \\mathbb{R}^{n \\times p}\\) and \\(Y \\in \\mathbb{R}^{n \\times q}\\), and a desired number of latent components \\(K\\), the objective is to find projections that maximize shared covariance while allowing reconstruction of both \\(X\\) and \\(Y\\) from latent components.\nPLS Canonical focuses on symmetric covariance maximization between \\(X\\) and \\(Y\\), treating both matrices equally. This distinguishes it from PLS Regression, which focuses on predicting \\(Y\\) from \\(X\\). The key algorithmic differences are:\n\nIn PLS Regression, the \\(Y\\)-weight vector \\(c_k\\) is never normalized (no unit length constraint);\nPLS Regression regresses \\(Y_{k-1}\\) on \\(X\\)-scores \\(t_k\\) instead of \\(Y\\)-scores \\(u_k\\):\n\n\\[\nq_k = \\frac{Y_{k-1}^\\top t_k}{t_k^\\top t_k} \\quad \\text{(PLS Regression)}\n\\] versus: \\[\nq_k = \\frac{Y_{k-1}^\\top u_k}{u_k^\\top u_k} \\quad \\text{(PLS Canonical)}\n\\]\n\n\nAlgorithm\nThe algorithm initializes with \\(X_0 = X\\) and \\(Y_0 = Y\\). For each component \\(k = 1, \\dots, K\\), the following steps are performed iteratively.\nStep (a): Compute Weights via Covariance Maximization. Compute the cross-covariance matrix \\(C_k = X_{k-1}^\\top Y_{k-1}\\) and find the top left and right singular vectors: \\[\n(w_k, c_k) = \\arg\\max_{\\|w\\|=\\|c\\|=1} w^\\top C_k c\n\\]\nThese are obtained from the singular value decomposition of \\(C_k\\), where \\(w_k \\in \\mathbb{R}^p\\) is the \\(X\\)-weight vector and \\(c_k \\in \\mathbb{R}^q\\) is the \\(Y\\)-weight vector. These directions define projections of \\(X\\) and \\(Y\\) that maximize their covariance. Implementation options include full SVD of \\(X_{k-1}^\\top Y_{k-1}\\) or power iteration (NIPALS algorithm).\nStep (b): Compute Scores. Project data onto the weight vectors: \\[\nt_k = X_{k-1} w_k \\quad \\in \\mathbb{R}^n \\quad \\text{($X$-score)}\n\\] \\[\nu_k = Y_{k-1} c_k \\quad \\in \\mathbb{R}^n \\quad \\text{($Y$-score)}\n\\]\nScores are the sample coordinates in the shared latent space.\nStep (c): Compute Loadings via Regression. Regress original variables onto their respective scores: \\[\np_k = \\frac{X_{k-1}^\\top t_k}{t_k^\\top t_k} \\quad \\in \\mathbb{R}^p \\quad \\text{($X$-loading)}\n\\] \\[\nq_k = \\frac{Y_{k-1}^\\top u_k}{u_k^\\top u_k} \\quad \\in \\mathbb{R}^q \\quad \\text{($Y$-loading)}\n\\]\nLoadings indicate how each original variable contributes to the latent component and provide the best rank-1 reconstruction of the deflated data.\nStep (d): Deflation. Remove the extracted rank-1 structure: \\[\nX_k = X_{k-1} - t_k p_k^\\top\n\\] \\[\nY_k = Y_{k-1} - u_k q_k^\\top\n\\]\nDeflation ensures each subsequent component captures new, orthogonal covariance structure.\n\n\nMatrix Decompositions\nAfter extracting \\(K\\) components, we obtain low-rank approximations. For \\(X\\): \\[\nX \\approx T P^\\top\n\\]\nwhere \\(T = [t_1, \\dots, t_K] \\in \\mathbb{R}^{n \\times K}\\) is the \\(X\\)-score matrix and \\(P = [p_1, \\dots, p_K] \\in \\mathbb{R}^{p \\times K}\\) is the \\(X\\)-loading matrix. For \\(Y\\): \\[\nY \\approx U Q^\\top\n\\]\nwhere \\(U = [u_1, \\dots, u_K] \\in \\mathbb{R}^{n \\times K}\\) is the \\(Y\\)-score matrix and \\(Q = [q_1, \\dots, q_K] \\in \\mathbb{R}^{q \\times K}\\) is the \\(Y\\)-loading matrix.\n\n\nRotation Matrices\nDuring training, we compute scores iteratively on deflated data. For new data \\(X_{\\text{new}}\\), we need a single transformation that maps directly from the original space to scores without re-running the deflation algorithm. We seek a matrix \\(R_x \\in \\mathbb{R}^{p \\times K}\\) such that \\(T = X R_x\\), and then for new data: \\(T_{\\text{new}} = X_{\\text{new}} R_x\\).\nOne might expect \\(T = XW\\) where \\(W = [w_1, \\dots, w_K]\\), but this is incorrect after the first component because each \\(w_k\\) was computed on deflated data \\(X_{k-1}\\) while \\(X\\) is the original, undeflated data. Deflation breaks the direct relationship between weights and scores, so \\(XW \\neq T\\) in general.\nFrom the decomposition \\(X \\approx TP^\\top\\), multiply both sides by \\(W\\): \\(XW \\approx TP^\\top W\\). Since \\(P^\\top W \\in \\mathbb{R}^{K \\times K}\\) is invertible in PLS Canonical, we have \\(T = XW(P^\\top W)^{-1}\\). Define the \\(X\\)-rotation matrix: \\[\nR_x = W(P^\\top W)^{-1}\n\\]\nThen \\(T = XR_x\\). The dimensions are: \\[\nX \\in \\mathbb{R}^{n \\times p}, \\quad W \\in \\mathbb{R}^{p \\times K}, \\quad P \\in \\mathbb{R}^{p \\times K}, \\quad P^\\top W \\in \\mathbb{R}^{K \\times K}, \\quad R_x \\in \\mathbb{R}^{p \\times K}, \\quad T \\in \\mathbb{R}^{n \\times K}.\n\\]\nIntuitively, \\(W\\) provides ‚Äúraw‚Äù projections based on deflated data, \\((P^\\top W)^{-1}\\) corrects for interference between components, and \\(R_x\\) gives the equivalent direct projection from original space.\nBy identical reasoning, from \\(Y \\approx UQ^\\top\\): \\(U = YC(Q^\\top C)^{-1}\\). Define the \\(Y\\)-rotation matrix: \\[\nR_y = C(Q^\\top C)^{-1}\n\\]\nwhere \\(C = [c_1, \\dots, c_K] \\in \\mathbb{R}^{q \\times K}\\). Then \\(U = YR_y\\). In scikit-learn, x_rotations_ stores \\(R_x\\) and y_rotations_ stores \\(R_y\\). Geometrically, rotation matrices re-embed the sequential deflation process into a single linear transformation in the original space.\n\n\nPrediction: Mapping \\(X\\) to \\(Y\\)\nThe objective is to find a coefficient matrix \\(B \\in \\mathbb{R}^{p \\times q}\\) such that \\(\\hat{Y} = XB\\). The prediction follows a three-stage pipeline through the latent space.\n\nStage 1: \\(X\\) to \\(X\\)-scores uses \\(T = XR_x\\).\nStage 2: \\(X\\)-scores to \\(Y\\)-scores (latent regression) models the relationship between latent scores as \\(U \\approx TG\\), where \\(G \\in \\mathbb{R}^{K \\times K}\\) is obtained via least squares: \\[\nG = (T^\\top T)^{-1}T^\\top U\n\\]\nStage 3: \\(Y\\)-scores to \\(Y\\) (reconstruction) uses the decomposition \\(Y \\approx UQ^\\top\\) to get \\(\\hat{Y} = UQ^\\top\\). Combining all stages: \\[\n\\hat{Y} = XR_x \\cdot G \\cdot Q^\\top\n\\] Therefore, the coefficient matrix is: \\[\nB = R_x G Q^\\top\n\\] In scikit-learn, this is stored in the coef_ attribute. The pipeline summary is: \\[\nX \\xrightarrow{R_x} T \\xrightarrow{G} U \\xrightarrow{Q^\\top} \\hat{Y}\n\\]\n\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\nConcept\nSymbol\nDimension\nMeaning\n\n\n\n\nWeights\n\\(W, C\\)\n\\(p \\times K\\), \\(q \\times K\\)\nDirections of maximal shared covariance\n\n\nScores\n\\(T, U\\)\n\\(n \\times K\\)\nLatent representations of samples\n\n\nLoadings\n\\(P, Q\\)\n\\(p \\times K\\), \\(q \\times K\\)\nReconstruction directions\n\n\nRotations\n\\(R_x, R_y\\)\n\\(p \\times K\\), \\(q \\times K\\)\nDirect projection from original to latent space\n\n\nCoefficients\n\\(B\\)\n\\(p \\times q\\)\nPrediction map \\(X \\to Y\\) via latent space\n\n\n\nThe core equations are:\nScores from original data: \\[\nT = XR_x, \\quad U = YR_y\n\\] Decompositions: \\[\nX \\approx TP^\\top, \\quad Y \\approx UQ^\\top\n\\] Prediction: \\[\n\\hat{Y} = XB \\quad \\text{where} \\quad B = R_xGQ^\\top\n\\]"
  },
  {
    "objectID": "posts/mhc/mhc.html",
    "href": "posts/mhc/mhc.html",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "",
    "text": "This blog post explores the application of machine learning techniques to predict the binding affinity between peptides and Major Histocompatibility Complex (MHC) molecules. In Section¬†1, we introduce the biological aspects of MHC. In Section¬†2, we then present the deep learning approach and the results of MHC antigen presentation."
  },
  {
    "objectID": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "href": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "MHC Class I and Class II",
    "text": "MHC Class I and Class II\nMHC molecules are divided into two main classes: MHC class I and MHC class II.\nMHC Class I\nMHC class I molecules are present on most cells (except red blood cells). They present antigens from inside the cell. When a cell is infected or becomes cancerous, proteins within the cell are broken down into smaller fragments called epitopes. These epitopes are loaded onto MHC class I molecules and displayed on the cell surface. Killer T cells (cytotoxic T lymphocytes or CTLs) recognize these epitopes and can destroy infected or cancerous cells.\n\n\n\n\n\n\nFigure¬†1: MHC class I pathway: Proteins in the cytosol are degraded by the proteasome, liberating peptides internalized by TAP channel in the endoplasmic reticulum, there associating with MHC-I molecules freshly synthesized. MHC-I/peptide complexes enter Golgi apparatus, are glycosylated, enter secretory vesicles, fuse with the cell membrane, and externalize on the cell membrane interacting with T lymphocytes. This figure shows the detailed steps of how MHC Class I molecules get loaded. Source\n\n\n\nIn humans, the main MHC class I molecules are HLA-A, HLA-B, and HLA-C (HLA stands for Human Leukocyte Antigen).\nMHC Class II\nMHC class II molecules are primarily found on specialized immune cells called antigen-presenting cells (APCs) like macrophages, dendritic cells, and B cells. They present antigens from outside the cell. APCs engulf foreign invaders and break them down into epitopes in a process called phagocytosis. These epitopes are loaded onto MHC class II molecules and displayed on the cell surface. Helper T cells recognize these epitopes and activate other immune cells to fight the infection. These Helper T cells have receptors that specifically bind to MHC Class II molecules. If a helper T cell recognizes a foreign epitope presented by MHC class II, it becomes activated and starts to coordinate the immune response. It releases chemical signals (cytokines) that help other immune cells, like B cells and killer T cells, to fight off the infection.\n\n\n\n\n\n\nFigure¬†2: This diagram shows how MHC class I molecules present antigens from inside the cell to cytotoxic T cells (CD8+), leading to the destruction of infected cells. Notice how the antigen is processed inside the cell and then presented on the cell surface. On the contrary, MHC class II molecules present antigens from outside the cell to helper T cells (CD4+), which then activate other immune cells. Notice how this differs from MHC class I, which presents antigens from inside the cell.(Antunes et al. 2018)\n\n\n\n\n\n\n\n\n\nFigure¬†3: Molecular structures of class I and class II MHCs. Molecular representation of a class I MHC (A, C) and a class II MHC (B, D). The upper panel shows a top view, while the bottom panel shows a cross section side-view of the binding clefts. Note that the binding cleft of a class I receptor is deeper, with ‚Äúclosed‚Äù extremities, while the class II cleft is shallower, with open extremities. The pockets involved in binding primary ‚Äúanchor‚Äù residues are indicated. Together, structural differences in the shape of the cleft and the location of binding pockets have an impact on the overall conformation of bound ligands (e.g., peptides tend to adopt bulged conformations when bound to class I, and more linear conformations when bound to class II). (Antunes et al. 2018)\n\n\n\nIn humans, the main MHC class II molecules are HLA-DP, HLA-DQ, and HLA-DR.\nHuman Leukocyte Antigens (HLA)\nIn humans, MHC molecules are called Human Leukocyte Antigens (HLAs). The genes for HLA proteins are located within the MHC region on chromosome 6.\nHLAs are important for:\n\nOrgan transplantation: HLA matching is crucial to prevent organ rejection.\nAutoimmune diseases: Certain HLA types are associated with increased risk of autoimmune diseases.\nDrug responses: HLA variations can influence how individuals respond to certain medications.\nEvolutionary advantage: HLA diversity is important for population survival against various pathogens.\n\n\n\n\n\n\n\nFigure¬†4: Codominant expression of HLA genes. Each person inherits HLA genes from both parents, resulting in the expression of multiple HLA types. This increases the diversity of antigens that can be presented. Source\n\n\n\nMHC Diversity\nThe MHC is highly diverse, with many different versions (alleles) of each MHC gene. This diversity is essential because different MHC molecules bind to different peptides. A diverse population is more likely to have individuals who can present antigens from new pathogens, ensuring better survival chances for the species."
  },
  {
    "objectID": "posts/mhc/mhc.html#datasets",
    "href": "posts/mhc/mhc.html#datasets",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Datasets",
    "text": "Datasets\nThe IPD-IMGT/HLA Database is a specialized resource focusing on the sequences of the human major histocompatibility complex (MHC) or human leukocyte antigen (HLA) system. It provides comprehensive information about HLA alleles, including their sequences, nomenclature, and associated metadata. This database is crucial for researchers in immunology, transplantation, and vaccine development, as accurate HLA typing is essential for understanding immune responses and predicting transplant compatibility. The IEDB (Immune Epitope Database) is a widely used resource for curated experimental data on immune epitopes. It catalogs epitopes recognized by T cells and B cells in various diseases and conditions. The IEDB facilitates research in epitope discovery, vaccine design, and understanding immune recognition, offering tools and data for analyzing and predicting immune responses."
  },
  {
    "objectID": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "href": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Relevance to Machine Learning",
    "text": "Relevance to Machine Learning\nThe interaction between an MHC molecule and a peptide is highly dependent on the amino acid sequence of the peptide and the specific type of MHC molecule. This sequence-structure-function relationship makes MHC-peptide binding prediction a suitable problem for machine learning. Experimental data on MHC-peptide binding affinities, though sometimes sparse, is available for training and evaluating predictive models. The high polymorphism of MHC genes, with numerous variants (alleles) existing in the population, adds complexity and motivates the development of specific prediction models.\nKey MHC Concepts:\n\nMHC Class I and Class II: The two main classes of MHC molecules.\nEpitope: The specific part of the peptide recognized by the T cell receptor.\nPolymorphism: The existence of multiple versions (alleles) of MHC genes within a population. Different MHC alleles bind to different sets of peptides.\nBinding Affinity: The strength of the interaction between an MHC molecule and a peptide. Often measured experimentally, it serves as the target variable for many machine learning models.\n\nMachine Learning Approaches\nMHC-peptide binding prediction aims to develop models that accurately predict the binding affinity between a given peptide sequence and a specific MHC allele. This can be framed as a regression or classification task.\n\nInput: Peptide sequence, MHC allele (represented as a sequence or encoding).\nOutput: Binding affinity (e.g., IC50 value, Kd value) or a binary label (binder/non-binder).\n\nFeature engineering and model selection are crucial for building effective predictors. Common approaches include:\n\nSequence-based Features: Amino acid composition, n-grams, physicochemical properties.\nStructure-based Features: (If available) Information about the 3D structure of the MHC-peptide complex.\nMHC Allele Encoding: Techniques such as one-hot encoding, amino acid embeddings, or other methods to represent the MHC allele sequence.\nMachine Learning Algorithms: Linear regression, Support Vector Machines (SVMs), Random Forests, Neural Networks (including Convolutional Neural Networks and Transformers)."
  },
  {
    "objectID": "posts/mhc/mhc.html#project-overview",
    "href": "posts/mhc/mhc.html#project-overview",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Project Overview",
    "text": "Project Overview\n\n\n\n\n\n\nNote\n\n\n\nThe code is available at https://github.com/nbrosse/mhcpred.\n\n\nThe goal of this project is to build a machine learning classifier that predicts whether a given peptide will be presented by a specific MHC class I protein, identified by its allele name. The data used for this project is derived from the training and evaluation data of NetMHCPan4.1 (Reynisson et al. 2020), a well-established framework for MHC binding prediction. The data is split into training and testing sets, with the training data further divided into five folds for cross-validation.\nThe dataset contains a binary target variable, ‚Äúhit‚Äù (1 if the peptide is presented by the MHC, 0 otherwise), and two features:\n\n‚Äúpeptide‚Äù: The amino acid sequence of the peptide. These short chains of amino acids are potential antigens that could be presented to the immune system.\n‚Äúallele‚Äù: The name of the MHC class I allele. MHC molecules are highly polymorphic, meaning there are many different versions (alleles) within the human population. Each allele has a slightly different binding groove, affecting which peptides it can bind and present. You can find details on the naming convention here (nomenclature).\n\n\n\n\n\n\n\nNote\n\n\n\nPredicting MHC antigen presentation is a complex field. This project provides a simplified introduction to the problem. For a more in-depth understanding, we recommend exploring NetMHCPan (Reynisson et al. 2020) and MHCflurry (O‚ÄôDonnell, Rubinsteyn, and Laserson 2020) and the references cited within those publications. Note that the specific data used in this project is derived from NetMHCPan4.1 but must remain private.\n\n\nWe begin with Exploratory Data Analysis (EDA) to understand the characteristics of our data.\n\nEDA\n\n# Import data loading functions\nfrom mhcpred.data import get_train_data, get_test_data\n\n# Load training and test data\ndf_train = get_train_data()\ndf_test = get_test_data()\n\n\n# View first few rows of training data\ndf_train.head()\n\n\n\n\n\n\n\n\npeptide\nallele\nhit\nfold\n\n\n\n\n0\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n0\n\n\n1\nKESKINQVF\nHLA-B*44:02\nTrue\n0\n\n\n2\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n0\n\n\n3\nRTIADSLINSF\nHLA-B*57:03\nTrue\n0\n\n\n4\nEEKTIIKKL\nHLA-B*44:03\nTrue\n0\n\n\n\n\n\n\n\n\n# Get allele counts in training data\ndf_train[[\"allele\"]].value_counts()\n\nallele     \nHLA-A*02:01    265252\nHLA-B*07:02    201038\nHLA-B*57:01    184773\nHLA-A*29:02    181136\nHLA-B*40:02    145817\n                ...  \nHLA-A*69:01        12\nHLA-A*02:06         6\nHLA-A*26:02         6\nHLA-A*26:03         6\nHLA-A*25:01         6\nName: count, Length: 130, dtype: int64\n\n\n\ndf_test[\"allele\"].value_counts()\n\nallele\nHLA-A*02:02    77053\nHLA-A*02:06    54510\nHLA-A*02:11    48445\nHLA-B*53:01    46991\nHLA-B*15:17    45917\nHLA-A*02:05    45136\nHLA-B*15:03    44968\nHLA-A*33:01    43333\nHLA-A*66:01    41538\nHLA-C*12:03    36448\nHLA-C*03:03    35568\nHLA-A*11:01    33424\nHLA-A*30:02    33180\nHLA-C*08:02    32416\nHLA-A*23:01    30467\nHLA-A*32:01    28036\nHLA-B*40:02    23768\nHLA-B*14:02    21601\nHLA-B*37:01    20048\nHLA-B*40:01    18908\nHLA-B*45:01    18750\nHLA-B*18:01    18284\nHLA-B*58:01    17946\nHLA-B*15:02    16702\nHLA-B*15:01    16624\nHLA-A*30:01    15837\nHLA-C*07:02    15293\nHLA-B*46:01    14015\nHLA-B*38:01     9509\nHLA-B*35:03     8275\nHLA-A*26:01     7730\nHLA-C*05:01     7033\nHLA-A*25:01     6906\nHLA-A*68:01     5648\nHLA-B*08:01     3365\nHLA-B*07:02     2469\nName: count, dtype: int64\n\n\n\n# Get positive samples per allele in training\ndf_train.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*01:01     7156\nHLA-A*01:03        7\nHLA-A*02:01    13025\nHLA-A*02:03     1873\nHLA-A*02:04     3155\n               ...  \nHLA-C*12:04        3\nHLA-C*14:02     2441\nHLA-C*15:02     1873\nHLA-C*16:01     2970\nHLA-C*17:01      602\nName: hit, Length: 130, dtype: int64\n\n\n\ndf_train.hit.sum()\n\n197547\n\n\n\nlen(df_train)\n\n3679405\n\n\n\n# ~5.37% positive rate\ndf_train.hit.sum() / len(df_train)\n\n0.05368993084479692\n\n\n\ndf_test.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*02:02    3063\nHLA-A*02:05    2016\nHLA-A*02:06    1975\nHLA-A*02:11    2035\nHLA-A*11:01    2309\nHLA-A*23:01    1697\nHLA-A*25:01     396\nHLA-A*26:01     555\nHLA-A*30:01     892\nHLA-A*30:02    2415\nHLA-A*32:01    1436\nHLA-A*33:01    2138\nHLA-A*66:01    1988\nHLA-A*68:01     433\nHLA-B*07:02     159\nHLA-B*08:01     180\nHLA-B*14:02    1056\nHLA-B*15:01     769\nHLA-B*15:02     637\nHLA-B*15:03    1953\nHLA-B*15:17    1712\nHLA-B*18:01     784\nHLA-B*35:03     330\nHLA-B*37:01    1253\nHLA-B*38:01     619\nHLA-B*40:01    1268\nHLA-B*40:02    1333\nHLA-B*45:01     760\nHLA-B*46:01     575\nHLA-B*53:01    2016\nHLA-B*58:01     866\nHLA-C*03:03    2003\nHLA-C*05:01     383\nHLA-C*07:02     593\nHLA-C*08:02    1546\nHLA-C*12:03    1273\nName: hit, dtype: int64\n\n\n\ndf_test.hit.sum() / len(df_test)\n\n0.04800130213150049\n\n\n\n# Find alleles only in test set\nset(df_test.allele.unique()) - set(df_train.allele.unique())\n\n{'HLA-A*02:02', 'HLA-A*02:11', 'HLA-A*33:01', 'HLA-B*53:01'}\n\n\n\nDataset Class Imbalance\n\nTraining Set:\n\nTotal samples: 3,679,405\nPositive rate: 5.37%\n\nTest Set:\n\nTotal samples: 453,934\nPositive rate: 4.8%\n\n\nAllele Distribution\n\nMost frequent: HLA-A*02:01 (265,252 samples)\nLeast frequent: Multiple alleles with only 6 samples\nDistribution: Highly imbalanced across alleles\n\nTest-Only Alleles\n\nHLA-A*02:02\nHLA-A*02:11\nHLA-A*33:01\nHLA-B*53:01\n\n\nSource: EDA\n\nUsing MHCflurry Pretrained Models for Prediction\nWe leverage the mhcflurry package to build our classifier. MHCflurry is a tool specifically designed for MHC binding affinity prediction. See also the associated paper (O‚ÄôDonnell, Rubinsteyn, and Laserson 2020). MHCflurry is a software package focused on predicting how strongly peptides bind to MHC class I molecules. It‚Äôs based on machine learning models trained on a large dataset of experimentally measured peptide-MHC binding affinities. The current version uses neural networks trained with a mix of binding affinity and mass spectrometry data (ligand presentation).\nWe use the Binding Affinity pretrained model from mhcflurry to predict the binding affinity of peptides to MHC class I molecules using Class1AffinityPredictor. The following code assumes you have installed mhcflurry and downloaded the required pretrained models.\nmhcflurry-downloads fetch models_class1_presentation\npython scripts/mhcflurry_benchmark.py\ndef predict_with_mhcflurry() -&gt; pd.DataFrame:\n    predictor = Class1AffinityPredictor.load()\n    df_test = get_test_data()\n    mhcflurry_predictions = predictor.predict_to_dataframe(\n        peptides=df_test.peptide.values,\n        alleles=df_test.allele.values,\n        allele=None,\n    )\n    df = pd.merge(df_test, mhcflurry_predictions, on=[\"allele\", \"peptide\"], how=\"left\")\n    df.to_csv(str(output_path / \"mhcflurry_predictions.csv\"), index=False)\n    return df\nThe output is of the form:\n\n\n\nTable¬†1: MHCflurry pretrained model predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npeptide\nhit\nallele\nprediction\nprediction_low\nprediction_high\nprediction_percentile\n\n\n\n\nAAPATRAAL\nTrue\nHLA-B*35:03\n94.297\n59.902\n144.624\n0.205\n\n\nAAPSAAREL\nTrue\nHLA-B*35:03\n116.19\n79.847\n169.241\n0.262\n\n\nAEISQIHQSVTD\nTrue\nHLA-B*35:03\n26103.26\n22695.389\n28415\n15.739\n\n\nALEEQLQQIRAE\nTrue\nHLA-B*35:03\n24797.131\n19988.967\n28062.65\n13.571\n\n\nAQDPLLLQM\nTrue\nHLA-B*35:03\n2164.336\n745.888\n5390.727\n1.413\n\n\nASAPPGPPA\nTrue\nHLA-B*35:03\n1398.729\n387.675\n3293.692\n1.157\n\n\nDAHKGVAL\nTrue\nHLA-B*35:03\n84.315\n54.736\n133.899\n0.175\n\n\nDNPIQTVSL\nTrue\nHLA-B*35:03\n1386.767\n565.122\n3667.21\n1.151\n\n\nDPEAFLVQI\nTrue\nHLA-B*35:03\n245.485\n133.986\n394.752\n0.484\n\n\n\n\n\n\nThe first 3 columns come from the test dataset.\n\npeptide: The amino acid sequence of the peptide being evaluated.\nhit: The ground truth, a boolean value indicating whether the peptide is known to be presented by the given MHC allele (True) or not (False).\nallele: The name of the MHC class I allele being considered.\n\nThe following columns are added by the binding affinity predictions:\n\nprediction: The raw prediction score from the MHCflurry model. Higher values generally indicate a stronger predicted binding affinity. These values are not directly interpretable in isolation.\nprediction_low/prediction_high: These represent the lower and upper bounds of a 95% confidence interval around the prediction value. They provide an estimate of the uncertainty associated with the prediction.\nprediction_percentile: This is the most useful column for interpreting the results. It represents the percentile rank of the prediction score compared to a background distribution of scores for random peptides. A lower percentile indicates a stronger predicted binding affinity. For example, a percentile of 1.0 means that the predicted score is in the top 1% of all possible scores.\n\nThe uncertainty estimation comes from the ensemble of neural networks used for the prediction. A percentile threshold (e.g., 2%) is commonly used to determine whether a peptide is likely to bind (lower is better).\n\n\nPrediction: Fitting a Class1BinaryNeuralNetwork\nWe now fit a Class1BinaryNeuralNetwork on the training dataset. The code is available at https://github.com/nbrosse/mhcpred.\nHere‚Äôs a glimpse of the training data structure:\n\n\n\nTable¬†2: Training data.\n\n\n\n\n\npeptide\nallele\nhit\n\n\n\n\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n\n\nKESKINQVF\nHLA-B*44:02\nTrue\n\n\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n\n\nRTIADSLINSF\nHLA-B*57:03\nTrue\n\n\n\n\n\n\nChallenges arise in encoding the peptide and allele sequences for use in a neural network. Peptides have variable lengths, and alleles are represented by their names.\nThe MHCFlurry package provides a mapping between alleles and their corresponding MHC molecule sequences within the allele_sequences.csv file. This mapping is crucial for encoding the alleles.\n\n\n\nTable¬†3: Allele sequences.\n\n\n\n\n\nAllele\nSequence\n\n\n\n\nHLA-A*01:01\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:02\nYSAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:03\nYFAMYQENMAHTDANTLYGIMYDRDYTWVARVYRGYA\n\n\nHLA-A*01:04\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYX\n\n\nHLA-A*01:06\nYFAMYQENMAHTDANTLYGIIYDRDYTWVALAYRGYA\n\n\n\n\n\n\nFirst, we import necessary libraries. We also import components from our own mhcpred library, which contains the neural network architecture and data loading functions.\nimport pickle\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport numpy as np\nimport pandas as pd\nfrom mhcflurry.allele_encoding import AlleleEncoding\nfrom mhcflurry.encodable_sequences import EncodableSequences\nfrom sklearn.model_selection import train_test_split\n\nfrom mhcpred.class1_binary_nn import Class1BinaryNeuralNetwork\nfrom mhcpred.config import settings\nfrom mhcpred.data import get_test_data, get_train_data\nfrom mhcpred.hyperparameters import base_hyperparameters\nWe load the allele sequences, training data, and test data using helper functions. The allele sequences are crucial for encoding the MHC alleles.\nallele_sequences = pd.read_csv(\n    str(data_path / \"allele_sequences.csv\"), index_col=0\n).iloc[:, 0]\n\ndf_total_train = get_train_data()\ndf_test = get_test_data()\nWe determine the alleles present in our data and filter the loaded allele sequences to only include those we‚Äôll be using.\nalleles_in_use = set(df_total_train.allele).union(set(df_test.allele))\nallele_sequences_in_use = allele_sequences[allele_sequences.index.isin(alleles_in_use)]\nThe AlleleEncoding class is designed to cache encodings for a sequence of alleles. It maps allele names to integer indices and sequences, allowing consistent use of these mappings, especially as inputs to neural networks. The EncodableSequences class is used to encode variable-length peptides into fixed-size numerical matrices. It caches various encodings of a list of sequences and provides methods to encode these sequences into fixed-length categorical or vector representations.\nWe also split the training data into training and validation sets using train_test_split from sklearn. Stratified splitting ensures the class balance is maintained across the training and validation sets. The validation data is also preprocessed by encoding the peptides and alleles.\nallele_encoding = AlleleEncoding(\n    alleles=allele_sequences_in_use.index.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\ndf_train, df_val = train_test_split(\n    df_total_train, test_size=0.1, shuffle=True, stratify=df_total_train.hit.values\n)\n\nval_peptides = EncodableSequences(df_val.peptide.values)\nval_alleles = AlleleEncoding(\n    alleles=df_val.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\nAlleleEncoding provides a robust and efficient way to manage and encode allele sequences. It handles the complexities of mapping allele names to indices, storing and padding sequences, and providing different encoding options. The AlleleEncoding class manages allele sequences efficiently:\n\nAllele Universe vs.¬†Used Alleles: The class distinguishes between two sets of alleles:\n\n\nAllele Universe: The complete set of alleles the system knows about. This is defined by the allele_to_sequence dictionary, mapping allele names to their amino acid sequences.\nUsed Alleles: The specific set of alleles used in a particular analysis or task. This is provided as a list when creating an AlleleEncoding instance.\n\n\nallele_to_index Mapping: A dictionary (self.allele_to_index) is created to map each allele in the universe to a unique integer index. This includes a special index for None values, often used as padding. This mapping ensures consistency: the same allele always gets the same index.\nSequence Storage (self.sequences): The amino acid sequences for all alleles in the universe are stored in a Pandas Series (self.sequences). Critically, these sequences are padded to the same length using ‚ÄúX‚Äù characters. This padding is essential for creating fixed-length numerical representations, which many machine learning models require.\nBorrowing (borrow_from): The borrow_from parameter allows you to create a new AlleleEncoding instance that inherits the allele universe and mappings from an existing instance. This is a powerful way to ensure consistency across different parts of your code. You don‚Äôt have to redefine the allele_to_sequence mapping every time.\nEncoding: The class provides methods to encode the allele sequences into numerical matrices, suitable for machine learning.\n\n\nallele_representations(encoding_name): Encodes the entire allele universe. This is useful for pre-calculating encodings for all known alleles.\nfixed_length_vector_encoded_sequences(encoding_name): Encodes the used alleles (the subset provided when the object was initialized). This uses the pre-calculated encodings from allele_representations and selects only the encodings for the alleles in self.alleles, in the correct order. This gives you a matrix where each row represents an allele sequence.\n\n\nEncoding Methods (encoding_name): The type of encoding can be specified using the encoding_name parameter. Common options include ‚ÄúBLOSUM62‚Äù (a substitution matrix) and ‚Äúone-hot‚Äù encoding.\n\nBLOSUM62 (Blocks Substitution Matrix) is a widely used substitution matrix in bioinformatics. It represents the likelihood of one amino acid being substituted for another during evolution. The matrix assigns a score to each pair of amino acids, reflecting their similarity. Higher scores indicate a higher probability of substitution (or that the two amino acids are more similar). Negative scores indicate substitutions that are less likely or even unfavorable.\nThe AlleleEncoding class uses BLOSUM62 to convert amino acid sequences into numerical representations. Each amino acid in the sequence is replaced by a vector of 21 numbers (20 amino acids + the ‚ÄúX‚Äù character). Each of these 21 numbers is the BLOSUM62 score between the amino acid in the sequence and the amino acid represented by the position in the 21-element vector.\n\nAmino Acid Indexing: First, each amino acid is converted to an index. There‚Äôs a mapping from amino acid letter to index.\nBLOSUM62 Lookup: For each amino acid in the sequence, the code looks up its corresponding row in the BLOSUM62 matrix. This row represents the similarity scores between that amino acid and all other amino acids (and ‚ÄòX‚Äô).\nVector Representation: The row from the BLOSUM62 matrix becomes the vector representation of that amino acid. So, ‚ÄúM‚Äù would be represented by a vector of 21 numbers (the scores of M with every other amino acid and X), and ‚ÄúA‚Äù would also be represented by its own 21-number vector.\nSequence Encoding: The encoded sequence becomes a matrix. If the original sequence was of length n, the encoded sequence is now an n x 21 matrix.\n\nThe train_data_iterator function is a generator that yields batches of training data. This function also handles filtering of alleles that might be present in the training data but not in the allele_sequences data to handle potential data inconsistencies.\ndef train_data_iterator(\n    df_train: pd.DataFrame,\n    train_allele_encoding: AlleleEncoding,\n    batch_size: int = 1024,\n) -&gt; Iterator[tuple[AlleleEncoding, EncodableSequences, np.ndarray]]:\n    \"\"\"\n    This function creates a data generator for training the neural network.\n    It iterates over the training data in batches and yields tuples of \n    (allele_encoding, peptide_sequences, labels).  It also handles filtering\n    of alleles not found in the initial allele encoding.\n    \"\"\"\n    # Get unique alleles in the training set.\n    alleles = df_train.allele.unique()\n    # Filter alleles to keep only those for which sequences are available.\n    usable_alleles = [\n        c for c in alleles if c in train_allele_encoding.allele_to_sequence\n    ]\n    print(\"Using %d / %d alleles\" % (len(usable_alleles), len(alleles)))\n    print(\n        \"Skipped alleles: \",\n        [c for c in alleles if c not in train_allele_encoding.allele_to_sequence],\n    )\n    df_train = df_train.query(\"allele in @usable_alleles\")\n\n    # Calculate the number of batches.\n    n_splits = np.ceil(len(df_train) / batch_size)\n\n    # Infinite loop to allow for multiple epochs.\n    while True:\n        # Split the training data into batches.\n        epoch_dfs = np.array_split(df_train.copy(), n_splits)\n        for k, df in enumerate(epoch_dfs):\n            if len(df) == 0:\n                continue\n            # Encode peptides and alleles for the current batch.\n            encodable_peptides = EncodableSequences(df.peptide.values)\n            allele_encoding = AlleleEncoding(\n                alleles=df.allele.values,\n                borrow_from=train_allele_encoding,  # Reuse encoding from main allele_encoding\n            )\n            # Yield the encoded data and labels (hit column).\n            yield (allele_encoding, encodable_peptides, df.hit.values)\nThe neural network model is initialized using the Class1BinaryNeuralNetwork class. Base hyperparameters are used for initialization. The model is then trained using the fit_generator method. This method takes the training data generator, validation data, and other parameters like the number of epochs and steps per epoch. The steps_per_epoch is calculated based on the training data size and batch size.\nmodel = Class1BinaryNeuralNetwork(**base_hyperparameters)\nsteps_per_epoch = np.ceil(len(df_train) / batch_size)\nbatch_size = 1024  # Define batch_size here\n\ntrain_generator = train_data_iterator(df_train, allele_encoding, batch_size) #create the generator\n\nmodel.fit_generator(\n    generator=train_generator,\n    validation_peptide_encoding=val_peptides,\n    validation_affinities=df_val.hit.values,\n    validation_allele_encoding=val_alleles,\n    validation_inequalities=None,\n    validation_output_indices=None,\n    steps_per_epoch=steps_per_epoch,\n    epochs=2,\n)\nThe Class1BinaryNeuralNetwork neural network takes two inputs:\n\nAllele: A single input representing the MHC allele.\nPeptide: A sequence of 45 amino acids represented as a 21-dimensional vector for each amino acid (likely using BLOSUM62 encoding or a similar technique).\n\nThe network then processes these inputs through several layers:\n\nEmbedding Layer: The allele input is passed through an embedding layer. This layer learns a 777-dimensional vector representation for each allele, capturing its key characteristics relevant to peptide binding.\nFlatten Layers: These layers reshape the input data. The peptide input, which is initially a 45x21 matrix, is flattened into a 945-element vector. Similarly, the 1x777 allele embedding is flattened into a 777-element vector. This prepares the data for the subsequent dense layers.\nConcatenate Layer: The flattened representations of the peptide and allele are combined into a single 1722-element vector. This crucial step merges the information from both inputs, allowing the network to learn the combined effect of allele and peptide on binding affinity.\nDense Layers: These are fully connected layers. The first dense layer transforms the 1722-element vector into a 1024-element vector, and the second further reduces it to 512 elements. These layers learn complex non-linear relationships between the combined allele and peptide representation, extracting features crucial for predicting binding affinity.\nDropout Layers: Dropout is a regularization technique. During training, these layers randomly ‚Äúdrop out‚Äù (ignore) a fraction of neurons. This prevents the network from overfitting to the training data and improves its ability to generalize to unseen data.\nOutput Layer: The final dense layer has a single output neuron. This neuron outputs a single value, representing the predicted binding affinity between the given peptide and MHC allele. Since we‚Äôre predicting a binary ‚Äúhit‚Äù variable, a sigmoid activation function is used in this layer to output a probability between 0 and 1.\n\nThis architecture is designed to effectively learn the complex patterns governing MHC-peptide binding.\nFinally, the trained model is used to make predictions on the test data. The test data is preprocessed in the same way as the training data, and the predict method of the model is used to generate predictions. These predictions are then added to the test dataframe and saved to a CSV file.\ntest_peptides = df_test.peptide.values\ntest_allele_encoding = AlleleEncoding(\n    alleles=df_test.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\npredictions = model.predict(\n    peptides=test_peptides,\n    allele_encoding=test_allele_encoding,\n)\n\ndf_test[\"predictions\"] = predictions\ndf_test.to_csv(str(output_path / \"mhcpred_predictions.csv\"), index=False)\nWe evaluate the predictions of the two methods (mhcflurry and mhcpred) using standard binary classification metrics.\n\n\nMetrics\nThis notebook contains training metrics history and classification metrics computed on the predictions by - mhcflurry (benchmark) - mhcpred\n\nfrom pathlib import Path\nimport pickle\n\nfrom mhcpred.config import settings\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nmodels_path = Path(settings.models_path)\noutput_path = Path(settings.output_path)\n\nInformation on the training history\nI prefer to use tensorboard, but it is not implemented in the mhcflurry package. The information is quite scarce, but when you execute the code, you have the loss for each step and not only for the whole epoch. Of course, it is a very basic version of logging and should be improved.\n\nwith open(str(models_path / \"model.pickle\"), \"rb\") as f:\n    model = pickle.load(f)\n\n\nmodel.fit_info\n\n[{'learning_rate': 0.0010000000474974513,\n  'loss': [0.09700655937194824, 0.06465369462966919],\n  'val_loss': [0.06880103051662445, 0.05075661838054657],\n  'time': 524.7155420780182,\n  'num_points': 6628048}]\n\n\nBinary classification metrics\nWe compute the usual binary classification metrics on the unbalanced test dataset: accuracy, balanced accuracy, confusion matrix and classification report by scikit-learn.\nWe report the unbalanced accuracy because the dataset is very unbalanced so the accuracy only is not a good measure of accuracy (the model can predict always False and it works quite well).\nmhcflurry metrics\n\nmhcflurry_rank_percentile_threshold = 2  # rank threshold for positive hits\n# It comes from the mhcflurry article.\n\n\ndf = pd.read_csv(str(output_path / \"mhcflurry_predictions.csv\"))\ny_pred = df.prediction_percentile.values &lt;= mhcflurry_rank_percentile_threshold\ny_true = df.hit.values\nacc = accuracy_score(y_true=y_true, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=y_true, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.99    900996\n        True       0.67      0.86      0.76     45423\n\n    accuracy                           0.97    946419\n   macro avg       0.83      0.92      0.87    946419\nweighted avg       0.98      0.97      0.97    946419\n\n\n\n\nacc, balanced_acc\n\n(0.9731936911663861, 0.9217833819652606)\n\n\nThe metrics are quite good. We note that we do not have a good precision on the True class (0.67), the model has a tendency to predict True too often, so we have too many False Positives. We see it on the confusion matrix, 19234 False Positives.\nmhcpred metrics\n\nmhcpred_proba_threshold = 0.5  # by default, but we try to tune it later\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9731657332258088, 0.7775326900487307)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.98      0.99      0.99    900725\n        True       0.82      0.56      0.67     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.90      0.78      0.83    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nmhcpred has worse performances compared to mhcflurry, see the balanced accuracy. On the True class, in that case, the recall is not good (0.56), the model has a tendency to predict False too often, on the confusion matrix we have 20000 True Negatives. It indicates that if we lower the threshold, we may improve the model.\nThreshold tuning\nWe plot the precision recall curve to try to identify a better threshold.\n\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n\nprecision, recall, thresholds = precision_recall_curve(y_true=y_true, probas_pred=df.predictions.values)\ndisp = PrecisionRecallDisplay(precision=precision, recall=recall)\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprecision_recall_thresholds = pd.DataFrame({\n    \"precision\": precision[:-1],\n    \"recall\": recall[:-1],\n    \"thresholds\": thresholds,\n})\n\n\nprecision_recall_thresholds\n\n\n\n\n\n\n\n\nprecision\nrecall\nthresholds\n\n\n\n\n0\n0.048001\n1.000000\n0.000114\n\n\n1\n0.048001\n1.000000\n0.000116\n\n\n2\n0.048001\n1.000000\n0.000117\n\n\n3\n0.048001\n1.000000\n0.000125\n\n\n4\n0.048002\n1.000000\n0.000125\n\n\n...\n...\n...\n...\n\n\n889313\n1.000000\n0.000110\n0.992152\n\n\n889314\n1.000000\n0.000088\n0.992280\n\n\n889315\n1.000000\n0.000066\n0.992347\n\n\n889316\n1.000000\n0.000044\n0.992431\n\n\n889317\n1.000000\n0.000022\n0.992971\n\n\n\n\n889318 rows √ó 3 columns\n\n\n\nA threshold of approx. 0.2 seems to be a good compromise for precision/recall.\n\nmhcpred_proba_threshold = 0.2\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9697360118629252, 0.8462451280426622)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98    900725\n        True       0.68      0.71      0.69     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.83      0.85      0.84    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nWe see that we have improved the balanced accuracy. We have a deterioration of the precision but a better recall.\nSource: Metrics"
  },
  {
    "objectID": "posts/encoding-distances/unimol-gbf.html",
    "href": "posts/encoding-distances/unimol-gbf.html",
    "title": "Encoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT",
    "section": "",
    "text": "This blog post compares two different methods for encoding distances in 3D molecules and protein pockets: Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT). We analyze their performance within the Uni-Mol framework, a universal 3D molecular representation learning model. We observed unstable gradients with GBFPT and hypothesized that DCEPT, inspired by AlphaFold‚Äôs distance representation, might offer a more stable alternative. We found that while DCEPT exhibits more stable training behavior, GBFPT ultimately yields superior pocket embeddings for retrieval tasks.\nBefore diving into the details, let‚Äôs define the acronyms used in this article:\n\nGBFPT: Gaussian Basis Function with Pair Type\nDCEPT: Discretization Categorical Embedding with Pair Type\n\n\nIntroduction to Uni-Mol and Distance Encoding\nCode for Uni-Mol is available at https://github.com/dptech-corp/Uni-Mol, and the related article is (Zhou et al. 2023). In brief, Uni-Mol is a 3D foundation model for molecules and pockets based on a SE(3) Transformer architecture. It comprises two pretrained models: one for molecular conformations and another for protein pocket data. Uni-Mol is pretrained on large-scale unlabeled data and is able to directly take 3D positions as both inputs and outputs. Uni-Mol backbone is a Transformer based model that can capture the input 3D information and predict 3D positions directly. Uni-Mol pretraining is done on two large-scale datasets: a 209M molecular conformation dataset and a 3M candidate protein pocket dataset, for pretraining 2 models on molecules and protein pockets, respectively. In the pretraining phase, Uni-Mol has to predict masked atoms, as well as masked noisy atoms coordinates and distances for effectively learning 3D spatial representation. The overall pretraining architecture is illustrated in Figure¬†2 and the framework is given in Figure¬†1 (taken from (Zhou et al. 2023)).\n\n\n\n\n\n\nFigure¬†1: Schematic illustration of the Uni-Mol framework\n\n\n\n\n\n\n\n\n\nFigure¬†2: Left: the overall pretraining architecture. Middle: the model inputs, including atom representation and pair representation. Right: details in the model block.\n\n\n\nBackground: 3D Spatial Encoding in Uni-Mol\nWe focus here on the encoding of the coordinates in distances (pair representation in Figure¬†2 middle part) and the decoding part, prediction of distances (pair-dist head in Figure¬†2 left part). In (Zhou et al. 2023) Section D.1, 3D spatial positional encodings benchmark, they investigate the performance of different 3D spatial positional encoding on the 3D molecular pretraining. In particular, they benchmarked:\n\nGaussian kernel (GK), a simply Gaussian density function.\nGaussian kernel with pair type (GKPT) (Shuaibi et al. 2021). Based on GK, an affine transformation according to the pair type is applied on pair distances, before applying the Gaussian kernel.\nRadial Bessel basis (RBB) (Gasteiger, Yeshwanth, and G√ºnnemann 2021). A Bessel based radial function.\nDiscretization categorical embedding (DCE). They convert the continued distances to the discrete bins, by Discretization. With binned distances, embedding-based positional encoding is directly used.\nDelta coordinate (DC) (Zhao et al. 2021). Following Point Transformer, the deltas of coordinates are directly used as pair-wise spatial relative positional encoding.\nGaussian kernel with pair type and local graph (GKPTLG). Based on GKPT, they set up a model with locally connected graphs. In particular, the cutoff radius is set to 6 √Ö.\n\nThe validation loss during pretraining for each encoding is summarized in Figure¬†3 (taken from (Zhou et al. 2023)). From the results, they drew the following conclusions:\n\nThe performance of DCE and GK are almost the same, and outperform RBB and DC. And they choose GK as the basic encoding.\nCompared with GK, GKPT converges faster. This indicates the pair type is critical in the 3D spatial positional encoding.\nCompared with GKPT, GKPTLG converges slower. This indicates the locally cutoff graph is not effective for self-supervised learning, and the default fully connected graph structure inherent in the Transformer architecture is more effective.\nAs GKPT outperforms all other encoding, they use it in the backbone model of Uni-Mol.\n\n\n\n\n\n\n\nFigure¬†3: Validation loss in pretraining for different 3D spatial encodings\n\n\n\nThe code for the GKPT encoding is given by:\nimport torch\nimport torch.nn as nn\n\n@torch.jit.script\ndef gaussian(x, mean, std):\n    pi = 3.14159\n    a = (2 * pi) ** 0.5\n    return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std)\n\n\nclass GaussianLayer(nn.Module):\n    def __init__(self, K=128, edge_types=1024):\n        super().__init__()\n        self.K = K\n        self.means = nn.Embedding(1, K)\n        self.stds = nn.Embedding(1, K)\n        self.mul = nn.Embedding(edge_types, 1)\n        self.bias = nn.Embedding(edge_types, 1)\n        nn.init.uniform_(self.means.weight, 0, 3)\n        nn.init.uniform_(self.stds.weight, 0, 3)\n        nn.init.constant_(self.bias.weight, 0)\n        nn.init.constant_(self.mul.weight, 1)\n\n    def forward(self, x, edge_type):\n        mul = self.mul(edge_type).type_as(x)\n        bias = self.bias(edge_type).type_as(x)\n        x = mul * x.unsqueeze(-1) + bias\n        x = x.expand(-1, -1, -1, self.K)\n        mean = self.means.weight.float().view(-1)\n        std = self.stds.weight.float().view(-1).abs() + 1e-5\n        return gaussian(x.float(), mean, std).type_as(self.means.weight)\nK represents the number of Gaussian basis functions, edge_types the number of possible edge types, x the distance matrix (for an initial 3D molecule or pocket) and edge_type the corresponding edge type matrix. Edge types represent the different types of atom pairs (e.g., C-C, C-O, C-N, etc.)\n\n\nExperimental Setup\n\n\n\n\n\n\nNote\n\n\n\nAll the Uni-Mol experiments run for this article are based on a small pockets dataset inspired from the PDBbind database (http://www.pdbbind.org.cn/), a collection of protein-ligand complexes and their binding affinities. The dataset was split into training and validation sets based on pocket similarity. The wandb project is available https://wandb.ai/nicolasb/unimol_analysis/ as well as a summary report https://api.wandb.ai/links/nicolasb/kdz59bry.\n\n\nMotivation for DCEPT and Implementation\nWhen we train Uni-Mol on a small dataset of pockets inspired from the PDBbind database, we remark that the gradients related to GaussianLayer parameters are not stable and can take very large values. To address the gradient instability observed with GBFPT, and drawing inspiration from AlphaFold‚Äôs use of discretization, we explored using a Discretization Categorical Embedding with Pair Type (DCEPT) as an alternative. In Figure¬†4, some gradients are of the order of one thousand. Uni-Mol relies on Uni-Core which implements gradient clipping and these high values do not affect the stability of the training.\n\n\n\n\n\n\nFigure¬†4: Gradients of GaussianLayer parameters (GBFPT encoding)\n\n\n\nNevertheless, we wanted to try another encoding that would be naturally stable without exploding gradients. Furthermore, DCE is the encoding used in AlphaFold (Jumper et al. 2021). We implemented Discretization categorical embedding with Pair Type encoding (DCEPT) that takes into account the edge type. A distogram is a discrete representation of the distance matrix, where distances are binned into predefined intervals. The binning process transforms continuous distances into discrete categories.\nimport torch\nimport torch.nn as nn\n\n# Constants\nPAD_DIST = 0\n\nclass NonLinearModule(nn.Module):\n    def __init__(self, input_dim, out_dim, activation_fn):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, out_dim)\n        self.activation = getattr(nn, activation_fn)()\n\n    def forward(self, x):\n        return self.activation(self.linear(x))\n\nclass DistEncoding(nn.Module):\n    def __init__(\n        self,\n        distogram_nb_bins: int,\n        nb_edge_types: int,\n        embedding_dim: int,\n        edge_type_padding_idx: int,\n        encoder_attention_heads: int,\n        activation_fn: str,\n    ):\n        \"\"\"\n        Initializes the DistEncoding module for encoding distances and edge types.\n\n        Args:\n            distogram_nb_bins: Number of bins for the distogram (distance discretization).\n            nb_edge_types: Number of possible edge types (e.g., different bond types).\n            embedding_dim: Dimension of the embeddings for distances and edge types.\n            edge_type_padding_idx: Padding index for edge type embeddings.\n            encoder_attention_heads: Number of attention heads in the Transformer encoder.\n            activation_fn: Activation function to use in the projection layer.\n        \"\"\"\n        super(DistEncoding, self).__init__()\n\n        # Embedding layer for the distogram (discretized distances)\n        self.dist_embedding = nn.Embedding(\n            num_embeddings=distogram_nb_bins,\n            embedding_dim=embedding_dim,\n            padding_idx=PAD_DIST,  # Use PAD_DIST for padding\n        )\n\n        # Embedding layer for edge types\n        self.edge_type_embedding = nn.Embedding(\n            num_embeddings=nb_edge_types,\n            embedding_dim=embedding_dim,\n            padding_idx=edge_type_padding_idx,\n        )\n\n        # Projection layer to combine distance and edge type embeddings and project\n        # to the correct dimension for attention bias.\n        self.projection = NonLinearModule(\n            input_dim=2 * embedding_dim,  # Concatenate dist and edge embeddings\n            out_dim=encoder_attention_heads,  # Output dimension matches attention heads\n            activation_fn=activation_fn,\n        )\n\n    def forward(\n        self, distogram: torch.Tensor, edge_types: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the DistEncoding module.\n\n        Args:\n            distogram: Tensor of discretized distances (batch_size, seq_len, seq_len).\n            edge_types: Tensor of edge types (batch_size, seq_len, seq_len).\n\n        Returns:\n            attn_bias: Tensor of attention biases (batch_size, num_heads, seq_len, seq_len).\n        \"\"\"\n        n_node = distogram.size(-1)  # Sequence length (number of nodes/atoms)\n\n        # Embed the discretized distances\n        dist_embeddings = self.dist_embedding(distogram)  # (B, L, L, D)\n\n        # Embed the edge types\n        edge_types_embeddings = self.edge_type_embedding(edge_types)  # (B, L, L, D)\n\n        # Concatenate distance and edge type embeddings\n        embeddings = torch.cat((dist_embeddings, edge_types_embeddings), dim=-1)  # (B, L, L, 2D)\n\n        # Project the combined embeddings to generate attention bias\n        attn_bias = self.projection(embeddings)  # (B, L, L, H) where H = num_heads\n\n        # Reshape the attention bias to the correct format for the Transformer\n        attn_bias = attn_bias.permute(0, 3, 1, 2).contiguous()  # (B, H, L, L)\n        attn_bias = attn_bias.view(-1, n_node, n_node) # (B*H, L, L) - Correct if attention is applied per head.\n        return attn_bias\ndistogram_nb_bins is the number of bins (128 by default), nb_edge_types the number of edge types, embedding_dim the dimension of the embedding (128 by default), encoder_attention_heads the number of attention heads in the transformer because the distance encoding is directly injected in the attention matrix. distogram is the distogram (discretization of the distance matrix) and edge_types the edge types. We concatenate the two embeddings creating de facto the DCEPT and then project to feed into the attention matrix.\nTraining Dynamics\nDuring the training, the gradients related to DistEncoding parameters do not take large values and are naturally stable without clipping gradients. This is illustrated in Figure¬†5.\n\n\n\n\n\n\nFigure¬†5: Gradients of DistEncoding parameters (DCEPT encoding)\n\n\n\nFollowing (Jumper et al. 2021) distogram prediction task, we also replace the distance prediction task (mean squared error loss) implemented in Uni-Mol by a distogram prediction task (cross entropy loss). We remark that this loss replacement does not change the characteristics of Uni-Mol.\nOn the small pockets inspired from the PDBbind database, we notice that the training and validation loss curves are better with DCEPT encoding compared to GBFPT encoding (on average), see Figure¬†6. More precisely, the masked_token_loss and the masked_acc metrics related to the recovery of masked atoms seem to stagnate a little at first with DCEPT encoding compared to GBFPT encoding. It may be due to the fact that DCEPT are at first completely random embeddings and less intuitive for the neural network. However, the masked_coord_loss is better with DCEPT encoding both in the training and validation sets. Note that the masked_distogram_loss corresponds to the distogram loss (cross entropy loss) used in AlphaFold (Jumper et al. 2021) and is implemented only for DCEPT encoding. For DCEPT encoding, we also add a distance prediction head with the corresponding MSE loss taken from Uni-Mol and a small multiplication factor (0.01). The rationale for including this head was to maintain some of the original Uni-Mol distance prediction capabilities alongside the distogram prediction. That explains why the DCEPT masked_dist_loss decreases slightly slower than GBFPT. Several additional experiments (not shown here) demonstrate that using a distogram or distance loss does not change the behavior of Uni-Mol.\nIn conclusion, according to the loss curves and the stability of the gradients, DCEPT seems to be a better encoding than GBFPT (or at least as good as) during pre-training.\n\n\n\n\n\n\nFigure¬†6: Uni-Mol training and validation losses with GBFPT and DCEPT encoding\n\n\n\n\n\nDownstream Performance: Pocket Retrieval\nHowever, Uni-Mol stands as a foundational model pre-trained through unsupervised methods. The pre-training metrics do not reflect the expected capabilities of the model. Notably, we expect that the pockets embeddings obtained with Uni-Mol should be good proxies of the pockets themselves: if two pockets are close to each other, their embeddings should be close in cos similarity or euclidean distance.\nWe have collected a dataset of 5 pockets (taken from 2oax, 3oxc, 5kxi, 5zk3 and 6v7a proteins) and for each pocket, a group of similar and dissimilar pockets. We compute the cos similarities between each reference pocket and the similar/dissimilar pockets and we sort the pockets by their cos similarity. Better embeddings translate into more similar pockets in the top retrieved pockets. More precisely, we sort the pockets by their cos similarities, we select the top 100 pockets, we count the number of similar pockets in the top 100 and we get a number between 0 and 1, the higher the better. We test two different embeddings: either, the vector corresponding to the [CLS] token (see (Zhou et al. 2023) Section 2.2) (indicated by _cls) or the mean of the pocket atoms vectors (indicated by _mean). Table¬†1 summarizes the results for each encoding, embedding and reference pocket and we remark that\n\nGBFPT is superior to DCEPT for pockets retrieval,\nThe mean of the pocket atoms vectors is better or near as good as the [CLS] embedding.\n\n\n\n\nTable¬†1: Uni-Mol pockets retrieval with GBFPT or DCEPT encoding (higher is better)\n\n\n\n\n\n\n6v7a\n2oax\n5kxi\n5zk3\n3oxc\n\n\n\n\nunimol_gbfpt_cls\n0.46\n1.0\n0.32\n0.3\n1.0\n\n\nunimol_gbfpt_mean\n0.79\n1.0\n0.38\n0.29\n1.0\n\n\nunimol_dcept_cls\n0.3\n1.0\n0.26\n0.29\n1.0\n\n\nunimol_dcept_mean\n0.51\n1.0\n0.27\n0.28\n1.0\n\n\n\n\n\n\nInvestigating Noise Sensitivity\nIn conclusion, despite better pre-training behavior and metrics, DCEPT encoding is disappointing when it comes to embeddings comparison. We suppose that this defect comes from a higher sensitivity of the discretization procedure. Two distance matrices from two close pockets may be more distinctly differentiated with DCEPT encoding compared to GBFPT encoding. This could be due to the information loss inherent in discretizing continuous distances. GBFPT might be capturing subtle long-range interactions that DCEPT, with its discrete representation, misses. To test this hypothesis, we take the 6v7a pocket and we noise its coordinates with a uniform noise between 0 and 1A. Since we have a batch size of 16, we fill up a batch with the reference pocket 6v7a and 15 noisy pockets. For each pocket, the distance matrix is encoded by GBFPT or DCEPT and we get an encoding of size 128 for each distance in the distance matrix. We compute the cos similarities between each encoding and the reference encoding in the reference matrix distance of 6v7a and we obtain the overall statistics of these cos similarities for GBFPT and DCEPT. In Table¬†2, we have the absolute errors statistics between 1 and the cos similarities of the noisy pockets from 6v7a, the lower the better because the vectors are similar. We remark that as presumed DCEPT encoding is less robust to noise compared to GBFPT encoding.\n\n\n\nTable¬†2: Statistics of cos similarities errors for noisy pockets from 6v7a\n\n\n\n\n\n\n\n\n\n\n\nAbsolute errors cos similarities GBFPT encoding (1 - cos similarities)\nAbsolute errors cos similarities DCEPT encoding (1 - cos similarities)\n\n\n\n\nmean\n0.002\n0.020\n\n\nmedian\n0.000\n0.007\n\n\n\n\n\n\n\n\nConclusion\nThese results suggest that while DCEPT offers advantages during pre-training in terms of gradient stability, the discretization process might lead to a loss of information and reduced robustness, ultimately hindering the quality of the generated pocket embeddings for downstream tasks like pocket retrieval. Further research is needed to investigate the optimal bin size for distograms and to explore alternative distance encoding techniques that balance stability and information retention. This analysis was conducted on a small, custom-built pocket dataset, and future work should evaluate these encoding methods on larger, more diverse datasets to ensure the generalizability of our findings.\n\n\n\n\n\nReferences\n\nGasteiger, Johannes, Chandan Yeshwanth, and Stephan G√ºnnemann. 2021. ‚ÄúDirectional Message Passing on Molecular Graphs via Synthetic Coordinates.‚Äù In Advances in Neural Information Processing Systems, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:15421‚Äì33. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2021/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. ‚ÄúHighly Accurate Protein Structure Prediction with AlphaFold.‚Äù Nature 596 (7873): 583‚Äì89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nShuaibi, Muhammed, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi, and C. Lawrence Zitnick. 2021. ‚ÄúRotation Invariant Graph Neural Networks Using Spin Convolutions.‚Äù https://arxiv.org/abs/2106.09575.\n\n\nZhao, Hengshuang, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. 2021. ‚ÄúPoint Transformer.‚Äù In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 16239‚Äì48. https://doi.org/10.1109/ICCV48922.2021.01595.\n\n\nZhou, Gengmo, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. ‚ÄúUni-Mol: A Universal 3D Molecular Representation Learning Framework.‚Äù In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=6K2RM6wVqKu.\n\nCitationBibTeX citation:@online{brosse2024,\n  author = {Brosse, Nicolas},\n  title = {Encoding {Distances} in {Molecules} and {Pockets:} {A}\n    {Comparison} of {GBFPT} and {DCEPT}},\n  date = {2024-04-24},\n  url = {https://nbrosse.github.io/posts/encoding-distances/unimol-gbf.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBrosse, Nicolas. 2024. ‚ÄúEncoding Distances in Molecules and\nPockets: A Comparison of GBFPT and DCEPT.‚Äù April 24, 2024. https://nbrosse.github.io/posts/encoding-distances/unimol-gbf.html."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html",
    "href": "posts/llm-slides/llm-slides.html",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "",
    "text": "Creating effective slide decks is essential for communicating ideas in business, academia, and public speaking. Large Language Models (LLMs) can assist in this process by saving time, improving clarity, and boosting creativity. They can generate content, suggest visualizations, and write speaker notes.\nHowever, using LLMs to create slides ‚Äî especially in Google Slides and PowerPoint ‚Äî is challenging. Why is this the case ? Can we create a better experience ?\nIn this post, I will:\nTo provide a fair and practical comparison, I selected the first pages of a real-world presentation: the EDF Producer Booklet (FET17CR). I then prompted an LLM, using Google Gemini Pro 2.5, to replicate the first few pages of this presentation in several different formats:\nI evaluated the LLM‚Äôs output based on:\nBelow, I present the results and discuss the strengths and weaknesses of each approach.\nThe code is available on GitHub, with a companion website here where you can view the rendered slides.\nThe post is structured as follows:"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#method-1-the-google-slides-api",
    "href": "posts/llm-slides/llm-slides.html#method-1-the-google-slides-api",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Method 1: The Google Slides API",
    "text": "Method 1: The Google Slides API\nThis is the standard, modern way for an external program (e.g., a Python script on your server, a Node.js web app) to talk to Google Slides. It‚Äôs a RESTful API, meaning your code makes HTTP requests to Google‚Äôs servers to perform actions.\nHow it Works:\n\nSetup: You must first set up a project in the Google Cloud Console.\nAuthentication: You enable the ‚ÄúGoogle Slides API‚Äù for your project and create credentials. This is the most complex part. You‚Äôll typically use either:\n\nOAuth 2.0: For applications where a user logs in and grants your app permission to manage their slides.\nService Account: For backend applications that act on their own behalf (e.g., a nightly script that generates a report). This is common for automation.\n\nCode: You use a Google-provided client library (available for Python, Java, Node.js, PHP, etc.) or make raw HTTP requests to send commands. All operations are typically sent in a batchUpdate request, which bundles multiple changes into a single API call for efficiency.\n\nKey Capabilities:\n\nCreate/Read Presentations: Create blank presentations or read the entire structure of an existing one.\nManage Slides: Add new slides (with predefined layouts), delete slides, duplicate slides, and reorder them.\nManipulate Shapes & Text: Insert new text boxes, shapes, images, and tables. You can precisely control their size, position, and styling (font, color, fill, etc.).\nData Merging: You can create a template presentation with placeholder text like {customer_name} or {quarterly_revenue}. Your code can then find and replace these placeholders with actual data, generating hundreds of custom decks from a single template.\nRead Speaker Notes: Programmatically get or set the speaker notes for any slide.\n\nPros:\n\nRobust & Scalable: Built for heavy, programmatic use.\nLanguage Agnostic: Works with any language that can make HTTP requests. Official libraries make it easy.\nPowerful Data Integration: The best method for data-driven slide generation (e.g., from a database or a CSV file).\nSecure: Uses industry-standard authentication (OAuth 2.0).\n\nCons:\n\nComplex Initial Setup: The Google Cloud Console and authentication flow can be intimidating for beginners.\nVerbose: Requests can be complex JSON objects. A simple action like ‚Äúinsert text‚Äù requires specifying object IDs, locations, etc.\nCannot Automate Everything: It cannot interact with the UI, click buttons, or use third-party add-ons. It only works with what the API exposes.\n\nExample Use Case (Python): Example of how to create a new presentation and add a title slide.\n# Assumes you have set up credentials and have 'credentials.json'\n# pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\n\n# --- Authentication ---\nSCOPES = ['https://www.googleapis.com/auth/presentations']\ncreds = Credentials.from_authorized_user_file('token.json', SCOPES) # Handle auth flow\nservice = build('slides', 'v1', credentials=creds)\n\n# --- Create a new Presentation ---\nbody = {'title': 'Automated Weekly Report'}\npresentation = service.presentations().create(body=body).execute()\npresentation_id = presentation.get('presentationId')\nprint(f\"Created presentation with ID: {presentation_id}\")\n\n# --- Add a Title Slide (assuming the first slide exists and is a title slide) ---\nrequests = [\n    {\n        'insertText': {\n            'objectId': 'your_title_object_id', # You'd need to find this ID first\n            'text': 'Weekly Sales Performance',\n            'insertionIndex': 0\n        }\n    },\n    {\n        'insertText': {\n            'objectId': 'your_subtitle_object_id', # Or get it from the slide layout\n            'text': 'Data from week ending 2023-10-27',\n            'insertionIndex': 0\n        }\n    }\n]\n\n# --- Execute the batch update ---\nbody = {'requests': requests}\nservice.presentations().batchUpdate(presentationId=presentation_id, body=body).execute()"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#method-2-google-apps-script",
    "href": "posts/llm-slides/llm-slides.html#method-2-google-apps-script",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Method 2: Google Apps Script",
    "text": "Method 2: Google Apps Script\nThis is a server-side JavaScript platform that lives inside your Google account. You write code in a browser-based IDE, and it can be attached to a specific Google Doc/Sheet/Slide or run as a standalone project.\nHow it Works: You open a Google Slides presentation, go to Extensions &gt; Apps Script, and start writing JavaScript code. The code uses services like SlidesApp, SpreadsheetApp, and GmailApp to interact with Google products. It feels more like writing macros than interacting with a remote API.\nKey Capabilities:\n\nEverything the Slides API can do, but with a simpler, more object-oriented syntax.\nCreate Custom Menus: Add your own menus to the Google Slides UI to trigger your scripts.\nTriggers: Run scripts automatically on a timer (e.g., every morning), when a form is submitted, or when the presentation is opened (onOpen).\nSeamless Integration: Effortlessly pull data from Google Sheets, create slides, and then email a PDF of the presentation using Gmail, all in one script.\n\nPros:\n\nZero Setup: No cloud console, no complex authentication. You just start writing code.\nExtremely Easy to Learn: The syntax is much more direct and intuitive than the API.\nPerfect for Internal Automation: Ideal for tasks within the Google Workspace ecosystem (e.g., ‚ÄúGenerate slides from this Google Sheet‚Äù).\nCan create simple UI elements (menus, sidebars, dialogs).\n\nCons:\n\nJavaScript Only: You are locked into the Google-flavored JavaScript environment.\nNot for External Apps: It‚Äôs difficult to call an Apps Script from an external, non-Google application (though possible via the Apps Script API or as a Web App).\nExecution Limits: Scripts have maximum runtimes (e.g., 6 minutes for a consumer account) and daily quotas.\n\nExample Use Case: Add a custom menu item ‚ÄúCreate Summary Slide‚Äù that reads data from a specific Google Sheet and generates a new slide.\n// This code is written in the Apps Script editor attached to a presentation.\n\nfunction onOpen() {\n  // Add a custom menu to the UI when the presentation is opened.\n  SlidesApp.getUi()\n      .createMenu('Automation')\n      .addItem('Generate Summary Slide', 'createSummarySlide')\n      .addToUi();\n}\n\nfunction createSummarySlide() {\n  const presentation = SlidesApp.getActivePresentation();\n  const spreadsheet = SpreadsheetApp.openById('your_spreadsheet_id_here');\n  const sheet = spreadsheet.getSheetByName('SalesData');\n  \n  // Get data from a cell in Google Sheets\n  const totalRevenue = sheet.getRange('B2').getValue();\n  const topProduct = sheet.getRange('B5').getValue();\n\n  // Add a new slide to the presentation\n  const slide = presentation.appendSlide(SlidesApp.PredefinedLayout.TITLE_AND_BODY);\n  \n  // Populate the new slide\n  slide.getShapes().find(s =&gt; s.isPlaceholder() && s.getPlaceholderType() == SlidesApp.PlaceholderType.TITLE).getText().setText('Sales Summary');\n  slide.getShapes().find(s =&gt; s.isPlaceholder() && s.getPlaceholderType() == SlidesApp.PlaceholderType.BODY).getText().setText(`Total Revenue: $${totalRevenue}\\nTop Product: ${topProduct}`);\n}"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#summary-comparison-table",
    "href": "posts/llm-slides/llm-slides.html#summary-comparison-table",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Summary Comparison Table",
    "text": "Summary Comparison Table\n\n\n\n\n\n\n\n\nFeature\nGoogle Slides API\nGoogle Apps Script\n\n\n\n\nPrimary Use Case\nBackend services, data-driven generation\nInternal automation within Google Workspace\n\n\nRobustness\nHigh. Official, stable interface.\nHigh. Stable internal interface.\n\n\nEase of Setup\nHard. Requires Google Cloud project & OAuth.\nVery Easy. No setup required.\n\n\nLanguage Support\nAny (Python, Node.js, Java, etc.)\nJavaScript only\n\n\nPerformance\nFast and efficient.\nFast, but with execution time limits.\n\n\nIntegration\nExcellent for external systems (databases, etc.)\nExcellent for internal G Suite apps (Sheets, Gmail)\n\n\nRecommendation\nThe standard choice for most programmatic work.\nThe best choice for quick, internal G Suite tasks."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#example-on-the-edf-producer-booklet",
    "href": "posts/llm-slides/llm-slides.html#example-on-the-edf-producer-booklet",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Example on the EDF Producer Booklet",
    "text": "Example on the EDF Producer Booklet\nThis section focuses on the Google Slides API, which offers the most flexible approach to programmatic slide creation. The complete implementation code is available on GitHub.\nUsing Google Gemini Pro 2.5, I generated code to create the initial slides of the EDF Producer Booklet. The development process encountered multiple errors and required extensive trial-and-error iterations, along with substantial manual adjustments to produce functional code. The final implementation, while working, lacks readability and would be challenging to maintain or modify.\nHere is the rendered slides:\n\n    &lt;p&gt;Your browser doesn't support PDF viewing. &lt;a href=\"https://nbrosse.github.io/llm-slide-deck/google/EDF_Presentation_google_slides.pdf\" target=\"_blank\"&gt;Click here to download the presentation (PDF)&lt;/a&gt;&lt;/p&gt;"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#the-powerful-offline-way-third-party-libraries",
    "href": "posts/llm-slides/llm-slides.html#the-powerful-offline-way-third-party-libraries",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "The Powerful, Offline Way: Third-Party Libraries",
    "text": "The Powerful, Offline Way: Third-Party Libraries\nThis is the most common and practical approach for server-side generation. These libraries provide a high-level API to abstract away the complexity of the underlying OOXML format.\n\nPython: python-pptx\n\nThe de facto standard in the Python world. It lets you create and modify .pptx files without needing PowerPoint installed.\nExample:\nfrom pptx import Presentation\nfrom pptx.util import Inches\n\nprs = Presentation()\ntitle_slide_layout = prs.slide_layouts[0] # Title Slide layout\nslide = prs.slides.add_slide(title_slide_layout)\ntitle = slide.shapes.title\nsubtitle = slide.placeholders[1]\n\ntitle.text = \"Hello, from Python!\"\nsubtitle.text = \"Created with python-pptx\"\n\nprs.save(\"report.pptx\")\n\nJavaScript/Node.js: pptxgenjs\n\nAn excellent, feature-rich library that works in both Node.js and the browser. It has a fluent API and extensive documentation.\nExample:\nconst pptxgen = require(\"pptxgenjs\");\n\nlet pptx = new pptxgen();\nlet slide = pptx.addSlide();\n\nslide.addText(\"Hello from Node.js!\", {\n  x: 1, y: 1, w: '80%', h: 1, fontSize: 36, fill: { color: '003366' }\n});\n\npptx.writeFile({ fileName: \"report.pptx\" });\n\nJava: Apache POI\n\nA mature, robust library from the Apache Software Foundation for working with Microsoft Office formats. The component for PowerPoint is XSLF (for .pptx). It‚Äôs powerful but can be more verbose than its Python/JS counterparts.\nExample (Conceptual):\nXMLSlideShow ppt = new XMLSlideShow();\nXSLFSlideMaster defaultMaster = ppt.getSlideMasters().get(0);\nXSLFSlideLayout titleLayout = defaultMaster.getLayout(SlideLayout.TITLE);\nXSLFSlide slide = ppt.createSlide(titleLayout);\n\nXSLFTextShape title = slide.getPlaceholder(0);\ntitle.setText(\"Hello from Java!\");\n\nFileOutputStream out = new FileOutputStream(\"report.pptx\");\nppt.write(out);\nout.close();"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#the-low-level-way-office-open-xml-ooxml-sdk",
    "href": "posts/llm-slides/llm-slides.html#the-low-level-way-office-open-xml-ooxml-sdk",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "The Low-Level Way: Office Open XML (OOXML) SDK",
    "text": "The Low-Level Way: Office Open XML (OOXML) SDK\nThis is a .NET library directly from Microsoft. It allows for the most granular control but is also the most complex. You are essentially manipulating the raw XML parts of the .pptx package.\n\nHow it Works: You programmatically create and relate XML parts like slide1.xml, presentation.xml, theme1.xml, etc.\nUse Case: When a third-party library doesn‚Äôt support a specific, advanced feature you need, or when you are in a strict .NET environment and want maximum performance.\nLanguage: .NET (C#, F#, VB.NET)."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#the-modern-cloud-native-way-microsoft-graph-api",
    "href": "posts/llm-slides/llm-slides.html#the-modern-cloud-native-way-microsoft-graph-api",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "The Modern, Cloud-Native Way: Microsoft Graph API",
    "text": "The Modern, Cloud-Native Way: Microsoft Graph API\nThis is Microsoft‚Äôs unified API for all Microsoft 365 services, including OneDrive, where PowerPoint files are stored. It is the cloud equivalent of the Google Slides API.\n\nHow it Works: You make RESTful API calls to the Graph endpoint. Its capabilities for creating slide content are still maturing and are not yet as comprehensive as libraries like python-pptx. It‚Äôs often used to manage files (upload, download, copy) that were generated by other means.\nKey Features:\n\nCreate a blank presentation in OneDrive or SharePoint.\nLimited ability to add slides and basic content.\nPowerful for file management and integration with the M365 ecosystem."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#the-classic-automation-way-vba-and-office-add-ins",
    "href": "posts/llm-slides/llm-slides.html#the-classic-automation-way-vba-and-office-add-ins",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "The Classic Automation Way: VBA and Office Add-ins",
    "text": "The Classic Automation Way: VBA and Office Add-ins\nThis method requires the PowerPoint application to be running. It‚Äôs for automating the application itself, not for server-side file generation.\n\nVBA (Visual Basic for Applications): The legacy scripting language built into Office applications. You can write macros to create slides, insert objects, and control every aspect of the UI.\nOffice Add-ins (Office.js): The modern replacement for VBA. You use JavaScript, HTML, and CSS to create a web-based task pane inside PowerPoint that can interact with the presentation document."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#example-on-the-edf-producer-booklet-1",
    "href": "posts/llm-slides/llm-slides.html#example-on-the-edf-producer-booklet-1",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Example on the EDF Producer Booklet",
    "text": "Example on the EDF Producer Booklet\nThe complete code example is available on GitHub. This section examines the python-pptx library, a widely-adopted and well-documented solution for programmatic PowerPoint generation.\nSimilar to the Google Slides approach, Google Gemini Pro 2.5 successfully generated code to create the initial slides of the EDF Producer Booklet using the python-pptx library. However, the output quality and visual fidelity proved disappointing. The generated code also suffered from poor readability and would be difficult to maintain. The rendered slides are displayed below:\n\n    &lt;p&gt;Your browser doesn't support PDF viewing. &lt;a href=\"https://nbrosse.github.io/llm-slide-deck/powerpoint/EDF_Presentation_powerpoint_slides.pdf\" target=\"_blank\"&gt;Click here to download the presentation (PDF)&lt;/a&gt;&lt;/p&gt;"
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#quartoreveal.js-vs-google-slides-vs-powerpoint-llm-centric",
    "href": "posts/llm-slides/llm-slides.html#quartoreveal.js-vs-google-slides-vs-powerpoint-llm-centric",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "Quarto/Reveal.js vs Google Slides vs PowerPoint (LLM-centric)",
    "text": "Quarto/Reveal.js vs Google Slides vs PowerPoint (LLM-centric)\n\n\n\n\n\n\n\n\n\nAspect\nQuarto / Reveal.js\nGoogle Slides (API/Apps Script)\nPowerPoint (.pptx via code)\n\n\n\n\nGeneration Path\nDirect text ‚Üí Markdown/Quarto ‚Üí render to HTML\nLLM writes API calls/scripts that manipulate slides\nLLM writes .pptx-generation code (e.g., python-pptx)\n\n\nReliability & Fidelity (from LLM)\nHigh. Structured text matches LLM strengths; fewer moving parts\nMedium‚ÄìLow. Verbose JSON/object IDs; brittle to small errors\nMedium‚ÄìLow. APIs are verbose; layout fidelity is hard\n\n\nSpeed to First Draft\nFast. One file, one render\nSlow‚ÄìMedium. Auth + batchUpdate cycles\nMedium. No auth, but heavy coding\n\n\nEditability & Maintenance\nExcellent. Plain-text .qmd; easy diffs and refactors\nPoor. You edit code that edits slides; presentation is opaque\nPoor‚ÄìMedium. Generated scripts are verbose and hard to read\n\n\nCollaboration\nGit-first; great for PRs, reviews, traceability\nGreat in-GUI collaboration; weak for generated code\nGood in M365 GUI; weak for generated code\n\n\n\nBottom line: if an LLM is in the loop, code-first Quarto/Reveal.js provides the most reliable, editable path from prompt ‚Üí slides. GUI-first tools shine for human collaboration but are brittle when fully generated by LLMs."
  },
  {
    "objectID": "posts/llm-slides/llm-slides.html#the-ideal-future-ui-vs-codestyle-markdown-under-the-hood",
    "href": "posts/llm-slides/llm-slides.html#the-ideal-future-ui-vs-codestyle-markdown-under-the-hood",
    "title": "LLM-Powered Slide Decks: A Comparison of Formats",
    "section": "The ideal future UI: VS Code‚Äìstyle, Markdown under the hood",
    "text": "The ideal future UI: VS Code‚Äìstyle, Markdown under the hood\nBuild a dedicated editor that feels like VS Code and uses Quarto/Reveal.js internally:\n\nDual-pane workspace: Markdown/Quarto source on the left; live Reveal.js preview on the right, hot-reloading on save.\nContext-aware copilot: Chat that is slide- and selection-aware (‚Äútighten this bullet‚Äù, ‚Äúrestyle this image‚Äù, ‚Äúsplit slide into two‚Äù). Actions map to structured edits in the .qmd.\nRich authoring UX: Outline/slide navigator, templates and themes, design tokens, fragments, notes, citations, and asset management.\nOne-click outputs: Export to HTML/PDF; Export to PowerPoint (.pptx) using a robust Markdown‚Üí.pptx converter (e.g., via python-pptx) with layout heuristics. Import from .pptx back to Markdown when needed.\nVersion control native: Git integrated diffs and reviews against the plain-text source; share links to rendered previews.\n\nThis approach plays to the strengths of LLMs far more than working directly with PowerPoint or Google Slides, which present challenging paradigms for language models. LLMs excel at generating HTML, CSS, and Markdown thanks to extensive exposure to these formats in their training data."
  }
]