[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nicolas’ Notebook",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\nDescription\n\n\n\n\n\n\nMar 20, 2025\n\n\nPDF RAG: Enhanced PDF Processing\n\n\ndeep learning, LLM, RAG\n\n\nPDF RAG: Exploring techniques for processing PDF documents with Language Models, including raw conversion, reformatting, structure parsing and RAG.\n\n\n\n\nFeb 26, 2025\n\n\nPython Concurrency\n\n\npython\n\n\nAn introduction to Python concurrency, covering core concepts, practical examples, and considerations for its use.\n\n\n\n\nFeb 18, 2025\n\n\nPDF Parsing for LLM Input\n\n\ndeep learning, LLM\n\n\nAn exploration of current PDF parsing capabilities for Large Language Model (LLM) input.\n\n\n\n\nFeb 6, 2025\n\n\nPredicting MHC-Peptide Binding with Machine Learning\n\n\ndeep learning, biology\n\n\nUsing machine learning to predict peptide binding affinity to Major Histocompatibility Complex (MHC) molecules.\n\n\n\n\nApr 24, 2024\n\n\nEncoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT\n\n\ndeep learning, biology\n\n\nA comparative analysis of Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT) for encoding distances in 3D molecular representations, using the Uni-Mol framework.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html",
    "href": "posts/py-concurrency/py-concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "This blog post is organized as follows:\nEach section builds on the previous ones to provide a comprehensive understanding of Python concurrency."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#what-is-concurrency",
    "href": "posts/py-concurrency/py-concurrency.html#what-is-concurrency",
    "title": "Python Concurrency",
    "section": "What Is Concurrency?",
    "text": "What Is Concurrency?\nConcurrency is the simultaneous occurrence of events. In Python, these events are called:\n\nThread\nTask\nProcess\n\nThese all represent a sequence of instructions that run in order. They can be stopped, and the CPU can switch to a different one. The state of each sequence is saved so it can be restored.\nThreads, tasks, and processes differ in their details. Multiple system processes can enable Python to run these sequences at the same time.\nThreads and asynchronous tasks run on a single processor, meaning they run one at a time. They take turns to speed up the overall process.\nIn a multi-threaded approach, the operating system knows about each thread and can interrupt it to run a different thread. This is also true for processes and is called preemptive multitasking.\nIn preemptive multitasking, code in the thread doesn’t need to do anything special to make the switch. A context switch can happen in the middle of a Python statement. This is because Python statements consist of low-level bytecode instructions.\nAsynchronous tasks use cooperative multitasking. The tasks cooperate by announcing when they’re ready to be switched out. The code in the task has to change to make this happen.\nThe benefit is that you know where your task will be swapped out, making it easier to understand the flow of execution. A task won’t be swapped out in the middle of a Python statement unless that statement is appropriately marked."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#what-is-parallelism",
    "href": "posts/py-concurrency/py-concurrency.html#what-is-parallelism",
    "title": "Python Concurrency",
    "section": "What Is Parallelism?",
    "text": "What Is Parallelism?\nParallelism involves executing separate processes, each running in its own Python interpreter.\nEach process can run on a different CPU core. This means they can run at the same time.\nHere’s a summary of Python modules for concurrency and parallelism:\n\n\n\n\n\n\n\n\n\nPython Module\nCPU\nMultitasking\nSwitching Decision\n\n\n\n\nasyncio\nOne\nCooperative\nThe tasks decide when to give up control.\n\n\nthreading\nOne\nPreemptive\nThe operating system decides when to switch tasks external to Python.\n\n\nmultiprocessing\nMany\nPreemptive\nThe processes all run at the same time on different processors.\n\n\n\nThese modules will be explored in this blog post.\nthreading and multiprocessing are low-level building blocks. They can often be replaced with concurrent.futures, which provides a higher-level interface. asyncio offers a different approach to concurrency.\nEach type of concurrency can be useful in its own way."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#synchronous-version",
    "href": "posts/py-concurrency/py-concurrency.html#synchronous-version",
    "title": "Python Concurrency",
    "section": "Synchronous Version",
    "text": "Synchronous Version\nThis version doesn’t use concurrency:\n\n\n\nListing 1: io_non_concurrent.py\n\n\nimport time\n\nimport requests\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with requests.Session() as session:\n        for url in sites:\n            download_site(url, session)\n\ndef download_site(url, session):\n    with session.get(url) as response:\n        print(f\"Read {len(response.content)} bytes from {url}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThis program downloads site contents from a list of addresses and prints their sizes. A session object from requests is used. Creating a Session object allows the library to retain state across requests and reuse the connection to speed things up. The session is created in download_all_sites() and then the list of sites is iterated through, downloading each one. Finally, the execution time is printed. Here’s an example of the final output:\n$ python io_non_concurrent.py\nRead 10966 from https://www.jython.org\nRead 276 from http://olympus.realpython.org/dice\n ⋮\nDownloaded 160 sites in 14.289619207382202 seconds\nThese results may vary depending on network conditions."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#multi-threaded-version",
    "href": "posts/py-concurrency/py-concurrency.html#multi-threaded-version",
    "title": "Python Concurrency",
    "section": "Multi-Threaded Version",
    "text": "Multi-Threaded Version\nWriting a multi-threaded program takes more effort. Here’s the same program using concurrent.futures and threading:\n\n\n\nListing 2: io_threads.py\n\n\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\n\nthread_local = threading.local()\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with ThreadPoolExecutor(max_workers=5) as executor:\n        executor.map(download_site, sites)\n\ndef download_site(url):\n    session = get_session_for_thread()\n    with session.get(url) as response:\n        print(f\"Read {len(response.content)} bytes from {url}\")\n\ndef get_session_for_thread():\n    if not hasattr(thread_local, \"session\"):\n        thread_local.session = requests.Session()\n    return thread_local.session\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThe overall structure is the same.\nA ThreadPoolExecutor is created to manage the threads. In this case, five workers or threads are requested. The correct number of threads is not constant from one task to another. With IO-bound problems, you’re not limited to the number of CPU cores. However, at some point, diminishing returns will occur due to the overhead of switching threads. Experimentation is recommended. A ThreadPoolExecutor creates a pool of threads, each of which can run concurrently. The executor controls how and when each of the threads in the pool will run. Using a thread pool can be beneficial when you have limited system resources but still want to handle many tasks.\nIn this version, the executor calls download_site() instead of doing it manually in a loop. The executor.map() method distributes the workload across the available threads. This method takes:\n\nA function to be executed on each data item\nA collection of data items to be processed by that function\n\nSince the function passed to .map() must take one argument, download_site() was modified to only accept a URL.\nBecause the operating system controls when tasks get interrupted, any data shared between the threads needs to be thread-safe. requests.Session() isn’t thread-safe.\nOne strategy is to use a thread-safe data structure. Another strategy is to use thread-local storage. When threading.local() is called, an object that resembles a global variable but is specific to each individual thread is created.\nWhen get_session_for_thread() is called, the session it looks up is specific to the particular thread on which it’s running. So each thread will create a session the first time it calls get_session_for_thread() and then will use that session on each subsequent call.\nHere’s the output:\n$ python io_threads.py\nRead 10966 from https://www.jython.org\nRead 276 from http://olympus.realpython.org/dice\n ⋮\nDownloaded 160 sites in 3.190047219999542 seconds\nThis is faster than the non-concurrent version.\nHere’s the execution timing diagram:\n\n\n\nTiming Diagram of a Threading Solution\n\n\nThe program uses multiple threads to have many open requests out to web sites at the same time. It takes more code to make this happen, and you have to give some thought to what data is shared between threads. Threads can interact in ways that are subtle and hard to detect. These interactions can cause race conditions that frequently result in random, intermittent bugs that can be difficult to find."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asynchronous-version",
    "href": "posts/py-concurrency/py-concurrency.html#asynchronous-version",
    "title": "Python Concurrency",
    "section": "Asynchronous Version",
    "text": "Asynchronous Version\nUsing multithreading can cut down the total execution time. Python’s asyncio module enables asynchronous I/O and can be even faster.\nAsynchronous processing is a concurrency model that’s suited for I/O-bound tasks. It avoids the overhead of context switching between threads by employing the event loop, non-blocking operations, and coroutines. Asynchronous code needs only one thread of execution to run concurrently.\nThe event loop controls how and when each asynchronous task gets to execute. It continuously loops through your tasks while monitoring their state. When the current task starts waiting for an I/O operation to finish, the loop suspends it and switches to another task. When the expected event occurs, the loop resumes the suspended task in the next iteration.\nA coroutine is similar to a thread but more lightweight. You can spawn many more coroutines than threads without significant overhead.\nBlocking function calls aren’t allowed in coroutines. A blocking call prevents other code from running while it’s waiting for data to arrive. A non-blocking call can give up control and wait to be notified when the data is ready.\nIn Python, you create a coroutine object by calling an asynchronous function, also known as a coroutine function. These are defined with the async def statement. Only within the body of an asynchronous function can you use the await keyword, which pauses the execution of the coroutine until the awaited task is completed:\nimport asyncio\n\nasync def main():\n    await asyncio.sleep(3.5)\nIn this case, main() is defined as an asynchronous function. The await keyword makes a non-blocking call to asyncio.sleep(), simulating a delay. While main() awaits the wake-up event, other tasks could potentially run.\nNote: To run the sample code above, you’ll need to either wrap the call to main() in asyncio.run() or await main() in Python’s asyncio REPL.\nThe Requests library is blocking, so a non-blocking counterpart, such as aiohttp, is needed. After installing this library, you can use it in the asynchronous version of the code:\n\n\n\nListing 3: io_asyncio.py\n\n\nimport asyncio\nimport time\nimport aiohttp\n\nasync def main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    await download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\nasync def download_all_sites(sites):\n    async with aiohttp.ClientSession() as session:\n        tasks = [download_site(url, session) for url in sites]\n        await asyncio.gather(*tasks, return_exceptions=True)\n\nasync def download_site(url, session):\n    async with session.get(url) as response:\n        print(f\"Read {len(await response.read())} bytes from {url}\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n\n\nThis version looks similar to the synchronous one.\nHere are the main differences:\n\nasyncio is imported from Python’s standard library.\nThe aiohttp library is imported.\nFunctions are redefined as asynchronous ones with the async keyword.\nThe await keyword is prepended to download_all_sites().\nThe async with statement is leveraged to create asynchronous context managers.\nA list of tasks is created using a list comprehension.\nasyncio.gather() is used to run all the tasks concurrently.\nThe completion of the session’s HTTP GET request is awaited before printing.\n\nThe session is shared across all tasks because they’re all running on the same thread.\nOne of the advantages of asyncio is that it scales well. Each task takes fewer resources and less time to create than a thread.\nThe asynchronous version is the fastest:\n$ python io_asyncio.py\nRead 10966 bytes from https://www.jython.org\nRead 10966 bytes from https://www.jython.org\n ⋮\nDownloaded 160 sites in 0.49083488899850636 seconds\nHere’s the execution timing diagram:\n\n\n\nTiming Diagram of a Asyncio Solution\n\n\nAdding async and await can be a complication, but it forces you to think about when a given task will get swapped out. The asyncio example can be run with hundreds of tasks without slowing it down. You need special asynchronous versions of libraries to gain the full advantage of asyncio. If one of the tasks doesn’t cooperate, then all the advantages of cooperative multitasking get thrown away. See Section 4 for a deep dive into asynchronous I/O with AsyncIOs"
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#process-based-version",
    "href": "posts/py-concurrency/py-concurrency.html#process-based-version",
    "title": "Python Concurrency",
    "section": "Process-Based Version",
    "text": "Process-Based Version\nThe examples so far have run on a single CPU. The multiprocessing module was designed to break down that barrier and run your code across multiple CPUs. It does this by creating a new instance of the Python interpreter to run on each CPU. Bringing up a separate Python interpreter is a heavyweight operation. Unlike the previous approaches, using multiprocessing allows you to take full advantage of the all CPUs that your computer has. Here’s the sample code:\n\n\n\nListing 4: io_processes.py\n\n\nimport atexit\nimport multiprocessing\nimport time\nfrom concurrent.futures import ProcessPoolExecutor\nimport requests\n\nsession: requests.Session\n\ndef main():\n    sites = [\n        \"https://www.jython.org\",\n        \"http://olympus.realpython.org/dice\",\n    ] * 80\n    start_time = time.perf_counter()\n    download_all_sites(sites)\n    duration = time.perf_counter() - start_time\n    print(f\"Downloaded {len(sites)} sites in {duration} seconds\")\n\ndef download_all_sites(sites):\n    with ProcessPoolExecutor(initializer=init_process) as executor:\n        executor.map(download_site, sites)\n\ndef download_site(url):\n    with session.get(url) as response:\n        name = multiprocessing.current_process().name\n        print(f\"{name}:Read {len(response.content)} bytes from {url}\")\n\ndef init_process():\n    global session\n    session = requests.Session()\n    atexit.register(session.close)\n\nif __name__ == \"__main__\":\n    main()\n\n\n\nThis looks similar to the multi-threaded example.\nHere’s what this code does:\n\nA global variable is declared to hold the session object.\nThreadPoolExecutor is replaced with ProcessPoolExecutor from concurrent.futures and init_process() is passed.\nA custom initializer function is defined that each process will call shortly after starting.\nA cleanup function is registered with atexit.\n\nThe pool creates a number of separate Python interpreter processes. The communication between the main process and the other processes is handled. The pool instance doesn’t specify how many processes to create. By default, it’ll determine the number of CPUs and match that. For an I/O-bound problem, increasing the number of processes won’t make things faster.\nNote: If you need to exchange data between your processes, then it requires expensive inter-process communication (IPC) and data serialization.\nEach process in the pool has its own memory space. They can’t easily share things like a session object. The initializer function parameter is built for this case. A global session variable can be initialized to hold the single session for each process.\nHere’s the output:\n$ python io_processes.py\nForkProcess-3:Read 10966 bytes from https://www.jython.org\nForkProcess-4:Read 276 bytes from http://olympus.realpython.org/dice\n ⋮\nDownloaded 160 sites in 3.428215079999063 seconds\nOn a computer with four CPU cores, it runs about four times faster than the synchronous version. It’s slower than the multi-threaded version and much slower than the asynchronous version.\nThe execution timing diagram for this code looks like this:\n\n\n\nTiming Diagram of a Multiprocessing Solution\n\n\nThere are separate processes executing in parallel. The diagrams of each one resemble the non-concurrent version. Multiprocessing is more useful for CPU-bound examples."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#async-and-await-keywords",
    "href": "posts/py-concurrency/py-concurrency.html#async-and-await-keywords",
    "title": "Python Concurrency",
    "section": "async and await Keywords",
    "text": "async and await Keywords\nThe async and await keywords are fundamental to asyncio.\n\nasync def: Defines a coroutine, a function that can be suspended and resumed.\nawait: Pauses execution inside a coroutine until an awaitable object (another coroutine or a Future) completes. When a coroutine encounters await, it yields control back to the event loop.\n\nasync def f(x):\n    y = await z(x)  # OK - `await` and `return` allowed in coroutines\n    return y\n\nasync def g(x):\n    yield x  # OK - this is an async generator\nA function defined with async def is a coroutine. It may use await, return, or yield, but all are optional.\nUsing await and/or return creates a coroutine function. To call a coroutine function, you must await it to get its results. It is less common to use yield in an async def block. This creates an asynchronous generator, iterated over with async for. Anything defined with async def may not use yield from, which will raise a SyntaxError.\nExample:\nimport asyncio\n\nasync def my_coroutine():\n    print(\"Coroutine started\")\n    await asyncio.sleep(1)  # Simulate an I/O operation\n    print(\"Coroutine finished\")\n\nasync def main():\n    await my_coroutine()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\nThis code demonstrates:\n\nmy_coroutine is defined as an async function, making it a coroutine.\nawait asyncio.sleep(1) pauses the coroutine for 1 second, simulating an I/O operation. The event loop can schedule other coroutines.\nasyncio.run(main()) starts the event loop and runs the main coroutine."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asynchronous-vs.-synchronous-execution-in-python-a-comparison",
    "href": "posts/py-concurrency/py-concurrency.html#asynchronous-vs.-synchronous-execution-in-python-a-comparison",
    "title": "Python Concurrency",
    "section": "Asynchronous vs. Synchronous Execution in Python: A Comparison",
    "text": "Asynchronous vs. Synchronous Execution in Python: A Comparison\nThis section compares the execution time of asynchronous and synchronous code in Python using a simple example. The example consists of a count function that prints “One”, waits for 1 second, and then prints “Two”. We execute this function three times, once using asyncio for asynchronous execution and once using a simple for loop for synchronous execution.\nAsynchronous Example\n\n\n\nListing 5: countasync.py\n\n\nimport asyncio\nimport time\n\nasync def count():\n    print(\"One\")\n    await asyncio.sleep(1)\n    print(\"Two\")\n\nasync def main():\n    await asyncio.gather(count(), count(), count())\n\nif __name__ == \"__main__\":\n    s = time.perf_counter()\n    asyncio.run(main())\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\n\n\nExplanation:\n\nasync def count(): defines an asynchronous function. The await asyncio.sleep(1) line allows the event loop to switch to other tasks while waiting for the sleep to complete, enabling concurrency.\nasync def main(): defines another asynchronous function that uses asyncio.gather() to run three instances of count() concurrently. asyncio.gather() ensures that all provided awaitables complete before it returns.\nasyncio.run(main()) starts the asyncio event loop and runs the main() function.\n\nOutput:\n$ python countasync.py\nOne\nOne\nOne\nTwo\nTwo\nTwo\ncountasync.py executed in 1.01 seconds.\nAs observed, the execution time is approximately 1 second. This is because all three count() functions are executed concurrently. The asyncio.sleep(1) calls allow the event loop to switch between the functions, effectively overlapping the wait times. The small amount over 1 second likely represents overhead from the asyncio event loop and the print function.\nSynchronous Example\n\n\n\nListing 6: countsync.py\n\n\nimport time\n\ndef count():\n    print(\"One\")\n    time.sleep(1)\n    print(\"Two\")\n\ndef main():\n    for _ in range(3):\n        count()\n\nif __name__ == \"__main__\":\n    s = time.perf_counter()\n    main()\n    elapsed = time.perf_counter() - s\n    print(f\"{__file__} executed in {elapsed:0.2f} seconds.\")\n\n\n\nExplanation:\n\ndef count(): defines a regular synchronous function.\ntime.sleep(1) pauses the execution of the current thread for 1 second. No other code can execute during this time.\ndef main(): calls the count() function three times in a loop. Each call blocks until it completes.\n\nOutput:\n$ python3 countsync.py\nOne\nTwo\nOne\nTwo\nOne\nTwo\ncountsync.py executed in 3.01 seconds.\nAs observed, the execution time is approximately 3 seconds. This is because each count() function call takes 1 second of sleep time, and the functions are executed sequentially."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#components-of-asyncio",
    "href": "posts/py-concurrency/py-concurrency.html#components-of-asyncio",
    "title": "Python Concurrency",
    "section": "Components of asyncio",
    "text": "Components of asyncio\n\nEvent Loop: Manages coroutine execution and I/O events. asyncio.run() creates and manages it.\nCoroutines: Functions defined with async def that can be suspended and resumed.\nTasks: Represent a coroutine scheduled to run in the event loop, created using asyncio.create_task(). Tasks are essential for running coroutines concurrently.\nFutures: Represent the result of an asynchronous operation. Coroutines may await Futures, allowing them to wait for the completion of an asynchronous operation without blocking the entire program.\nQueues: asyncio provides queue classes (asyncio.Queue) similar to the standard queue.Queue but designed for asynchronous use. Queues are useful for coordinating communication between coroutines.\n\nThe Event Loop\nThe event loop monitors coroutines, identifies idle coroutines, and schedules executable tasks. It wakes up idle coroutines when their awaited resources become available.\nasyncio.run() manages the event loop implicitly:\nasyncio.run() obtains the event loop, runs tasks until completion, and then closes the loop.\nKey points about the event loop:\n\nCoroutines require the event loop to execute.\nBy default, an AsyncIO event loop runs in a single thread and on a single CPU core, which is often sufficient.\n\nThere’s a more long-winded way of managing the asyncio event loop, with get_event_loop(). The typical pattern looks like this and is taken from llama-index async_utils.py.\n\n\n\nListing 7: asyncio_run\n\n\ndef asyncio_run(coro: Coroutine) -&gt; Any:\n    \"\"\"Gets an existing event loop to run the coroutine.\n\n    If there is no existing event loop, creates a new one.\n    \"\"\"\n    try:\n        # Check if there's an existing event loop\n        loop = asyncio.get_event_loop()\n\n        # If we're here, there's an existing loop but it's not running\n        return loop.run_until_complete(coro)\n\n    except RuntimeError as e:\n        # If we can't get the event loop, we're likely in a different thread, or its already running\n        # asyncio.get_event_loop() raises RuntimeError if there's no running loop in the current thread.\n        try:\n            # If the first attempt failed, try creating and running a new event loop.\n            return asyncio.run(coro)  # asyncio.run() creates a new event loop, runs the coroutine, and closes the loop.\n        except RuntimeError as e:\n            # If asyncio.run() also raises RuntimeError, it likely means there's a nested asyncio call.\n            raise RuntimeError(\n                \"Detected nested async. Please use nest_asyncio.apply() to allow nested event loops.\"\n                \"Or, use async entry methods like `aquery()`, `aretriever`, `achat`, etc.\"\n            )\n\n\n\nThis function aims to run an asyncio coroutine. It first tries to get the current event loop. If one exists and is not running, it runs the coroutine in that loop. If getting the current loop fails (RuntimeError), it means there’s no running loop in the current thread. In that case, it tries to create a new loop using asyncio.run(). If creating a new loop also fails (another RuntimeError), it likely indicates a nested asyncio scenario (trying to run asyncio code within already running asyncio code). The error suggests using nest_asyncio or asynchronous alternatives to avoid nested loops. Nesting is generally discouraged in asyncio. nest_asyncio allows nesting, but it can lead to unexpected behavior. It is generally recommended to re-factor code that uses nested async calls."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#asyncio-examples-in-practice",
    "href": "posts/py-concurrency/py-concurrency.html#asyncio-examples-in-practice",
    "title": "Python Concurrency",
    "section": "AsyncIO Examples in Practice",
    "text": "AsyncIO Examples in Practice\n\nChained Coroutines\n\n\n\nListing 8: chained.py\n\n\nimport asyncio\nimport random\nimport time\n\nasync def part1(n: int) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part1({n}) sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-1\"\n    print(f\"Returning part1({n}) == {result}.\")\n    return result\n\nasync def part2(n: int, arg: str) -&gt; str:\n    i = random.randint(0, 10)\n    print(f\"part2{n, arg} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n    result = f\"result{n}-2 derived from {arg}\"\n    print(f\"Returning part2{n, arg} == {result}.\")\n    return result\n\nasync def chain(n: int) -&gt; None:\n    start = time.perf_counter()\n    p1 = await part1(n)\n    p2 = await part2(n, p1)\n    end = time.perf_counter() - start\n    print(f\"--&gt;Chained result{n} =&gt; {p2} (took {end:0.2f} seconds).\")\n\nasync def main(*args):\n    await asyncio.gather(*(chain(n) for n in args))\n\nif __name__ == \"__main__\":\n    import sys\n    random.seed(444)\n    args = [1, 2, 3] if len(sys.argv) == 1 else map(int, sys.argv[1:])\n    start = time.perf_counter()\n    asyncio.run(main(*args))\n    end = time.perf_counter() - start\n    print(f\"Program finished in {end:0.2f} seconds.\")\n\n\n\nExample output:\n$ python3 chained.py 9 6 3\npart1(9) sleeping for 4 seconds.\npart1(6) sleeping for 4 seconds.\npart1(3) sleeping for 0 seconds.\nReturning part1(3) == result3-1.\npart2(3, 'result3-1') sleeping for 4 seconds.\nReturning part1(9) == result9-1.\npart2(9, 'result9-1') sleeping for 7 seconds.\nReturning part1(6) == result6-1.\npart2(6, 'result6-1') sleeping for 4 seconds.\nReturning part2(3, 'result3-1') == result3-2 derived from result3-1.\n--&gt;Chained result3 =&gt; result3-2 derived from result3-1 (took 4.00 seconds).\nReturning part2(6, 'result6-1') == result6-2 derived from result6-1.\n--&gt;Chained result6 =&gt; result6-2 derived from result6-1 (took 8.01 seconds).\nReturning part2(9, 'result9-1') == result9-2 derived from result9-1.\n--&gt;Chained result9 =&gt; result9-2 derived from result9-1 (took 11.01 seconds).\nProgram finished in 11.01 seconds.\npart1() sleeps for a variable time, and part2() starts as results become available. This example demonstrates how asyncio.gather can run multiple coroutines concurrently, and how await ensures that the second coroutine in each chain only runs after the first has completed. Each chain function represents an independent, sequential process. The final result showcases the execution time of individual chains and the total program execution time. Note that the total program time is roughly the longest individual chain time, demonstrating concurrency.\n\n\nProducer/Consumer Pattern with asyncio.Queue\n\n\n\nListing 9: asyncq.py\n\n\nimport asyncio\nimport itertools as it\nimport os\nimport random\nimport time\n\nasync def makeitem(size: int = 5) -&gt; str:\n    return os.urandom(size).hex()\n\nasync def randsleep(caller=None) -&gt; None:\n    i = random.randint(0, 10)\n    if caller:\n        print(f\"{caller} sleeping for {i} seconds.\")\n    await asyncio.sleep(i)\n\nasync def produce(name: int, q: asyncio.Queue) -&gt; None:\n    n = random.randint(0, 10)\n    for _ in it.repeat(None, n):  # Synchronous loop for each single producer\n        await randsleep(caller=f\"Producer {name}\")\n        i = await makeitem()\n        t = time.perf_counter()\n        await q.put((i, t))\n        print(f\"Producer {name} added &lt;{i}&gt; to queue.\")\n\nasync def consume(name: int, q: asyncio.Queue) -&gt; None:\n    while True:\n        await randsleep(caller=f\"Consumer {name}\")\n        i, t = await q.get()\n        now = time.perf_counter()\n        print(f\"Consumer {name} got element &lt;{i}&gt;\"\n              f\" in {now-t:0.5f} seconds.\")\n        q.task_done()\n\nasync def main(nprod: int, ncon: int):\n    q = asyncio.Queue()\n    producers = [asyncio.create_task(produce(n, q)) for n in range(nprod)]\n    consumers = [asyncio.create_task(consume(n, q)) for n in range(ncon)]\n    await asyncio.gather(*producers)\n    await q.join()  # Implicitly awaits consumers, too\n    for c in consumers:\n        c.cancel()\n\nif __name__ == \"__main__\":\n    import argparse\n    random.seed(444)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-p\", \"--nprod\", type=int, default=5)\n    parser.add_argument(\"-c\", \"--ncon\", type=int, default=10)\n    ns = parser.parse_args()\n    start = time.perf_counter()\n    asyncio.run(main(**ns.__dict__))\n    elapsed = time.perf_counter() - start\n    print(f\"Program completed in {elapsed:0.5f} seconds.\")\n\n\n\nOutput\n$ python3 asyncq.py -p 2 -c 2\nProducer 0 sleeping for 2 seconds.\nProducer 1 sleeping for 2 seconds.\nConsumer 0 sleeping for 2 seconds.\nConsumer 1 sleeping for 4 seconds.\nProducer 0 added &lt;50679576b9&gt; to queue.\nProducer 0 sleeping for 3 seconds.\nProducer 1 added &lt;413dd61b37&gt; to queue.\nConsumer 0 got element &lt;50679576b9&gt; in 0.00015 seconds.\nConsumer 0 sleeping for 1 seconds.\nProducer 1 added &lt;1b64e8bb2f&gt; to queue.\nConsumer 1 got element &lt;413dd61b37&gt; in 0.00012 seconds.\nConsumer 1 sleeping for 5 seconds.\nProducer 1 added &lt;413dd61b37&gt; to queue.\nConsumer 0 got element &lt;1b64e8bb2f&gt; in 0.00009 seconds.\nConsumer 0 sleeping for 5 seconds.\nProgram completed in 7.02420 seconds.\nThis example illustrates the producer-consumer pattern using asyncio.Queue. Multiple producers generate random items and place them in the queue, while multiple consumers retrieve items from the queue and process them. asyncio.Queue handles synchronization between producers and consumers, ensuring that consumers don’t try to retrieve items from an empty queue and that producers don’t overwhelm the consumers. The q.join() method ensures that the main function waits until all items in the queue have been processed before canceling the consumers and exiting. The producers are stopped when they are finished producing items, while the consumers are stopped after the queue is emptied, preventing an infinite loop. The time elapsed between item production and consumption is measured, demonstrating the efficiency of the asynchronous queue in handling concurrent operations.\n\n\nSome Utility Functions from Llama-Index for Handling Asynchronous Tasks\nThis section presents three utility functions from the llama-index library that help manage asynchronous tasks in different ways:\n\nA function to execute a list of async tasks (with optional progress bar)\nA function to run tasks in batches to avoid memory issues\nA function to run tasks with a limited number of workers\n\nThese functions build on the core asyncio concepts covered earlier and provide practical solutions for common async programming needs.\n\n\n\nListing 10: llamaindex_async_utils.py\n\n\ndef run_async_tasks(\n    tasks: List[Coroutine],\n    show_progress: bool = False,\n    progress_bar_desc: str = \"Running async tasks\",\n) -&gt; List[Any]:\n    \"\"\"Run a list of async tasks.\"\"\"\n    tasks_to_execute: List[Any] = tasks\n    if show_progress:\n        try:\n            import nest_asyncio\n            from tqdm.asyncio import tqdm\n\n            # jupyter notebooks already have an event loop running\n            # we need to reuse it instead of creating a new one\n            nest_asyncio.apply()\n            loop = asyncio.get_event_loop()\n\n            async def _tqdm_gather() -&gt; List[Any]:\n                return await tqdm.gather(*tasks_to_execute, desc=progress_bar_desc)\n\n            tqdm_outputs: List[Any] = loop.run_until_complete(_tqdm_gather())\n            return tqdm_outputs\n        # run the operation w/o tqdm on hitting a fatal\n        # may occur in some environments where tqdm.asyncio\n        # is not supported\n        except Exception:\n            pass\n\n    async def _gather() -&gt; List[Any]:\n        return await asyncio.gather(*tasks_to_execute)\n\n    outputs: List[Any] = asyncio_run(_gather())\n    return outputs\n\n\n\nThis function runs a list of asynchronous tasks concurrently using asyncio.gather().\n\nIt takes a list of coroutines (tasks), a boolean to show progress, and an optional progress bar description.\nIf show_progress is True, it attempts to use tqdm.asyncio to display a progress bar.\n\nIt uses nest_asyncio.apply() to handle cases where the code is running in an environment like Jupyter notebooks, which already have an event loop.\nIt defines an inner coroutine _tqdm_gather() that uses tqdm.gather() to run the tasks and display the progress.\nIt runs _tqdm_gather() using asyncio_run().\nIf any exception occurs during the tqdm process (e.g., tqdm.asyncio is not supported), it falls back to running the tasks without a progress bar.\n\nIf show_progress is False or the tqdm process fails, it defines an inner coroutine _gather() that uses asyncio.gather() to run the tasks concurrently.\nIt runs _gather() using asyncio_run().\nIt returns a list of the results from the completed tasks.\n\n\n\n\nListing 11: llamaindex_async_utils.py\n\n\ndef chunks(iterable: Iterable, size: int) -&gt; Iterable:\n    args = [iter(iterable)] * size\n    return zip_longest(*args, fillvalue=None)\n\nasync def batch_gather(\n    tasks: List[Coroutine], batch_size: int = 10, verbose: bool = False\n) -&gt; List[Any]:\n    output: List[Any] = []\n    for task_chunk in chunks(tasks, batch_size):\n        task_chunk = (task for task in task_chunk if task is not None)\n        output_chunk = await asyncio.gather(*task_chunk)\n        output.extend(output_chunk)\n        if verbose:\n            print(f\"Completed {len(output)} out of {len(tasks)} tasks\")\n    return output\n\n\n\nThe function chunks splits an iterable into chunks of a specified size.\n\nIt takes an iterable and the desired chunk size as input.\nIt creates multiple iterators from the input iterable using [iter(iterable)] * size.\nIt uses zip_longest() to group elements from these iterators into tuples, effectively creating the chunks.\nThe fillvalue=None argument ensures that shorter chunks are padded with None if the input iterable is not evenly divisible by the chunk size.\nThe function returns an iterator that yields the chunks. For example, chunks([1, 2, 3, 4, 5, 6, 7], 3) will return [(1, 2, 3), (4, 5, 6), (7, None, None)]\n\nThe function batch_gather runs a list of asynchronous tasks in batches.\n\nIt takes a list of coroutines (tasks), a batch size, and a boolean for verbose output.\nIt uses the chunks() function to split the tasks into batches.\nFor each batch (task_chunk), it filters out any None values (which might have been added by chunks() due to padding).\nIt uses asyncio.gather() to run the tasks in the current batch concurrently.\nThe results from each batch are added to the output list.\nIf verbose is True, it prints a message indicating how many tasks have been completed so far.\nThe function returns a list of the results from all the completed tasks. This is useful when you have a very large list of coroutines, as sending them all to asyncio.gather could result in memory errors.\n\n\n\n\nListing 12: llamaindex_async_utils.py\n\n\nasync def run_jobs(\n    jobs: List[Coroutine[Any, Any, T]],\n    show_progress: bool = False,\n    workers: int = DEFAULT_NUM_WORKERS,\n    desc: Optional[str] = None,\n) -&gt; List[T]:\n    \"\"\"Run jobs.\n\n    Args:\n        jobs (List[Coroutine]):\n            List of jobs to run.\n        show_progress (bool):\n            Whether to show progress bar.\n\n    Returns:\n        List[Any]:\n            List of results.\n    \"\"\"\n    semaphore = asyncio.Semaphore(workers)\n\n    async def worker(job: Coroutine) -&gt; Any:\n        async with semaphore:\n            return await job\n\n    pool_jobs = [worker(job) for job in jobs]\n\n    if show_progress:\n        from tqdm.asyncio import tqdm_asyncio\n        results = await tqdm_asyncio.gather(*pool_jobs, desc=desc)\n    else:\n        results = await asyncio.gather(*pool_jobs)\n\n    return results\n\n\n\nThis function runs a list of asynchronous jobs (coroutines) with a specified number of workers and optional progress tracking.\n\nIt uses a semaphore to limit the number of concurrent workers.\nIt defines an inner coroutine worker() that acquires the semaphore before running a job and releases it afterward, ensuring that only a limited number of jobs run concurrently.\nIt creates a list of worker tasks (pool_jobs) by mapping each job to the worker() coroutine.\nIf show_progress is True, it uses tqdm_asyncio.gather() to run the worker tasks and display a progress bar.\nIf show_progress is False, it uses asyncio.gather() to run the worker tasks concurrently without a progress bar.\nThe function returns a list of the results from the completed jobs. This is useful when you want to limit the number of concurrent asyncio tasks being run, perhaps because of rate limiting by an API, or resource constraints on your system."
  },
  {
    "objectID": "posts/py-concurrency/py-concurrency.html#considerations-for-using-asyncio",
    "href": "posts/py-concurrency/py-concurrency.html#considerations-for-using-asyncio",
    "title": "Python Concurrency",
    "section": "Considerations for Using asyncio",
    "text": "Considerations for Using asyncio\nasyncio is suitable for I/O-bound tasks:\n\nNetwork applications: web servers, chat applications, API clients.\nWeb scraping: fetching data from multiple websites concurrently.\nReal-time applications: handling asynchronous events and data streams.\n\nIt is not appropriate for CPU-bound tasks like complex calculations or image processing. For CPU-bound tasks, consider multiprocessing.\nAdvantages of asyncio\n\nEfficiency: Reduces blocking and increases resource utilization.\nScalability: Handles concurrent connections with limited overhead.\nConcurrency with a single thread: Reduces multithreading complexities."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html",
    "href": "posts/pdf-rag/pdf-rag.html",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "",
    "text": "In this blog post, we focus on RAG (Retrieval-Augmented Generation) specifically for PDF documents. We will study how PDFs can be processed for use with Language Models (LLMs). The code relies on llama-index package which is versatile but evolving rapidly and unstable.\nPDFs are ubiquitous in businesses and throughout the human world. What’s remarkable is the sheer diversity of PDF types we encounter. These range from reports of just a few pages to documents spanning hundreds of pages, generally including a table of contents. We also frequently deal with slide decks, or presentations, which are common in corporate settings. These can vary from just a few slides to dozens or even hundreds, often with their own unique tables of contents and organizational structures.\nIt’s fascinating to consider how differently humans and computers process this information. Humans, when facing lengthy documents, generally tend to look at the table of contents and assess the document’s overall structure to identify areas of interest.\nHowever, machines, especially with page-by-page parsing, often lose the document’s overall structure. One way to address this is to use embeddings and chunking, attempting to find similarities through embeddings or semantic similarity to identify potentially relevant chunks of information.\nThis highlights the core challenge of the method: the need to process data differently as a human than as a machine, particularly to gain a holistic view of the problem.\nIn Section 1, we delve into PDF metadata and structure extraction. We explore the challenges and limitations of raw conversion and the necessity of reformatting to improve document structure. We also discuss the parsing of document structure to create a hierarchical index for efficient document retrieval. In Section 2, we implement a multi-document PDF RAG system based on LlamaIndex, focusing on query-based document retrieval and generation.\nSome important resources for this blog post include:\nAll code paths referenced in this post are relative to the pdf-rag GitHub Repo."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-markdown",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-markdown",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF to Markdown Conversion",
    "text": "PDF to Markdown Conversion\nFor PDF to Markdown conversion, we refer to the related blog post PDF parsing for LLM Input which describes different methods to parse PDFs for LLM input.\nFor RAG applications with PDFs, we’ll focus on using Vision Language Models (VLMs) for PDF-to-Markdown conversion. As discussed in the related blog post PDF parsing for LLM Input, VLMs offer a straightforward and effective solution. While the approach is more expensive than basic text extraction, it provides valuable capabilities like image description integration. Similar to LlamaParse but with more flexibility, our implementation allows:\n\nFull control over model selection\nChoice of different API providers\nEnd-to-end process management\n\nThe code implements a VLMPDFReader class (in src/pdf_rag/readers.py) that uses Vision Language Models (VLMs) to convert PDF documents to markdown format. Here’s how it works.\nKey Features\n\nUses both Gemini and Mistral models for processing.\nSupports both single PDFs and batch processing.\nIncludes a caching mechanism to avoid reprocessing.\nHandles API rate limiting and retries.\nProcesses PDFs page by page with parallel execution.\n\nArchitecture\nThe conversion process follows these steps:\n\nInitialization\n\nSets up API clients for Gemini and Mistral.\nValidates API keys and cache directory.\nLoads conversion template.\n\nPDF Processing\n\nLoads PDF file using PdfReader.\nProcesses each page individually.\nConverts PDF pages to images for VLM processing.\n\nVLM Processing Pipeline\n\nFirst attempts conversion with Gemini.\nFalls back to Mistral if Gemini returns “RECITATION”.\nUses a template to guide the conversion.\n\nOutput Generation\n\nCombines processed pages.\nAdds page markers for reference.\nCaches results to avoid reprocessing.\nReturns document with metadata.\n\n\nUsage Example\nreader = VLMPDFReader(\n    cache_dir=\"./cache\",\n    api_key_gemini=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    api_key_mistral=\"your_mistral_key\"  # or use env var MISTRAL_API_KEY\n)\n\n# Single file\ndoc = reader.load_data(\"path/to/document.pdf\")\n\n# Multiple files\ndocs = reader.load_data([\"doc1.pdf\", \"doc2.pdf\"])\nWhy do we use Gemini and Mistral models ?\nThe “RECITATION” Error in Gemini API\nThe RECITATION error in the Gemini API occurs when the model detects that it is generating content that closely resembles its training data, particularly copyrighted material. This error is designed to prevent the model from reproducing protected content verbatim. The issue arises unpredictably and can halt content generation mid-stream, leading to incomplete responses.\nIn this case of failure, we fall back to the Mistral model to parse the problematic pdf page.\nOverall, the RECITATION error is a significant challenge for developers using the Gemini API, highlighting the tension between content safety and usability\nVLMPDFReader is designed to be robust and efficient, with built-in caching and parallel processing capabilities to handle large documents or multiple files efficiently.\nHere is the associated template prompt for the VLM.\n\n\nClick to view template\n\nYou are a specialized document transcription assistant converting PDF documents to Markdown format.\nYour primary goal is to create an accurate, complete, and well-structured Markdown representation.\n\n&lt;instructions&gt;\n1. Language and Content:\n   - MAINTAIN the original document language throughout ALL content\n   - ALL elements (headings, tables, descriptions) must use source language\n   - Preserve language-specific formatting and punctuation\n   - Do NOT translate any content\n\n2. Text Content:\n   - Convert all text to proper Markdown syntax\n   - Use appropriate heading levels (# ## ###)\n   - Preserve emphasis (bold, italic, underline)\n   - Convert bullet points to Markdown lists (-, *, +)\n   - Maintain original document structure and hierarchy\n\n3. Visual Elements (CRITICAL):\n   a. Tables:\n      - MUST represent ALL data cells accurately in original language\n      - Use proper Markdown table syntax |---|\n      - Include header rows\n      - Add caption above table: [Table X: Description] in document language\n\n   b. Charts/Graphs:\n      - Create detailed tabular representation of ALL data points\n      - Include X/Y axis labels and units in original language\n      - List ALL data series names as written\n      - Add caption: [Graph X: Description] in document language\n\n   c. Images/Figures:\n      - Format as: ![Figure X: Detailed description](image_reference)\n      - Describe key visual elements in original language\n      - Include measurements/scales if present\n      - Note any text or labels within images\n\n4. Quality Requirements:\n   - NO content may be omitted\n   - Verify all numerical values are preserved\n   - Double-check table column/row counts match original\n   - Ensure all labels and legends are included\n   - Maintain document language consistently throughout\n\n5. Structure Check:\n   - Begin each section with clear heading\n   - Use consistent list formatting\n   - Add blank lines between elements\n   - Preserve original content order\n   - Verify language consistency across sections\n&lt;/instructions&gt;\n\n\nAn example of raw conversion is shown in Figure 1.\n\n\n\n\n\n\nFigure 1: Raw conversion from PDF to Markdown using VLM\n\n\n\nWe also have a PDFDirectoryReader class (in src/pdf_rag/readers.py) to provide batch processing capabilities for PDF files within a directory structure. It acts as a wrapper around the VLMPDFReader, adding directory handling and metadata extraction features.\nKey Features\n\nDirectory-based Processing: Handles both single PDF files and directories containing multiple PDFs\nMetadata Extraction: Automatically extracts and includes file metadata like:\n\nCreation date\nLast modified date\nFile size\nFile type\nRelative path information\n\nConfiguration Management:\n\nValidates directory paths and creates cache directories\nManages API keys for both Gemini and Mistral models\nSupports environment variable configuration\n\nParallel Processing: Configurable number of workers for concurrent processing\n\nUsage Example\nreader = PDFDirectoryReader(\n    root_dir=\"./documents\",\n    cache_dir=\"./cache\",\n    num_workers=4,\n    show_progress=True\n)\n\n# Process single PDF file\ndocs = reader.load_data(\"documents/sample.pdf\")\n\n# Process directory of PDFs\ndocs = reader.load_data(\"documents/reports/\")\n\n# Async processing\ndocs = await reader.aload_data(\"documents/reports/\")\nDifficulties and Drawbacks of Raw VLM Conversion\nThe raw conversion process has several limitations and drawbacks that can affect the quality and accuracy of the output. One of the main difficulties in PDF parsing is that it is generally done page by page, causing the loss of the overall document structure. We lose the appropriate headings that help retrieve the general structure of the document, and we end up with a very local and focused view. For example, when converting PDF to markdown, the heading structure specified by the markdown is generally incorrect - the headers and their levels don’t correspond to what is expected."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-reformatting",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-reformatting",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF Reformatting",
    "text": "PDF Reformatting\nThe loss of document structure during raw conversion necessitates reformatting to improve the overall document organization and coherence. Reformatting involves restructuring the content to ensure proper hierarchy, headings, and formatting. This step is crucial for enhancing readability and navigation within the document. In practice, we use a Large Language Model (LLM) to reprocess the document chunk by chunk, given that some documents can be very long. The aim is to provide a more comprehensive overview. We’re using Gemini, which has a very large context, which makes this approach quite helpful. We are observing an improvement in the markdown formatting, particularly with better title identification. It’s not always perfect, but there’s a more effective segmentation based on titles and markdown, resulting in better adherence to the overall document structure compared to what we had before. This makes it easier to have a structure a bit more coherent when you cut through markdowns, especially in reports.\nThe ReformatMarkdownComponent class (in src/pdf_rag/transforms.py) reformats markdown content using the Gemini API. Here’s a detailed breakdown of its functionalities:\n\nConfiguration Options\n\nmax_iters: Maximum iterations for reformatting (default: 50)\nin_place: Whether to modify nodes directly or create copies (default: True)\nnum_workers: Number of concurrent workers (default: 4)\nshow_progress: Display progress during processing (default: True)\n\nCaching Mechanism\n\nStores reformatted content in .reformatted.md files\nAvoids reprocessing previously reformatted content\nUses file path and cache directory from node metadata\n\nProcessing Pipeline\n\nHandles individual nodes asynchronously\nUses Jinja2 templates for content reformatting\nSupports both landscape and portrait formats\nAccumulates reformatted content iteratively\n\n\nUsage Example\ncomponent = ReformatMarkdownComponent(\n    api_key=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    num_workers=4,\n    show_progress=True\n)\n\n# Process nodes\nprocessed_nodes = component(nodes)\n\n# Async processing\nprocessed_nodes = await component.acall(nodes)\nThe component is designed to improve document structure through:\n\nHeading Hierarchies: Ensures proper nesting and levels (H1 -&gt; H2 -&gt; H3)\nConsistent Formatting: Standardizes markdown syntax, lists, and spacing\nContent Preservation: Maintains original language, technical details, and metadata\nFormat Handling: Supports both portrait (reports) and landscape (presentations) layouts\nQuality Checks: Validates completeness and structure accuracy\n\nHere is the associated template prompt for Gemini.\n\n\nClick to view template prompt for Gemini\n\n&lt;document&gt;\n{{ document }}\n&lt;/document&gt;\n\n{% if processed %}\n&lt;processed&gt;\n{{ processed }}\n&lt;/processed&gt;\n{% endif %}\n\nYou are a professional technical documentation editor specializing in markdown documents.\nYour task is to transform the document into a well-structured markdown document with clear hierarchy and organization.\n\n&lt;instructions&gt;\n1. Content Preservation (CRITICAL):\n    - PRESERVE ALL original content without exception\n    - Do not summarize or condense any information\n    - Maintain all technical details, examples, and code snippets\n    - Keep all original links, references, and citations\n    - Preserve all numerical data and specifications\n    {% if landscape %}\n    - Preserve `---end page ...` markers\n    {% endif %}\n\n2. Document Structure:\n    - Ensure exactly one H1 (#) title at the start\n    - Use maximum 3 levels of headers (H1 -&gt; H2 -&gt; H3)\n    - Avoid excessive nesting - prefer flatter structure\n    - Group related sections under appropriate headers\n    - If an existing TOC is present, maintain and update it\n    - Only create new TOC if none exists\n\n3. Formatting Standards:\n    - Use consistent bullet points/numbering\n    - Format code blocks with appropriate language tags\n    - Properly format links and references\n    - Use tables where data is tabular\n    - Include blank lines between sections\n\n4. Quality Checks:\n    - Compare final document with original for completeness\n    - Verify all technical information is preserved\n    - Ensure all examples remain intact\n    - Maintain all nuances and specific details\n\n5. Metadata & Front Matter:\n    - Include creation/update dates if present\n    - Preserve author information\n    - Maintain any existing tags/categories\n&lt;/instructions&gt;\n\n{% if processed %}\nPlease continue reformatting from where it was left off, maintaining consistency with the processed portion.\nEnsure NO content is omitted - preserve everything from the original document.\nAll sections should seamlessly integrate with the existing structure.\nEnd your response with &lt;end&gt;.\n{% else %}\nProvide the complete reformatted document following the above guidelines.\nWARNING: Do not omit ANY content - preserve everything from the original document.\nEnsure all sections are properly nested and formatted.\nEnd your response with &lt;end&gt;.\n{% endif %}\n\n\nPDFs can be either portrait-formatted reports or landscape-formatted slides. In both cases, the PDF is processed page by page, resulting in the loss of the overall document structure. However, the processing approach needs to differ depending on whether we’re dealing with slides or reports.\nFor slides (landscape format):\n\nHave a clear page-by-page structure\nEach slide is a discrete unit\nPage markers are essential for maintaining structure\n\nFor reports (portrait format):\n\nDo not rely on page breaks\nHave a clear heading hierarchy\nStructure comes from nested headers\n\nNote that the template prompt has a conditional part based on format:\n{% if landscape %}\n- Preserve `---end page ...` markers\n{% endif %}\nAn example of Markdown reformatting is shown in Figure 2.\n\n\n\n\n\n\nFigure 2: Markdown reformatting using LLM"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-metadata",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-metadata",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "PDF Metadata Extraction",
    "text": "PDF Metadata Extraction\nOnce we’ve converted and reformatted the PDF content into markdown format using Vision Language Models (VLMs), the next step is extracting valuable metadata from the documents. This section explores a series of specialized extractors built on the LlamaIndex framework, each designed to capture specific aspects of PDF documents:\n\nDocument context (author, date, language)\nTable of contents (both extraction and creation)\nPage structure (especially for presentations)\nDocument hierarchy and relationships\n\nThese extractors work together to create a comprehensive metadata layer that enhances document searchability and understanding.\nAll the extractors are defined in the file src/pdf_rag/extractors.py. Note that as experienced with reformatting markdown in Section 1.2, landscape-based vs portrait-based PDFs require different processing approaches. The extractors are designed to handle these differences effectively.\nThe GeminiBaseExtractor serves as the foundation for all extractors, providing:\n\nGemini API integration with configurable temperature and model selection\nAPI key validation and management\nAbstract interface for extraction operations\nParallel processing capabilities\n\n\n1. ContextExtractor\nExtracts contextual information from documents:\n\nProcesses document content using a specialized template\nReturns structured JSON with document context\nHandles parallel processing of multiple nodes\n\n\n\nClick to view template prompt for Gemini\n\n&lt;document&gt;\n{{ document }}\n&lt;/document&gt;\n\nPlease analyze the above document and provide output in the following JSON format:\n\n{\n    \"author\": \"detected_author\",\n    \"publication_date\": \"YYYY-MM-DD\",\n    \"language\": \"detected_language in ISO 639-1 language code format\",\n    \"document_type\": \"type_if_identifiable\"\n    \"themes\": [\n        {\n            \"name\": \"Theme name\",\n            \"description\": \"Brief explanation\"\n        }\n    ],\n    \"entities\": [\n            {\n                \"name\": \"Entity name\",\n                \"role\": \"Role/significance\"\n            }\n    ],\n    \"time_periods\": [\n        {\n            \"start_date\": \"YYYY-MM-DD\",\n            \"end_date\": \"YYYY-MM-DD\",\n            \"period_description\": \"Description of events/developments in this timeframe\",\n            \"is_approximate\": boolean,\n        }\n    ],\n    \"keywords\": [\n        \"keyword1\",\n        \"keyword2\"\n    ],\n    \"summary\": \"Concise summary text focusing on main points\"\n}\n\nNote: Keep descriptions concise and factual.\nIf an item is missing, answer with \"\".\nAnswer in the language of the document.\n\n\n\n\n2. TableOfContentsExtractor\nExtracts existing tables of contents from documents:\n\nFocuses on the first 10 pages by default\nSupports both portrait and landscape formats\nReturns an empty string if no TOC is found\nUses a templated approach for extraction\n\n\n\nClick to view template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\n{% if format == 'landscape' %}\nExtract the table of contents (TOC) from the document if present and specify the page number where the table of contents is located.\nThe table of contents, if it exists, is located on a single page.\nNote that each page in the document ends with a marker '--- end page n' where n is the page number.\n\nOutput format:\n- If a table of contents exists:\n  First line: \"Table of contents found on page X\"\n  Following lines: Complete TOC with its original structure and hierarchy\n- If no table of contents exists: Respond with exactly \"&lt;none&gt;\"\n{% else %}\nExtract the table of contents (TOC) from the document if it exists.\nIMPORTANT: the table of contents must be contained in consecutive lines in the source document itself.\n\nOutput format:\n- If a table of contents exists: complete TOC with its original structure and hierarchy\n- If no table of contents exists: Respond with exactly \"&lt;none&gt;\"\n{% endif %}\n\n\n\n\n3. TableOfContentsCreator\nCreates new tables of contents:\n\nPortrait Mode:\n\nAnalyzes markdown headers\nIncludes line numbers for reference\nRequires reformatted content\n\nLandscape Mode:\n\nGenerates TOC using Gemini\nIncludes a validation step\nTwo-phase process: draft and check\n\n\n\n\nClick to view draft template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\nGenerate a hierarchical table of contents for the slides deck above by:\n\n1. IDENTIFY SECTION BREAKS\n- Section breaks are marked by \"--- end page {n}\" where n is the page number\n\n2. EXTRACT SECTION INFO\n- Get the title text from each section break page\n- Record the corresponding page number\n- Validate that page numbers are unique and ascending\n\n3. FORMAT OUTPUT\nFormat each entry as:\n# {Section Title} (Page {n})\n\nExample output:\n# Introduction (Page 1)\n# Key Concepts (Page 5)\n# Implementation (Page 12)\n\nRequirements:\n- Page numbers must be unique and sequential\n- Ignore any formatting in the section titles\n\nThe TOC should help readers quickly navigate the main sections of the deck.\n\n\n\n\nClick to view check template prompt for Gemini\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\nGenerate a standardized table of contents following these rules:\n\n1. FORMAT REQUIREMENTS\n- Each entry: \"# {Title} (Page {n})\"\n- Page numbers must be integers in parentheses\n- One entry per line, no blank lines\n- Preserve original markdown formatting in titles\n- Page numbers ascending order\n\n2. VALIDATION\n- Reject duplicate page numbers\n- Reject duplicate titles\n- Page numbers must exist and be &gt; 0\n- Title cannot be empty\n\nExample valid output:\n# Executive Summary (Page 1)\n# Market Analysis (Page 3)\n# Financial Projections (Page 7)\n\n\n\n\n4. LandscapePagesExtractor\nSpecialized for landscape (presentation) documents:\n\nRequires an existing TOC (extracted or created)\nProcesses document content page by page\nUses template-based extraction\nReturns structured page information\n\n\n\nClick to view template prompt for Gemini\n\n&lt;doc&gt;\n{{ doc }}\n&lt;/doc&gt;\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\nExtract and list all pages from the slides deck in &lt;doc&gt;, using the table of contents in &lt;toc&gt; as reference.\n\nRules:\n1. Format each line exactly as: Page N : [title]\n2. List pages in ascending numerical order (1, 2, 3...)\n3. When a page has no title, use the title from its preceding page\n4. Include all pages, even those without content\n\nExample:\nPage 1 : Introduction\nPage 2 : Market Analysis\nPage 3 : Market Analysis\nPage 4 : Key Findings\n\n\n\n\n5. StructureExtractor\nHandles document structure analysis:\n\nPortrait Mode:\n\nUses the created TOC for structure\nSimple structure representation\n\nLandscape Mode:\n\nCombines TOC and page information\nCreates a comprehensive structure\nRequires both TOC and page metadata\n\n\n\n\nClick to view template prompt for Gemini\n\n&lt;toc&gt;\n{{ toc }}\n&lt;/toc&gt;\n\n&lt;pages&gt;\n{{ pages }}\n&lt;/pages&gt;\n\nAnalyze the table of contents (TOC) in &lt;toc&gt; and the pages of the slides deck provided in &lt;pages&gt;.\nGroup the pages under their corresponding TOC sections using this format:\n\n# [TOC Section]\n- Page X : [Full Page Title]\n- Page Y : [Full Page Title]\n\nRules:\n- Each TOC section should have an appropriate level heading with #, ##, or ###.\n- List all pages that belong under each section\n- Maintain original page numbers and full titles\n- Include pages even if their titles are slightly different from TOC entries\n- Group subsections under their main section\n- List pages in numerical order within each section\n- Don't omit any pages\n- If a page doesn't clearly fit under a TOC section, place it under \"Other Pages\"\n\nExample:\n# Introduction\n- Page 1 : Welcome Slide\n- Page 2 : Project Overview\n\n# Key Findings\n- Page 3 : Financial Results\n- Page 4 : Market Analysis\n\n\nUsage Example\n# Initialize extractor\nextractor = TableOfContentsExtractor(\n    api_key=\"your_gemini_key\",  # or use env var GEMINI_API_KEY\n    head_pages=10,\n    temperature=0.1\n)\n\n# Process nodes\nresults = await extractor.aextract(nodes)\nEach extractor is designed to work asynchronously and handle batch processing efficiently, with built-in error handling and progress reporting.\nTo visualize the metadata and structure of processed documents, you can:\n\nRun the ingestion pipeline using the script in src/scripts/metadata_structure.py (detailed in Section 1.5)\nView the results in the Python Shiny app at src/app/app-metadata.py\n\nA live demo is available on the HF Space Demo for PDF Metadata. The visualization interface is shown in Figure 3.\n\n\n\n\n\n\nFigure 3: HF space demo for metadata and structure extraction from PDF"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-parsing-structure",
    "href": "posts/pdf-rag/pdf-rag.html#sec-parsing-structure",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Parsing Structure",
    "text": "Parsing Structure\nIn this section, we focus on parsing the structure of extracted metadata to create a hierarchical index for efficient document retrieval. We distinguish between portrait (report) and landscape (presentation) structures and describe the benefits of each.\n\nParsing Structures from Extracted Metadata\nAt the end of the extraction, we have two types of parsed structure. The portrait (report) structure and the landscape (presentation) structure. We will describe the differences between the two and their benefits.\nPortrait (Report) Structure\n\nHierarchical Headers: Uses markdown heading levels (#, ##, ###, ####) to represent document hierarchy\nLine Numbers: Each heading includes line numbers [line X] for precise content location\nLinear Flow: Follows a traditional document structure with nested sections\nContent Focus: Emphasizes content hierarchy and relationships\nExample Use Case: Technical reports, research papers, documentation\n\n\n\n\nListing 1: Portrait structure example\n\n\n# Deloitte. [line 0]\n## Pushing through undercurrents [line 2]\n### Technology's impact on systemic risk: A look at banking [line 3]\n### Risk 1: Risk exposure from Banking as a Service offerings [line 15]\n#### Table 1: Risk exposure from Banking as a Service offerings [line 29]\n### Risk 2: Inadequate stability mechanisms for stablecoin arrangements [line 38]\n#### Table 1: Information about forces that could amplify the risk and how the industry mitigate it? [line 52]\n### Contacts [line 63]\n\n\n\nLandscape (Presentation) Structure\n\nPage-Based Organization: Groups content by pages under main sections\nExplicit Page Numbers: Each entry shows “Page X : Title” format\nFlat Section Hierarchy: Uses mainly top-level headings (#) for major sections\nSequential Listing: Lists pages sequentially within each section\nExample Use Case: Slide decks, presentations, visual-heavy documents\n\n\n\n\nListing 2: Landscape structure example\n\n\n# Everest Group® Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n- Page 1 : Everest Group® Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n\n# Introduction\n- Page 2 : Introduction\n\n# Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n- Page 3 : Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n- Page 4 : Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n- Page 12 : Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n- Page 13 : Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023\n\n# Deloitte profile\n- Page 5 : Deloitte profile (page 1 of 6)\n- Page 6 : Deloitte profile (page 2 of 6)\n- Page 7 : Deloitte profile (page 3 of 6)\n- Page 8 : Deloitte profile (page 4 of 6)\n- Page 9 : Deloitte profile (page 5 of 6)\n- Page 10 : Deloitte profile (page 6 of 6)\n\n# Appendix\n- Page 11 : Appendix\n\n# FAQs\n- Page 14 : FAQs\n\n# Everest Group®\n- Page 15 : Everest Group®\n\n\n\nBenefits\n\nDocument Navigation\n\nPortrait: Quick access to specific content sections via line numbers\nLandscape: Easy location of specific slides via page numbers\n\nContent Retrieval\n\nPortrait: Granular access to nested content hierarchies\nLandscape: Efficient slide/page-based content lookup\n\nStructure Preservation\n\nPortrait: Maintains detailed document hierarchy and relationships\nLandscape: Preserves presentation flow and slide organization\n\n\nWe then use distinct parsers to process these structures into formal tree representations. Here’s a detailed description of both parsing functions. The function parse_portrait_structure (in src/pdf_rag/structure_parsers.py) parses the structure of portrait-formatted documents (like reports) and creates a tree representation.\n\nTakes a BaseNode document as input\nCreates a hierarchical tree structure based on document headers\nHeaders must follow format: #{level} {title} [line {number}]\nValidates line numbers are in ascending order\nReturns a TreeNode representing the document structure\n\nThe function parse_landscape_structure (in src/pdf_rag/structure_parsers.py) parses the structure of landscape-formatted documents (like presentations) and creates a tree representation.\n\nTakes a BaseNode document as input\nHandles both section headers and page entries\nCreates a hierarchical structure with page numbers\nTracks missing or duplicate pages\nCreates an “Uncategorized” section for orphaned pages\n\nBoth functions return a TreeNode object that can be traversed using breadth-first search (bfs) and supports standard tree operations like adding/removing children and setting parents.\nHere are the parsed structures for both document formats, shown in Listing 3 and Listing 4, which correspond to the raw structures in Listing 2 and Listing 1 respectively. These visualizations are also available in the HF Space Demo for PDF Metadata.\nIndentation shows parent/child relationships For landscape-oriented content, Numbers in [] represent:\n\nPositive numbers: page numbers\nNegative numbers: abstract nodes grouping related sections\n\n\n\n\nListing 3: Landscape structure parsed\n\n\nlife-sciences-smart-manufacturing-services-peak-matrix-assessment-2023 [-1]\n  Everest Group® Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [-2]\n    Everest Group® Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [1]\n  Introduction [-3]\n    Introduction [2]\n  Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [-4]\n    Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [3]\n    Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [4]\n    Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [12]\n    Life Sciences Smart Manufacturing Services PEAK Matrix® Assessment 2023 [13]\n  Deloitte profile [-5]\n    Deloitte profile (page 1 of 6) [5]\n    Deloitte profile (page 2 of 6) [6]\n    Deloitte profile (page 3 of 6) [7]\n    Deloitte profile (page 4 of 6) [8]\n    Deloitte profile (page 5 of 6) [9]\n    Deloitte profile (page 6 of 6) [10]\n  Appendix [-6]\n    Appendix [11]\n  FAQs [-7]\n    FAQs [14]\n  Everest Group® [-8]\n    Everest Group® [15]\n\n\n\nFor portrait-oriented content, Numbers in [] represent:\n\nPositive numbers: line numbers\nNegative numbers: abstract nodes grouping related sections\n\n\n\n\nListing 4: Portrait structure parsed\n\n\ndeloitte-tech-risk-sector-banking [-1]\n  Deloitte. [0]\n    Pushing through undercurrents [2]\n      Technology's impact on systemic risk: A look at banking [3]\n      Risk 1: Risk exposure from Banking as a Service offerings [15]\n        Table 1: Risk exposure from Banking as a Service offerings [29]\n      Risk 2: Inadequate stability mechanisms for stablecoin arrangements [38]\n        Table 1: Information about forces that could amplify the risk and how the industry mitigate it? [52]\n      Contacts [63]\n\n\n\n\n\nTree Index\nGiven these parsed structures, we now have access to a structured representation of the document content, divided by line numbers (portrait) or page numbers (landscape). The next step is to create a hierarchical index of the processed documents to support efficient traversal and querying. We need several components to build this index. First, we need two types of node parsing\n\nMarkdownLineNodeParser: For portrait-oriented content,\nMarkdownPageNodeParser: For landscape-oriented content\n\nHere’s a description of the two Markdown node parser classes.\nMarkdownLineNodeParser\n\nParses portrait-oriented documents (like reports) by splitting on headers\nTracks header hierarchy and line numbers\nPreserves code blocks by avoiding header parsing within them\nBuilds text nodes with header path and line number metadata\nUsed for documents where structure comes from markdown headers\n\nMarkdownPageNodeParser\n\nParses landscape-oriented documents (like presentations) by splitting on page markers\nSplits on --- end page N markers\nOptionally chunks content using a sentence splitter\nBuilds text nodes with page number metadata\nUsed for documents with clear page-based structure like slide decks\n\nThe key difference is that MarkdownLineNodeParser focuses on header-based structure while MarkdownPageNodeParser focuses on page-based structure, matching the different needs of reports versus presentations.\nUsing these parsers, we obtain document nodes that can be integrated into a hierarchical index. The TreeIndex class, located in src/pdf_rag/tree_index.py, extends BaseIndex to offer a hierarchical tree-based indexing structure for document nodes. It is specifically designed to manage both portrait (reports) and landscape (presentations) formatted documents. The TreeIndex class leverages the LlamaIndex framework, which provides a flexible and extensible indexing system for document nodes. Please note that this class is a work in progress; we have provided an initial draft of TreeIndex, but it is not yet fully implemented and should not be used in its current state.\nThe key features of the TreeIndex class (in src/pdf_rag/tree_index.py) include:\n\nData Structure\n\n\nUses IndexStructTree to maintain hierarchical relationships\nStores nodes in a tree structure with parent-child relationships\nSupports both root-level and nested nodes\nMaintains bidirectional references (parent-to-children and child-to-parent)\n\n\nNode Management\n\n\nNode insertion with automatic relationship building\nNode deletion with relationship cleanup\nReference document tracking\nSupport for abstract nodes (section headers) and content nodes (pages)\n\n\nNeo4j Integration\n\n\nExport functionality to Neo4j graph database\nCreates nodes and relationships in Neo4j\nMaintains document structure and metadata\nSupports constraints and cleanup operations\n\nUsage Example\n# Create index with nodes\nindex = TreeIndex(\n    nodes=document_nodes,\n    show_progress=True\n)\n\n# Export to Neo4j\nconfig = Neo4jConfig(\n    uri=\"neo4j://localhost:7687\",\n    username=\"neo4j\",\n    password=\"password\",\n    database=\"neo4j\"\n)\nindex.export_to_neo4j(config=config)\nFor now, we provide a basic implementation of the TreeIndex class to demonstrate the concept of hierarchical indexing. A visual representation of the index is shown in Figure 4.\n\n\n\n\n\n\nFigure 4: Neo4j visualisation of TreeIndex"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-ingestion-pipeline",
    "href": "posts/pdf-rag/pdf-rag.html#sec-ingestion-pipeline",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Script and Ingestion Pipeline",
    "text": "Script and Ingestion Pipeline\nThe script src/scripts/metadata_structure.py executes all the extractors in sequence through an ingestion pipeline. This process enriches documents with comprehensive metadata including author information, document context, table of contents, and structural relationships. The resulting metadata provides a rich foundation for document analysis and retrieval. It stores the processed data in both a local storage and optionally in a Neo4j database.\nTo execute the script, you need to provide a configuration file in YAML format. An example configuration file is provided in configs/metadata_structure_example.yaml. The configuration file includes settings for the pipeline, API keys, and file paths. The script supports three main modes:\n\ningest: Runs only the ingestion pipeline\ntree: Creates the tree index\nall: Runs both ingestion and indexing\n\nThe pipeline first ingests the PDF documents using PDFDirectoryReader to read PDF files. Then it processes the documents through the ingestion pipeline, which includes several transformations:\n\nReformatMarkdownComponent: Reformats content to markdown\nContextExtractor: Extracts contextual information\nTableOfContentsExtractor: Extracts table of contents\nTableOfContentsCreator: Creates structured TOC\nLandscapePagesExtractor: Handles landscape-oriented pages\nStructureExtractor: Extracts document structure\n\nIt stores the results in a SimpleDocumentStore for persistence and loading of processed data. Then, the script creates a hierarchical index of the processed documents using the TreeIndex class. The index is stored in a local directory and optionally exported to a Neo4j database.\nUsage\npython scripts/metadata_structure.py --config path/to/config.yaml\nThis script is particularly useful to process large collections of PDF documents and create searchable, structured knowledge bases from them. A Shiny app is provided to visualize and explore the metadata and structure of processed documents. The app allows you to:\n\nBrowse processed documents and their metadata\nView extracted table of contents\nExplore document structure trees\nCompare raw and reformatted markdown content\nVisualize document hierarchies\n\nYou can run the app locally using:\npython src/app/app-metadata.py\nA live demo showcasing these capabilities is available on the HF Space Demo for PDF Metadata. The visualization interface, shown in Figure 3, provides an interactive way to explore document metadata and structure."
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-description",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-description",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Description",
    "text": "Description\nThe code src/pdf_rag/react_agent_multi_pdfs.py implements a ReActAgentMultiPdfs class designed for querying information from multiple PDF documents using a ReAct agent approach. The core idea is to build an agent for each PDF document, and then a “top-level” agent orchestrates the use of these individual agents to answer complex queries. It leverages LlamaIndex for indexing, querying, and agent creation, and uses Gemini for LLM and embeddings. It includes both synchronous and asynchronous methods for building agents and processing queries to utilize concurrency.\nClass Definition:\nclass ReActAgentMultiPdfs:\n    def __init__(\n        self,\n        api_key_gemini: str,\n        api_key_mistral: str,\n        root_dir: Path,\n        pdfs_dir: Path,\n        cache_dir: Path,\n        storage_dir: Path,\n        num_workers: int = 16,\n        chunks_top_k: int = 5,\n        nodes_top_k: int = 10,\n        max_iterations: int = 20,\n        verbose: bool = True,\n    ) -&gt; None:\n        # ... initialization logic ...\nThe ReActAgentMultiPdfs class is initialized with several parameters:\n\napi_key_gemini: API key for the Google Gemini model.\napi_key_mistral: API key for the Mistral AI model.\nroot_dir: The root directory of the project.\npdfs_dir: The directory containing the PDF files to process.\ncache_dir: The directory to store cached data.\nstorage_dir: The directory to store the persisted indexes.\nnum_workers: Number of workers to use for parallel processing.\nchunks_top_k: Number of top chunks to retrieve from the vector index for each document.\nnodes_top_k: Number of top nodes to retrieve from the object index.\nmax_iterations: Maximum number of iterations for the ReAct agent.\nverbose: Whether to print verbose output.\n\nThe constructor initializes various components including:\n\nPDFDirectoryReader: Used to load data from PDF files.\nMarkdownPageNodeParser: Used to parse documents into nodes.\n\nKey Methods:\n\n_get_vector_summary_paths(self, pdf_file: Path) -&gt; tuple[str, Path, Path, Path]:\n\nThis helper function takes a pdf_file path and returns paths for storing the vector index, summary index, and summary text file, respectively.\nIt ensures that the paths are unique based on the PDF filename and are stored within the specified storage_dir.\nCritically it cleans the filenames, replacing spaces and parentheses with underscores, which is essential to avoid issues with LLM tool parsing.\n\nbuild_agent_per_doc(self, pdf_file: Path) -&gt; tuple[ReActAgent, str]:\n\nThis method is the heart of the per-document agent creation process.\nIt takes a pdf_file path as input.\nIt first checks if the vector and summary indexes already exist at the paths defined by _get_vector_summary_paths. If they do, it loads them from disk; otherwise, it creates them.\nIndex Creation: Loads the PDF data using self._pdf_reader, parses it into nodes using self._markdown_parser, and then builds both a VectorStoreIndex (for semantic search) and a SummaryIndex (for generating summaries). These are persisted to disk using storage_context.persist.\nQuery Engine Creation: Creates two query engines: vector_query_engine using the vector index and summary_query_engine using the summary index. The vector_query_engine uses FullPagePostprocessor to retrieve the full page for context.\nSummary Extraction: Extracts a summary of the document using the summary_query_engine and stores it in a text file.\nTool Definition: Defines two QueryEngineTools: one for the vector query engine and one for the summary query engine. These tools are used by the ReAct agent to answer questions. Each tool has a descriptive name and description that informs the agent when to use it.\nAgent Creation: Creates a ReActAgent from the defined tools and a system prompt that instructs the agent on how to use the tools. The system prompt emphasizes the need to use the tools and avoid relying on prior knowledge.\nReturns the created ReActAgent and the document summary.\n\nabuild_agent_per_doc(self, pdf_file: Path) -&gt; tuple[ReActAgent, str]:\n\nThis is the asynchronous version of build_agent_per_doc.\nIt uses async and await keywords to perform asynchronous operations, particularly when loading data using self._pdf_reader.aload_data and querying the summary engine.\nThe rest of the logic is very similar to the synchronous version.\n\nbuild_agents(self) -&gt; tuple[dict[str, ReActAgent], dict[str, dict]]:\n\nThis method iterates through all PDF files in the self._pdfs_dir directory.\nFor each PDF file, it calls self.build_agent_per_doc to create an agent and extract a summary.\nIt stores the agents in a dictionary agents_dict and the summaries in a dictionary extra_info_dict.\nReturns the dictionaries of agents and extra information.\n\nabuild_agents(self) -&gt; tuple[dict[str, ReActAgent], dict[str, dict]]:\n\nThis is the asynchronous version of build_agents.\nIt uses asyncio.gather (via run_jobs from LlamaIndex) to build the agents concurrently, improving performance.\nThe logic is otherwise similar to the synchronous version.\n\ntop_agent(self) -&gt; ReActAgent:\n\nThis method creates the “top-level” ReAct agent that orchestrates the use of the per-document agents.\nIt first calls self.build_agents to create the per-document agents.\nIt then iterates through the agents and creates a QueryEngineTool for each agent, using the document summary as the tool description.\nObject Index: Creates an ObjectIndex from the QueryEngineTools, and then a VectorStoreIndex on these objects (tools).\nCustom Object Retriever: Creates a CustomObjectRetriever that wraps the vector store retriever from the object index. This CustomObjectRetriever is critical: it adds a query planning tool to the retrieval process. This query planning tool is a SubQuestionQueryEngine which is specifically designed for comparing multiple documents.\nTop Agent Creation: Creates the ReActAgent using the CustomObjectRetriever as the tool_retriever. The system prompt instructs the agent to always use the tools provided.\nIt is decorated with @cached_property so it’s only built once.\n\nabuild_top_agent(self) -&gt; ReActAgent:\n\nThis is the asynchronous version of top_agent.\nIt uses await self.abuild_agents() to build the agents asynchronously.\nThe rest of the logic is similar to the synchronous version.\n\nprocess_queries(self, queries: list[str]) -&gt; list[Any]:\n\nProcesses a list of queries using the synchronous top_agent.\n\naprocess_queries(self, queries: list[str]) -&gt; list[Any]:\n\nProcesses a list of queries using the asynchronous top_agent.\n\n\nCustom Object Retriever (CustomObjectRetriever)\nThe CustomObjectRetriever class inherits from ObjectRetriever and overrides the retrieve method. The important modification here is the addition of a SubQuestionQueryEngine as a tool. This tool is specifically designed for comparison queries across multiple documents.\nHere’s a breakdown of the critical part of CustomObjectRetriever.retrieve:\n        sub_question_engine = SubQuestionQueryEngine.from_defaults(\n            query_engine_tools=tools,\n            llm=self._llm,\n            question_gen=LLMQuestionGenerator.from_defaults(llm=self._llm),\n        )\n        sub_question_description = f\"\"\"\n        Useful for any queries that involve comparing multiple documents. ALWAYS use this tool for comparison queries - make sure to call this\n        tool with the original query. Do NOT use the other tools for any queries involving multiple documents.\n        \"\"\"\n        sub_question_tool = QueryEngineTool(\n            query_engine=sub_question_engine,\n            metadata=ToolMetadata(name=\"compare_tool\", description=sub_question_description),\n        )\n        retrieved_tools = tools + [sub_question_tool]\n        return retrieved_tools\n\nA SubQuestionQueryEngine is created, using all the per-document tools. The LLMQuestionGenerator helps break down complex questions into sub-questions that can be answered by the individual tools.\nA QueryEngineTool is created for the SubQuestionQueryEngine, with a description emphasizing its use for comparison queries.\nThis sub_question_tool is added to the list of retrieved tools.\n\nKey Concepts:\n\nReAct Agent: A type of agent that interleaves reasoning and acting. It uses a language model to generate thoughts, actions, and observations.\nVector Store Index: An index that stores vector embeddings of text data. This allows for efficient semantic search.\nSummary Index: An index that stores summaries of text data. This allows for efficient summarization of documents.\nQuery Engine: A component that allows you to query an index.\nQueryEngineTool: A tool that allows an agent to query a query engine.\nObjectIndex: An index that stores objects (in this case, QueryEngineTools).\nObjectRetriever: A component that allows you to retrieve objects from an object index.\nSubQuestionQueryEngine: A query engine that breaks down complex questions into sub-questions and answers them using a set of tools. It excels at comparison queries.\nGemini: The LLM and embedding model used for this agent.\n\nOverall Functionality:\nThe ReActAgentMultiPdfs class provides a system for answering complex questions based on multiple PDF documents. It works by creating an agent for each document and then creating a top-level agent that can orchestrate the use of these individual agents. The CustomObjectRetriever plays a crucial role in enabling the top-level agent to handle comparison queries by including a sub-question query engine as a tool. The asynchronous methods offer a way to speed up processing when dealing with many documents. This architecture allows the system to answer questions that require information from multiple documents, such as “What are the key differences between document A and document B?”"
  },
  {
    "objectID": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-example",
    "href": "posts/pdf-rag/pdf-rag.html#sec-pdf-rag-example",
    "title": "PDF RAG: Enhanced PDF Processing",
    "section": "Example Usage",
    "text": "Example Usage\nWe provide an example usage in the notebook notebooks/query_multi_pdfs_example.ipynb. The notebook demonstrates how to use the ReActAgentMultiPdfs class to process multiple PDF documents and answer queries across them. It includes:\n\nSetting up the ReActAgent with multiple PDF documents\nProcessing queries about documents\nAnalyzing the agent’s reasoning process and tool selection\n\nThe complete example code and sample outputs are shown below, and the full notebook is available at ReActAgent: Querying Multiple PDFs Example.\n\n\nimport os\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nfrom omegaconf import OmegaConf, ValidationError\n\nfrom pdf_rag.react_agent_multi_pdfs import ReActAgentMultiPdfs\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n\n@dataclass\nclass ReActAgentConfig:\n    data_dir: Path | str\n    api_key_gemini: str | None = None\n    api_key_mistral: str | None = None\n    num_workers: int = 16\n    chunks_top_k: int = 5\n    nodes_top_k: int = 10\n    max_iterations: int = 20\n    verbose: bool = True\n    queries: list[str] = field(default_factory=list)\n\n    def __post_init__(self):\n        self.data_dir = Path(self.data_dir)\n        self.root_dir = self.data_dir / \"pdfs\"\n        self.pdfs_dir = self.data_dir / \"pdfs\"\n        self.cache_dir = self.data_dir / \"cache\"\n        self.storage_dir = self.data_dir / \"storage_queries\"\n\n        self.api_key_gemini = self.api_key_gemini or os.environ.get(\"GEMINI_API_KEY\")\n        self.api_key_mistral = self.api_key_mistral or os.environ.get(\"MISTRAL_API_KEY\")\n        if not self.api_key_gemini:\n            raise ValueError(\n                \"Gemini API Key is required. Provide api_key_gemini or set GEMINI_API_KEY environment variable.\"\n            )\n        if not self.api_key_mistral:\n            raise ValueError(\n                \"Mistral API Key is required. Provide api_key_mistral or set MISTRAL_API_KEY environment variable.\"\n            )\n\n\ndef load_and_validate_config(config_path: str) -&gt; ReActAgentConfig:\n    try:\n        config = OmegaConf.load(config_path)\n        react_agent_schema = OmegaConf.structured(ReActAgentConfig)\n        react_agent_config = OmegaConf.merge(react_agent_schema, config)\n        react_agent_config = ReActAgentConfig(**react_agent_config)\n        print(\"Configuration loaded and validated successfully:\")\n        return react_agent_config\n    except ValidationError as e:\n        raise ValidationError(f\"Validation error: {e}\")\n    except Exception as e:\n        raise Exception(f\"Error loading configuration: {e}\")\n\n\nconfig_path = \"../configs/query_multi_pdfs_nicolas.yaml\"\nconfig = load_and_validate_config(config_path)\n\nreact_agent_multi_pdfs = ReActAgentMultiPdfs(\n    api_key_gemini=config.api_key_gemini,\n    api_key_mistral=config.api_key_mistral,\n    root_dir=config.root_dir,\n    pdfs_dir=config.pdfs_dir,\n    cache_dir=config.cache_dir,\n    storage_dir=config.storage_dir,\n    num_workers=config.num_workers,\n    chunks_top_k=config.chunks_top_k,\n    nodes_top_k=config.nodes_top_k,\n    max_iterations=config.max_iterations,\n    verbose=config.verbose,\n)\n\nresponses = react_agent_multi_pdfs.process_queries(queries=config.queries)\nfor q, r in zip(config.queries, responses):\n    print(30 * \"-\")\n    print(f\"Query: {q}\")\n    print(f\"Response: {r}\")\n\nConfiguration loaded and validated successfully:\n\n\n&gt; Running step 0dc45634-70c7-4656-9b12-fc2b086848ce. Step input: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'}\n&gt; Running step b574597e-591e-42f5-b2fe-075603369f1c. Step input: vulnerabilities of APIs in Banking as a Service (BaaS)\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'API vulnerabilities in Banking as a Service (BaaS)'}\nObservation: BaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\n&gt; Running step ab6397c0-d075-499f-8d74-ce5ef25de495. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\nObservation: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n&gt; Running step 8b2c762a-4a21-47ad-81f9-5dba4bb5e0f3. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\n&gt; Running step 17a33cea-70cf-4150-98a9-73d9e08299cf. Step input: What mitigation opportunities are there to ensure strong security for BaaS platforms and API connectivity?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'mitigation strategies for BaaS platforms and API security'}\n&gt; Running step 0cde1ce4-d18b-4957-b0a9-0e8526141ad8. Step input: mitigation strategies for BaaS platforms and API security\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'mitigation strategies for BaaS platforms and API security'}\nObservation: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\n\n&gt; Running step 176b5919-f695-4091-8e2b-7ccc379b4dd0. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\nObservation: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. They can also improve due diligence on BaaS providers and help BaaS and other fintech providers get better at risk management and compliance through institutional knowledge transfer from banks to BaaS partners.\n&gt; Running step bf56f882-81b4-4053-9376-1dd9a231b405. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. Improving due diligence on BaaS providers and facilitating institutional knowledge transfer from banks to BaaS partners for better risk management and compliance are also beneficial mitigation strategies.\n&gt; Running step 69fc5519-d8d6-4ea3-ab8f-6ee9d32adcb7. Step input: How can the industry best improve due diligence on BaaS providers in this landscape?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'due diligence on BaaS providers'}\n&gt; Running step 57cce3c7-58b1-46a5-ae23-684437434197. Step input: due diligence on BaaS providers\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'due diligence on BaaS providers'}\nObservation: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\n\n&gt; Running step fa50b051-1a5f-4068-bbe5-0ace2f8b0a20. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\nObservation: Improving due diligence on BaaS providers is a mitigation opportunity for the risk exposure that comes from Banking as a Service offerings.\n&gt; Running step a835048a-e05e-4c02-b3bd-32f47024b7b4. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The industry can improve due diligence on BaaS providers by implementing strong security measures for BaaS platforms and thoroughly vetting BaaS partners.\n&gt; Running step 23143917-1992-4bf2-b67d-d81cb14b7fdc. Step input: What are the common objectives of the Open Data ecosystem?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_gx-iif-open-data\nAction Input: {'input': 'common objectives of Open Data ecosystem'}\n&gt; Running step ac79ff95-9ad1-4e03-a0ea-f75ff7b28194. Step input: common objectives of Open Data ecosystem\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_gx-iif-open-data\nAction Input: {'input': 'common objectives of Open Data ecosystem'}\nObservation: Open Data ecosystems share common objectives, including promoting innovation to increase consumer choice, creating more secure methods of data sharing, improving consumer data privacy and portability, and fostering cross-sectoral collaboration and interoperability.\n\n&gt; Running step 674ae599-801a-4725-8ea7-5ffe05a0a371. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Open Data ecosystems generally aim to promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\nObservation: Open Data ecosystems generally aim to promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n&gt; Running step 46685aec-07d0-46f4-b8ad-4424cfc72ebd. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The common objectives of the Open Data ecosystem are to promote innovation, create secure data sharing methods, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n&gt; Running step 537f15c8-4be9-4f96-be10-6d63a2959001. Step input: What are key strategic decisions to be made by ecosystem participants?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_gx-iif-open-data\nAction Input: {'input': 'strategic decisions for open data ecosystem participants'}\n&gt; Running step 4f1f9e8c-aca3-4b46-8546-f4584318dc4c. Step input: strategic decisions for open data ecosystem participants\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: summary_tool_gx-iif-open-data\nAction Input: {'input': 'strategic decisions for open data ecosystem participants'}\nObservation: When looking at the key strategic decisions to be made by ecosystem participants, the following hypotheses are of particular interest:\n\n- Ecosystems are built on symbiotic relationships and generate value in standard ways;\n- Data sharing is (and will continue to be) critical to ecosystems, but client consent and permission are priorities;\n- Interoperability will likely be key to increasing the value capture in ecosystems;\n- Broadening the set of data used by Fls and non-Fls could realize significant benefits for consumers, authorities, and the private sector; and\n- Cyber-security and operational resilience will likely play a key role in building trust in Open Data ecosystems.\n\n&gt; Running step 3c89568b-6983-4a14-ac6b-bb1d53104325. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The key strategic decisions for open data ecosystem participants revolve around symbiotic relationships, data sharing with client consent, interoperability, broadening data usage, and ensuring cyber-security and operational resilience to build trust.\nObservation: The key strategic decisions for open data ecosystem participants revolve around symbiotic relationships, data sharing with client consent, interoperability, broadening data usage, and ensuring cyber-security and operational resilience to build trust.\n&gt; Running step 106b85da-5fee-4418-9a40-1bea7d6f56cc. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Key strategic decisions for ecosystem participants involve establishing symbiotic relationships, facilitating data sharing with client consent, ensuring interoperability, broadening data usage, and prioritizing cyber-security and operational resilience to foster trust.\n&gt; Running step 54f9c69d-a738-4028-bc14-10b1efec857a. Step input: How can the public and private sectors collaborate to promote innovation, secure data sharing, and data privacy within the Open Data ecosystem?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_gx-iif-open-data\nAction Input: {'input': 'collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy'}\n&gt; Running step c8953756-0575-4165-9a6e-a29c23a8bbd1. Step input: collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_gx-iif-open-data\nAction Input: {'input': 'collaboration between public and private sectors in Open Data ecosystem to promote innovation, secure data sharing, and data privacy'}\nObservation: Open Data ecosystems promote innovation to increase consumer choice, create more secure methods of data sharing, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability. To leverage the potential of Open Data, data from different industries can be merged through sector-specific approaches that bring together private entities, the public sector, and consumers to create common data spaces for certain industries, while a holistic approach to data would help develop more customer-centric solutions.\n\n&gt; Running step be48c51d-9b67-4dfe-adf7-11b0591c0d09. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Open Data ecosystems foster cross-sectoral collaboration between private entities, the public sector, and consumers to create common data spaces for certain industries, promoting innovation, secure data sharing, and data privacy.\nObservation: Open Data ecosystems foster cross-sectoral collaboration between private entities, the public sector, and consumers to create common data spaces for certain industries, promoting innovation, secure data sharing, and data privacy.\n&gt; Running step 5d9dd1d3-8e6e-4291-b0aa-f5ba27b3c62a. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The public and private sectors can collaborate within the Open Data ecosystem by fostering cross-sectoral partnerships to create common data spaces. This collaboration promotes innovation, secure data sharing methods, and improvements in consumer data privacy.\n&gt; Running step 54dc5bbe-979a-435f-871c-c2c4dc733b26. Step input: What are the key characteristics that define the Leaders, Major Contenders, and Aspirants within the Life Sciences Smart Manufacturing Services PEAK Matrix?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix'}\n&gt; Running step f6a15c99-4121-4b3d-a5ed-3024171d1e1e. Step input: key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'key characteristics of Leaders, Major Contenders, and Aspirants in Life Sciences Smart Manufacturing Services PEAK Matrix'}\nObservation: **Leaders:** They act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n**Major Contenders:** This group includes a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n**Aspirants:** Aspirants have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\n\n&gt; Running step 2b6e54f6-31a9-4178-92e2-1c4a3efb5561. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Here are the key characteristics of Leaders, Major Contenders, and Aspirants in the Life Sciences Smart Manufacturing Services PEAK Matrix:\n\n**Leaders:** Act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n**Major Contenders:** Include a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n**Aspirants:** Have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\nObservation: Here are the key characteristics of Leaders, Major Contenders, and Aspirants in the Life Sciences Smart Manufacturing Services PEAK Matrix:\n\n**Leaders:** Act as digital transformation partners, providing end-to-end capabilities and a balanced set of offerings across the life sciences manufacturing value chain. They demonstrate flexibility and innovation in engagement models, possess a specialized talent pool, and have strong partnerships and investments in areas like digital twins and cybersecurity. They also have a clear roadmap for enhancing their capabilities through IP development and strategic initiatives.\n\n**Major Contenders:** Include a mix of midsize and large firms with a less balanced portfolio than Leaders, often specializing in specific areas of the value chain. They may have limited solutions for high-growth areas like specialty drugs or sustainable manufacturing. They address smart manufacturing use cases by leveraging cross-industry resources and focus on the mid-tier client segment.\n\n**Aspirants:** Have a restricted focus on specific areas within the life sciences manufacturing value chain and limited digital service capabilities. They have a limited partnership ecosystem and focus on leveraging horizontal capabilities rather than developing domain-specific services. They concentrate on capturing market share in the small and midsize buyer segment.\n&gt; Running step 5d2cea8b-eee1-4ee4-a8ab-98b0d453474a. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The key characteristics are:\n\n*   **Leaders:** Act as digital transformation partners with end-to-end capabilities, balanced offerings, flexible engagement models, specialized talent, strong partnerships, and a clear roadmap for enhancing capabilities.\n*   **Major Contenders:** Have a less balanced portfolio, often specializing in specific areas, with limited solutions for high-growth areas. They leverage cross-industry resources and focus on the mid-tier client segment.\n*   **Aspirants:** Have a restricted focus, limited digital service capabilities and partnership ecosystem, and concentrate on capturing market share in the small and midsize buyer segment.\n&gt; Running step 423c986e-395b-403b-afe0-8acada1ae7f1. Step input: What are some of the solutions that can assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality'}\n&gt; Running step bccdeb0c-a589-460a-b810-59927dee9e0e. Step input: solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality\nThought: The current language of the user is: english. I need to use a tool to help me answer the question.\nAction: summary_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'solutions for biopharma and MedTech manufacturers to optimize manufacturing processes and improve product quality'}\nObservation: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n&gt; Running step 2d287b00-2e7d-474e-94fe-75e04853131d. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\nObservation: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n&gt; Running step a8339752-282e-4d96-861c-339fe81db351. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n&gt; Running step e3b4b9cd-9da2-4737-9c43-662051410718. Step input: How are different areas of a manufacturing line benefiting from AI?\nThought: The current language of the user is: English. I need to find information about how AI is benefiting manufacturing lines. I don't have a specific tool that directly addresses this, but the \"tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\" tool seems most relevant as it discusses smart manufacturing in the life sciences, which likely involves AI applications.\nAction: tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'AI applications in life sciences manufacturing'}\n&gt; Running step 8285ebfe-60e4-4c07-b369-84bf63a817a8. Step input: AI applications in life sciences manufacturing\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_life-sciences-smart-manufacturing-services-peak-matrix-assessment-2023\nAction Input: {'input': 'AI applications in life sciences manufacturing'}\nObservation: Life sciences enterprises are investing in critical use cases, including digital twins and predictive maintenance, and exploring opportunities such as sustainable and personalized medicine manufacturing. Service providers are becoming end-to-end digital transformation partners, co-developing solutions to assist enterprises in their digital journeys. One solution enables visibility of multi-tier supply networks and senses upcoming risks to mitigate disruptions. Another platform solution enables a comprehensive view of manufacturing operations across the organization with connected data, creating dynamic data visualizations that provide useful insights and recommended actions, assisting in predictive analytics and quality analytics.\n\n&gt; Running step 2dc34498-25fe-491a-9487-ea77a5562c2b. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Life sciences companies are investing in AI applications like digital twins and predictive maintenance, and exploring opportunities in sustainable and personalized medicine manufacturing. Service providers are offering end-to-end digital transformation partnerships, co-developing solutions for digital journeys. These solutions include providing visibility into multi-tier supply networks to mitigate disruptions and offering platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\nObservation: Life sciences companies are investing in AI applications like digital twins and predictive maintenance, and exploring opportunities in sustainable and personalized medicine manufacturing. Service providers are offering end-to-end digital transformation partnerships, co-developing solutions for digital journeys. These solutions include providing visibility into multi-tier supply networks to mitigate disruptions and offering platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n&gt; Running step 3cd8ceec-02a0-4544-95a6-673923dca8b5. Step input: None\nThought: The current language of the user is: English. The tool provided some information about AI applications in life sciences manufacturing, including digital twins, predictive maintenance, supply chain visibility, and platforms for comprehensive views of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics. This gives a good overview of how AI is benefiting different areas of a manufacturing line.\nAnswer: AI is benefiting different areas of manufacturing lines through applications like digital twins and predictive maintenance. It also provides better visibility into supply chains and offers platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n&gt; Running step b23d31dd-3e78-4d90-bbe9-0c17280967db. Step input: What are ConocoPhillips' key financial priorities for the next 10 years?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_2023-conocophillips-aim-presentation\nAction Input: {'input': \"ConocoPhillips' 10-year financial plan\"}\n&gt; Running step c1216a19-59b1-4f3e-ba31-3e5bc2653a3c. Step input: ConocoPhillips' 10-year financial plan\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: summary_tool_2023-conocophillips-aim-presentation\nAction Input: {'input': \"ConocoPhillips' 10-year financial plan\"}\nObservation: ConocoPhillips has a compelling 10-year plan that includes the following:\n\n*   Peer-leading ROCE improving through time\n*   Top quartile ordinary dividend growth\n*   &gt;90% market cap distributed\n*   ~$35/BBL WTI FCF Breakeven\n*   ~6% CFO CAGR, ~11% FCF CAGR\n*   Unhedged for price upside\n\n&gt; Running step 85f186e1-6701-49c4-9ede-8877d99b5f7d. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips' 10-year financial plan aims for peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and being unhedged for price upside.\nObservation: ConocoPhillips' 10-year financial plan aims for peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and being unhedged for price upside.\n&gt; Running step ec8ff47a-d551-4907-93fb-d325380bc4ba. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips' key financial priorities for the next 10 years include: peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and remaining unhedged for price upside.\n&gt; Running step 296a9b6f-5116-4992-8417-a98b9ecf7e52. Step input: How does ConocoPhillips plan to meet it's Net-Zero and Emissions targets?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_2023-conocophillips-aim-presentation\nAction Input: {'input': 'ConocoPhillips Net-Zero and Emissions targets'}\n&gt; Running step b2a1e6ee-2120-498d-b83d-bfaa372b0646. Step input: ConocoPhillips Net-Zero and Emissions targets\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_2023-conocophillips-aim-presentation\nAction Input: {'input': 'ConocoPhillips Net-Zero and Emissions targets'}\nObservation: ConocoPhillips is progressing toward its 2050 net-zero ambition and accelerating its 2030 GHG emissions intensity reduction target. They plan to reduce GHG intensity 50-60% by 2030 and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They are also aiming for zero routine flaring by 2025.\n\n&gt; Running step 9b83e468-2f60-431e-940d-646ae3abca87. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips is working towards its 2050 net-zero ambition. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\nObservation: ConocoPhillips is working towards its 2050 net-zero ambition. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n&gt; Running step 52cb3db4-88f0-4e6c-98a2-85289b0dd630. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips plans to meet its Net-Zero ambition by 2050. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n&gt; Running step a9a3e10f-d4aa-4e20-afd1-fb3b6a6f4cb2. Step input: What is ConocoPhillips strategy to grow production?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_2023-conocophillips-aim-presentation\nAction Input: {'input': 'ConocoPhillips production growth strategy'}\n&gt; Running step 379518d2-dcd9-4126-96b0-6b262201571f. Step input: ConocoPhillips production growth strategy\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_2023-conocophillips-aim-presentation\nAction Input: {'input': 'ConocoPhillips production growth strategy'}\nObservation: ConocoPhillips has a strategy for capital-efficient production growth that underpins growing distribution capacity. They have a material low cost of supply resource base, leveraging existing infrastructure, with an average cost of supply of ~$30/BBL. Their production mix consists of approximately 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They anticipate a 4% CAGR at ~40% reinvestment rate over the next 10 years at $60/BBL WTI.\n\n&gt; Running step 36765352-5b93-4eac-b04d-c74a685229f3. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips' production growth strategy focuses on capital-efficient growth to support increasing distribution capacity. They leverage a substantial, low-cost supply resource base, utilizing existing infrastructure, with an average supply cost of approximately $30 per barrel. Their production is composed of roughly 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They project a 4% compound annual growth rate (CAGR) with about a 40% reinvestment rate over the next decade, assuming a West Texas Intermediate (WTI) crude oil price of $60 per barrel.\nObservation: ConocoPhillips' production growth strategy focuses on capital-efficient growth to support increasing distribution capacity. They leverage a substantial, low-cost supply resource base, utilizing existing infrastructure, with an average supply cost of approximately $30 per barrel. Their production is composed of roughly 60% oil, 5% NGL, 5% North American Gas, and 30% International Gas. They project a 4% compound annual growth rate (CAGR) with about a 40% reinvestment rate over the next decade, assuming a West Texas Intermediate (WTI) crude oil price of $60 per barrel.\n&gt; Running step 4fb25291-2915-4d64-b88a-2eba67a1f58d. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: ConocoPhillips' strategy to grow production involves capital-efficient growth supported by a substantial, low-cost resource base and existing infrastructure. They aim for a 4% compound annual growth rate (CAGR) over the next decade, with a reinvestment rate of about 40%, assuming a WTI crude oil price of $60 per barrel.\n&gt; Running step 271b97c5-318f-4709-9689-2efaa1692bfc. Step input: What are the key features and capabilities of the XC9500 In-System Programmable CPLD family?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_XC9500_CPLD_Family\nAction Input: {'input': 'key features and capabilities of XC9500 CPLD family'}\n&gt; Running step 38da0c2b-6f77-4b3e-998d-22dec2f0435f. Step input: key features and capabilities of XC9500 CPLD family\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_XC9500_CPLD_Family\nAction Input: {'input': 'key features and capabilities of XC9500 CPLD family'}\nObservation: The XC9500 CPLD family offers in-system programmability and testing for general purpose logic integration. Devices are in-system programmable for a minimum of 10,000 program/erase cycles and include IEEE 1149.1 (JTAG) boundary-scan support.\n\nKey features include:\n\n*   High-performance with 5 ns pin-to-pin logic delays and fCNT to 125 MHz.\n*   A density range of 36 to 288 macrocells, equating to 800 to 6,400 usable gates.\n*   5V in-system programmability with 10,000 program/erase cycle endurance.\n*   Enhanced pin-locking architecture.\n*   Flexible 36V18 Function Block with 90 product terms driving up to 18 macrocells.\n*   Global and product term clocks, output enables, set and reset signals.\n*   Extensive IEEE Std 1149.1 boundary-scan (JTAG) support.\n*   Programmable power reduction mode in each macrocell.\n*   Slew rate control on individual outputs.\n*   User programmable ground pin capability.\n*   Extended pattern security features for design protection.\n*   High-drive 24 mA outputs.\n*   3.3V or 5V I/O capability.\n*   Advanced CMOS 5V FastFLASH™ technology.\n*   Support for parallel programming of multiple devices.\n\nThe architecture includes features such as enhanced pin-locking, an expanded JTAG instruction set, and in-system programming across the full device operating range. Additional system features include output slew rate control, user-programmable ground pins, and configurable I/Os for 3.3V or 5V operation. All outputs provide 24 mA drive. The devices also offer a low-power mode for individual or all macrocells.\n\n&gt; Running step ad6a2bf8-8a03-4f32-a6f9-fae02d7d9e56. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The XC9500 CPLD family provides in-system programmability and testing. Key features include high performance (5 ns pin-to-pin delays, 125 MHz fCNT), densities from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability (10,000 cycles), enhanced pin-locking, flexible function blocks, global/product term clocks, JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, and advanced CMOS FastFLASH technology. It also supports parallel programming, expanded JTAG, and configurable I/Os.\nObservation: The XC9500 CPLD family provides in-system programmability and testing. Key features include high performance (5 ns pin-to-pin delays, 125 MHz fCNT), densities from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability (10,000 cycles), enhanced pin-locking, flexible function blocks, global/product term clocks, JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, and advanced CMOS FastFLASH technology. It also supports parallel programming, expanded JTAG, and configurable I/Os.\n&gt; Running step 125f1de5-aa3e-48fa-b445-a0a836de904e. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The key features and capabilities of the XC9500 In-System Programmable CPLD family include: high performance with 5 ns pin-to-pin delays and 125 MHz fCNT, densities ranging from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability with 10,000 program/erase cycles, enhanced pin-locking architecture, flexible Function Blocks, global and product term clocks, extensive JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, advanced CMOS FastFLASH technology, support for parallel programming, expanded JTAG, and configurable I/Os.\n&gt; Running step 7813cb38-11bb-41dd-b88b-5c5b044ba777. Step input: How does the Fast CONNECT switch matrix enable flexible signal routing and logic implementation within the XC9500 devices?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_XC9500_CPLD_Family\nAction Input: {'input': 'Fast CONNECT switch matrix'}\n&gt; Running step 5de85ea0-fe67-49a5-a791-1ee39c77be35. Step input: Fast CONNECT switch matrix\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_XC9500_CPLD_Family\nAction Input: {'input': 'Fast CONNECT switch matrix'}\nObservation: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\n\n&gt; Running step 7e74704f-8e13-4efd-bf1d-d2d80fc9a54e. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\nObservation: The Fast CONNECT switch matrix connects signals to the Function Block (FB) inputs. All I/O Block (IOB) outputs (corresponding to user pin inputs) and all FB outputs drive the Fast CONNECT matrix. Any of these signals (up to a FB fan-in limit of 36) may be selected, through user programming, to drive each FB with a uniform delay. The Fast CONNECT switch matrix can combine multiple internal connections into a single wired-AND output before driving the destination FB. This provides additional logic capability and increases the effective logic fan-in of the destination FB without any additional timing delay. This capability is available for internal connections originating from FB outputs only and is automatically invoked by the development software where applicable.\n&gt; Running step 6bfc34d0-b396-4ee0-aafe-6d14f487229b. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The Fast CONNECT switch matrix enables flexible signal routing by allowing any I/O Block (IOB) output (corresponding to user pin inputs) and all Function Block (FB) outputs to drive the matrix. User programming can select up to 36 of these signals to drive each FB with a uniform delay. It also increases logic implementation flexibility by combining multiple internal connections into a single wired-AND output before driving the destination FB, increasing the effective logic fan-in of the destination FB without additional timing delay.\n&gt; Running step 1d68eac4-1bf1-439d-a3d5-ede073dd7907. Step input: What design security options are available in the XC9500 family, and how do they protect programming data?\nThought: The current language of the user is: English. I need to find information about design security options in the XC9500 family.\nAction: tool_XC9500_CPLD_Family\nAction Input: {'input': 'design security'}\n&gt; Running step dca71de5-ce54-4035-81af-13b0d874fda6. Step input: design security\nThought: The current language of the user is: english. I need to use a tool to help me answer the question.\nAction: vector_tool_XC9500_CPLD_Family\nAction Input: {'input': 'design security'}\nObservation: XC9500 devices have advanced data security features that protect programming data against unauthorized reading or accidental erasure/reprogramming. There are four security settings available. The read security bits can be set to prevent the internal programming pattern from being read or copied, which also inhibits further program operations but allows device erasure. The write security bits provide added protection against accidental device erasure or reprogramming when the JTAG pins are subject to noise. Once set, the write-protection may be deactivated when the device needs to be reprogrammed with a valid pattern.\n\n&gt; Running step 8d883c8c-aaa2-4dc0-874e-625773179440. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: XC9500 devices offer advanced data security features to safeguard programming data from unauthorized access or accidental erasure/reprogramming. These devices provide four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\nObservation: XC9500 devices offer advanced data security features to safeguard programming data from unauthorized access or accidental erasure/reprogramming. These devices provide four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n&gt; Running step d27647c5-3477-4090-82b9-09ce5c02f89f. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The XC9500 family offers advanced data security features to protect programming data. It provides four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n&gt; Running step 10dc49fa-9fbd-40d4-a72e-1d84eda84d54. Step input: What strategies are companies implementing to address tax transformation in a data-driven world?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'strategies for tax transformation in a data-driven world'}\n&gt; Running step 9819b07e-37da-4fd9-9dac-a93d8b6e7e18. Step input: strategies for tax transformation in a data-driven world\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'strategies for tax transformation in a data-driven world'}\nObservation: To meet complex tax obligations, real-time reporting requirements, and increasing expectations of tax transparency, firms need immediate access to reliable, accurate, and fit-for-purpose tax data. Businesses should consider customizing existing ERP systems and identifying appropriate issues, involving the right stakeholders internally, obtaining budget when there are competing demands, and devising a robust schedule of maintenance. Tax departments are encouraged to develop a road map to use available technology, identify future needs, and then make build-or-buy decisions. Embedding tax into everyday processes and operations will lead to tax considerations in transformation efforts becoming \"business as usual,\" making building the business case for technology investment less onerous.\n\n&gt; Running step a498dc3f-6f71-44cc-9add-db894c222e83. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To transform tax in a data-driven world, businesses should focus on:\n\n*   Ensuring immediate access to reliable, accurate, and fit-for-purpose tax data to meet complex obligations, real-time reporting, and transparency expectations.\n*   Customizing existing ERP systems and addressing relevant issues.\n*   Involving the right internal stakeholders.\n*   Securing budget despite competing demands.\n*   Creating a robust maintenance schedule.\n*   Developing a technology roadmap to address current and future needs, guiding build-or-buy decisions.\n*   Embedding tax into everyday processes to make tax considerations routine in transformation efforts, simplifying the justification for technology investments.\nObservation: To transform tax in a data-driven world, businesses should focus on:\n\n*   Ensuring immediate access to reliable, accurate, and fit-for-purpose tax data to meet complex obligations, real-time reporting, and transparency expectations.\n*   Customizing existing ERP systems and addressing relevant issues.\n*   Involving the right internal stakeholders.\n*   Securing budget despite competing demands.\n*   Creating a robust maintenance schedule.\n*   Developing a technology roadmap to address current and future needs, guiding build-or-buy decisions.\n*   Embedding tax into everyday processes to make tax considerations routine in transformation efforts, simplifying the justification for technology investments.\n&gt; Running step f859adbd-5e63-4539-89f0-c13f01b9f2d2. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Companies are implementing strategies such as ensuring immediate access to reliable and accurate tax data, customizing ERP systems, involving internal stakeholders, securing budgets, creating maintenance schedules, developing technology roadmaps, and embedding tax into everyday processes to address tax transformation in a data-driven world.\n&gt; Running step 913a33a5-23de-43aa-a220-b71d419a6011. Step input: How can tax departments effectively invest in technology to meet compliance and strategic goals?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'How can tax departments effectively invest in technology to meet compliance and strategic goals?'}\n&gt; Running step 11574750-b6fa-48a0-b5c2-90b73fc01b01. Step input: How can tax departments effectively invest in technology to meet compliance and strategic goals?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'effective technology investment for tax compliance and strategic goals'}\nObservation: To effectively invest in technology, tax departments should first secure a budget and create a roadmap that leverages existing technology, identifies future needs, and guides build-or-buy decisions. Taking a top-down approach and explaining the impact of tax authority digitalization to the C-suite, as well as collaborating with the IT department, can help build a strong value case for budget. It's important to consider the long-term potential requirements, as finding a short-term solution without considering future needs may prevent the tax department from becoming a strategic advisor.\n\nWhen choosing technology, tax departments can implement using internal resources, appoint an implementation partner, or outsource the entire function requiring the technology. If using in-house resources, developing professional teams with data management and technology expertise is essential, especially in data analytics, strategic insights, and data management.\n\n&gt; Running step cf222cb7-a4c1-4012-9c72-6e32be1215b5. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To effectively invest in technology for tax compliance and strategic goals, tax departments should:\n\n1.  **Secure a budget and create a roadmap:** This roadmap should leverage existing technology, identify future needs, and guide build-or-buy decisions. A top-down approach, explaining the impact of tax authority digitalization to the C-suite, and collaboration with the IT department can help build a strong value case for budget.\n2.  **Consider long-term potential requirements:** Avoid short-term solutions that may hinder the tax department's ability to become a strategic advisor.\n3.  **Choose an implementation approach:** Options include using internal resources, appointing an implementation partner, or outsourcing the entire function.\n4.  **Develop professional teams:** If using in-house resources, create teams with data management and technology expertise, especially in data analytics, strategic insights, and data management.\nObservation: To effectively invest in technology for tax compliance and strategic goals, tax departments should:\n\n1.  **Secure a budget and create a roadmap:** This roadmap should leverage existing technology, identify future needs, and guide build-or-buy decisions. A top-down approach, explaining the impact of tax authority digitalization to the C-suite, and collaboration with the IT department can help build a strong value case for budget.\n2.  **Consider long-term potential requirements:** Avoid short-term solutions that may hinder the tax department's ability to become a strategic advisor.\n3.  **Choose an implementation approach:** Options include using internal resources, appointing an implementation partner, or outsourcing the entire function.\n4.  **Develop professional teams:** If using in-house resources, create teams with data management and technology expertise, especially in data analytics, strategic insights, and data management.\n&gt; Running step 62f00f19-9819-4183-a8d5-52fac1c5a514. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To effectively invest in technology for tax compliance and strategic goals, tax departments should secure a budget and create a roadmap that leverages existing technology and identifies future needs. They should also consider long-term potential requirements, choose an appropriate implementation approach (internal resources, partner, or outsourcing), and develop professional teams with data management and technology expertise.\n&gt; Running step cf14f83c-b67b-49a2-9429-4e9180ce5411. Step input: How can tax departments collaborate internally to secure budget and ensure technology alignment?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'internal collaboration in tax departments to secure budget and ensure technology alignment'}\n&gt; Running step 7f6e4001-7126-4e9b-87fd-e2713553240b. Step input: internal collaboration in tax departments to secure budget and ensure technology alignment\nThought: The current language of the user is: English. I need to use a tool to find information about internal collaboration in tax departments to secure budget and ensure technology alignment.\nAction: vector_tool_dttl-tax-technology-report-2023\nAction Input: {'input': 'internal collaboration in tax departments to secure budget and ensure technology alignment'}\nObservation: To make a case for budget, tax departments need to show the value they bring to the company, explain how tax authority digitalization impacts the C-suite, and work with the IT department. Companies that have taken a top-down approach have been more successful in building this value case.\n\nWhen seeking technology, best practice includes securing a budget and creating a road map to utilize available technology, identify future needs, and then decide whether to build or buy. A holistic view of the tax department's operating model is useful at this point to increase speed and accuracy, and free up tax professionals for more strategic business activity.\n\nWhen asked about the greatest need for skills over the next three to five years, respondents most often named data analytics, data-driven strategic insights, and data management.\n\n&gt; Running step 55db2b52-becf-4e0f-a00e-299c4a5048bb. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: To secure budget for tax technology, tax departments need to demonstrate their value to the company, explain the impact of tax authority digitalization on the C-suite, and collaborate with the IT department. Companies adopting a top-down approach have seen more success in building this value case. Best practices include creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model.\nObservation: To secure budget for tax technology, tax departments need to demonstrate their value to the company, explain the impact of tax authority digitalization on the C-suite, and collaborate with the IT department. Companies adopting a top-down approach have seen more success in building this value case. Best practices include creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model.\n&gt; Running step e35d23b0-83b9-4d79-b225-c08ea46ecec1. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Tax departments can collaborate internally to secure budget and ensure technology alignment by demonstrating their value to the company, explaining the impact of tax authority digitalization to the C-suite, and collaborating with the IT department. Creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model are best practices. Companies adopting a top-down approach have seen more success in building this value case.\n------------------------------\nQuery: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\nResponse: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\n------------------------------\nQuery: What mitigation opportunities are there to ensure strong security for BaaS platforms and API connectivity?\nResponse: To ensure strong security for BaaS platforms and API connectivity, the industry can use input validation protocols and apply network segmentation and access control measures. Improving due diligence on BaaS providers and facilitating institutional knowledge transfer from banks to BaaS partners for better risk management and compliance are also beneficial mitigation strategies.\n------------------------------\nQuery: How can the industry best improve due diligence on BaaS providers in this landscape?\nResponse: The industry can improve due diligence on BaaS providers by implementing strong security measures for BaaS platforms and thoroughly vetting BaaS partners.\n------------------------------\nQuery: What are the common objectives of the Open Data ecosystem?\nResponse: The common objectives of the Open Data ecosystem are to promote innovation, create secure data sharing methods, improve consumer data privacy and portability, and foster cross-sectoral collaboration and interoperability.\n------------------------------\nQuery: What are key strategic decisions to be made by ecosystem participants?\nResponse: Key strategic decisions for ecosystem participants involve establishing symbiotic relationships, facilitating data sharing with client consent, ensuring interoperability, broadening data usage, and prioritizing cyber-security and operational resilience to foster trust.\n------------------------------\nQuery: How can the public and private sectors collaborate to promote innovation, secure data sharing, and data privacy within the Open Data ecosystem?\nResponse: The public and private sectors can collaborate within the Open Data ecosystem by fostering cross-sectoral partnerships to create common data spaces. This collaboration promotes innovation, secure data sharing methods, and improvements in consumer data privacy.\n------------------------------\nQuery: What are the key characteristics that define the Leaders, Major Contenders, and Aspirants within the Life Sciences Smart Manufacturing Services PEAK Matrix?\nResponse: The key characteristics are:\n\n*   **Leaders:** Act as digital transformation partners with end-to-end capabilities, balanced offerings, flexible engagement models, specialized talent, strong partnerships, and a clear roadmap for enhancing capabilities.\n*   **Major Contenders:** Have a less balanced portfolio, often specializing in specific areas, with limited solutions for high-growth areas. They leverage cross-industry resources and focus on the mid-tier client segment.\n*   **Aspirants:** Have a restricted focus, limited digital service capabilities and partnership ecosystem, and concentrate on capturing market share in the small and midsize buyer segment.\n------------------------------\nQuery: What are some of the solutions that can assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality?\nResponse: CognitiveSpark for manufacturing is a cloud-based, AI-powered solution designed to assist biopharma and MedTech manufacturers with insights that help optimize manufacturing processes and improve product quality.\n------------------------------\nQuery: How are different areas of a manufacturing line benefiting from AI?\nResponse: AI is benefiting different areas of manufacturing lines through applications like digital twins and predictive maintenance. It also provides better visibility into supply chains and offers platforms for a comprehensive view of manufacturing operations with connected data, dynamic data visualizations, predictive analytics, and quality analytics.\n------------------------------\nQuery: What are ConocoPhillips' key financial priorities for the next 10 years?\nResponse: ConocoPhillips' key financial priorities for the next 10 years include: peer-leading ROCE improvement, top quartile ordinary dividend growth, distribution of over 90% of market cap, a ~$35/BBL WTI FCF breakeven, ~6% CFO CAGR, ~11% FCF CAGR, and remaining unhedged for price upside.\n------------------------------\nQuery: How does ConocoPhillips plan to meet it's Net-Zero and Emissions targets?\nResponse: ConocoPhillips plans to meet its Net-Zero ambition by 2050. They are accelerating their 2030 GHG emissions intensity reduction target to 50-60% and have a near-zero methane intensity target of &lt;1.5 kg CO2e/BOE. They also aim for zero routine flaring by 2025.\n------------------------------\nQuery: What is ConocoPhillips strategy to grow production?\nResponse: ConocoPhillips' strategy to grow production involves capital-efficient growth supported by a substantial, low-cost resource base and existing infrastructure. They aim for a 4% compound annual growth rate (CAGR) over the next decade, with a reinvestment rate of about 40%, assuming a WTI crude oil price of $60 per barrel.\n------------------------------\nQuery: What are the key features and capabilities of the XC9500 In-System Programmable CPLD family?\nResponse: The key features and capabilities of the XC9500 In-System Programmable CPLD family include: high performance with 5 ns pin-to-pin delays and 125 MHz fCNT, densities ranging from 36 to 288 macrocells (800 to 6,400 gates), 5V in-system programmability with 10,000 program/erase cycles, enhanced pin-locking architecture, flexible Function Blocks, global and product term clocks, extensive JTAG support, programmable power reduction, slew rate control, user-programmable ground pins, extended pattern security, high-drive 24 mA outputs, 3.3V/5V I/O capability, advanced CMOS FastFLASH technology, support for parallel programming, expanded JTAG, and configurable I/Os.\n------------------------------\nQuery: How does the Fast CONNECT switch matrix enable flexible signal routing and logic implementation within the XC9500 devices?\nResponse: The Fast CONNECT switch matrix enables flexible signal routing by allowing any I/O Block (IOB) output (corresponding to user pin inputs) and all Function Block (FB) outputs to drive the matrix. User programming can select up to 36 of these signals to drive each FB with a uniform delay. It also increases logic implementation flexibility by combining multiple internal connections into a single wired-AND output before driving the destination FB, increasing the effective logic fan-in of the destination FB without additional timing delay.\n------------------------------\nQuery: What design security options are available in the XC9500 family, and how do they protect programming data?\nResponse: The XC9500 family offers advanced data security features to protect programming data. It provides four security settings, including read security bits to prevent internal programming pattern reading or copying (while still allowing device erasure) and write security bits for added protection against accidental device erasure or reprogramming, which can be deactivated for reprogramming with a valid pattern.\n------------------------------\nQuery: What strategies are companies implementing to address tax transformation in a data-driven world?\nResponse: Companies are implementing strategies such as ensuring immediate access to reliable and accurate tax data, customizing ERP systems, involving internal stakeholders, securing budgets, creating maintenance schedules, developing technology roadmaps, and embedding tax into everyday processes to address tax transformation in a data-driven world.\n------------------------------\nQuery: How can tax departments effectively invest in technology to meet compliance and strategic goals?\nResponse: To effectively invest in technology for tax compliance and strategic goals, tax departments should secure a budget and create a roadmap that leverages existing technology and identifies future needs. They should also consider long-term potential requirements, choose an appropriate implementation approach (internal resources, partner, or outsourcing), and develop professional teams with data management and technology expertise.\n------------------------------\nQuery: How can tax departments collaborate internally to secure budget and ensure technology alignment?\nResponse: Tax departments can collaborate internally to secure budget and ensure technology alignment by demonstrating their value to the company, explaining the impact of tax authority digitalization to the C-suite, and collaborating with the IT department. Creating a roadmap to utilize available technology, identifying future needs, and deciding whether to build or buy, all while maintaining a holistic view of the tax department's operating model are best practices. Companies adopting a top-down approach have seen more success in building this value case.\n\n\nSource: ReActAgent: Querying Multiple PDFs Example\nLet’s analyze the output of the ReAct Agent for a specific query: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n&gt; Running step 0dc45634-70c7-4656-9b12-fc2b086848ce. Step input: What are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'}\n&gt; Running step b574597e-591e-42f5-b2fe-075603369f1c. Step input: vulnerabilities of APIs in Banking as a Service (BaaS)\nThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: vector_tool_deloitte-tech-risk-sector-banking\nAction Input: {'input': 'API vulnerabilities in Banking as a Service (BaaS)'}\nObservation: BaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n\n&gt; Running step ab6397c0-d075-499f-8d74-ce5ef25de495. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\nObservation: Banking as a Service (BaaS) relies heavily on APIs, which introduces vulnerabilities. These vulnerabilities can lead to risks for banks, including customer data and funds being at risk from phishing and social engineering, flawed APIs providing a backdoor for hackers, and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\n&gt; Running step 8b2c762a-4a21-47ad-81f9-5dba4bb5e0f3. Step input: None\nThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: Relying on application programming interfaces (APIs) in Banking as a Service (BaaS) introduces vulnerabilities that can put customer data and funds at risk from phishing and social engineering. Flawed APIs can also provide a backdoor for hackers. Furthermore, noncompliance with data privacy rules by BaaS providers can expose partner banks to reputational risks.\nHere’s a step-by-step explanation:\nWhat are the vulnerabilities introduced by relying on application programming interfaces (APIs) in Banking as a Service (BaaS)?\n\nInitial Thought: The agent recognizes the need for a tool to answer the question.\nAction (tool selection): The agent selects the tool_deloitte-tech-risk-sector-banking. This indicates that the agent has identified the appropriate source document for answering the question. It has arrived at this decision by retrieving tools based on the tools metadata.\nAction Input: The agent provides {'input': 'vulnerabilities of APIs in Banking as a Service (BaaS)'} as input to the chosen tool. This represents the refined query or search terms to be used within the selected document.\nSecondary Thought: The agent reiterates the need for a tool: either querying the Vector Index or the Summary Index.\nSecondary Action: The agent selects vector_tool_deloitte-tech-risk-sector-banking. This is the vector query engine associated with the deloitte document, meaning it’s going to use semantic search on the document’s content.\nObservation: The agent receives the following observation from the vector_tool_deloitte-tech-risk-sector-banking:\nBaaS increasingly relies on application programming interfaces, introducing vulnerabilities that can pose risks for banks, such as customers' sensitive data and funds being at risk from phishing and social engineering attacks, flawed APIs providing a back door for hackers to penetrate banks' systems and noncompliance with data privacy rules by BaaS providers exposing partner banks to reputational risks.\nThought: The agent decides it has enough information to answer the question without further tool usage.\nAnswer: The agent provides a concise answer in English, summarizing the vulnerabilities: customer data/funds at risk, backdoor for hackers, and noncompliance with data privacy rules.\nObservation & Answer Redux: The agent repeats its reasoning and repeats the final answer\n\nKey Observations:\n\nReAct Loop: The agent follows the ReAct loop (Reason, Act, Observe). It reasons about the question, selects a tool (Action), observes the tool’s output (Observation), and then decides on the next step.\nTool Selection: The agent correctly identifies and uses the appropriate tool (document) for each question. This demonstrates the agent’s ability to choose the right knowledge source.\nSemantic Search: The use of vector_tool_deloitte-tech-risk-sector-banking indicates the agent is leveraging semantic search to retrieve relevant information from the document’s vector index.\nConcise Answers: The agent provides concise and relevant answers based on the retrieved information.\nRedundancy: There’s a bit of redundancy in the steps, particularly the repeated “Thought” and “Observation” before the “Answer.” This is due to the specific implementation of the ReAct loop in LlamaIndex.\nSingle Document Usage: The agent doesn’t seem to be using the “compare_tool” or querying multiple documents simultaneously in these examples. All the information is coming from the Deloitte Tech Risk Sector Banking document.\n\nFeel free to use this notebook as a starting point for building your own multi-document query system using ReAct agents. You can adapt the code to work with your own PDF documents and queries, and explore the capabilities of the ReAct framework for document analysis and retrieval."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Why I Write\nWelcome to my blog, a personal space where I share notes on my work and explorations in science, machine learning, and the broader tech landscape. These posts serve as a journal for my thoughts and discoveries, but I hope they also provide value and inspiration to anyone interested in these dynamic fields. Whether you’re a fellow researcher, a tech enthusiast, or simply curious, I invite you to explore and engage with the ideas I present here. Feel free to reach out if you’d like to connect or collaborate on exciting projects !\nShort Bio\nI am a Data Scientist with expertise in applied mathematics and artificial intelligence. My professional experience spans diverse domains, including signal processing and anomaly detection at Thales, as well as drug discovery at Iktos. Throughout my career, I have been deeply involved in research and development, driven by a passion for innovation and the challenge of addressing complex problems."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html",
    "href": "posts/pdf-parsing/pdf-parsing.html",
    "title": "PDF Parsing for LLM Input",
    "section": "",
    "text": "This blog post explores the current landscape of PDF parsing for use as input to Large Language Models (LLMs). Extracting meaningful information from PDFs can be challenging due to their complex structure. This article examines several approaches, their strengths, and limitations, with a focus on their suitability for LLM integration (markdown output).\nWe begin with a detailed presentation of some open-source PDF parsing libraries: Docling, Marker-PDF, and MinerU in Section 1. In particular, we provide a comprehensive overview of the Docling pipeline, a modular and open-source PDF processing pipeline designed to transform PDFs into a structured representation (the DoclingDocument) in Section 1.1. We also discuss Marker-PDF and MinerU in Section 1.2 and Section 1.3. The objective is to gain an understanding of how modern PDF parsing libraries function, in particular the role of deep learning models in PDF parsing and extraction, focusing on layout analysis, table structure recognition, and Optical Character Recognition (OCR).\nFollowing this, we compare and evaluate various PDF parsing libraries and tools in Section 2, including open-source libraries (Docling, Marker, MinerU, PyMuPDF) and closed-source solutions (LlamaParse, Gemini, Mistral). We provide a detailed comparison based on a qualitative analysis using a diverse set of test PDFs (slides, reports, scanned documents, and documents with complex tables). The results of this analysis are available through an interactive demo that visualizes the differences between PDF input and Markdown output for each parsing solution."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-docling",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-docling",
    "title": "PDF Parsing for LLM Input",
    "section": "Docling: A Modular PDF Processing Pipeline",
    "text": "Docling: A Modular PDF Processing Pipeline\nOverview\nDocling is a modular and extensible pipeline designed to ingest various document formats, primarily PDFs (but also DOCX, HTML, etc.), and transform them into a unified, structured representation: the DoclingDocument. The core aim is to create a standardized representation suitable for downstream tasks, such as feeding data into an LLM. Docling is in active development and is completely open-source. It provides a comprehensive technical documentation (Auer et al. 2024), (Livathinos et al. 2025).\n\n\n\n\n\n\nFigure 1: Sketch of Docling’s default processing pipeline. Source: (Auer et al. 2024)\n\n\n\nThe Docling pipeline, as shown in Figure 1, consists of the following stages:\n\nPDF Backend (for raw PDF document processing)\nAI Models (Layout Analysis, Table Structure Recognition, OCR)\nAssembly and Post-processing\n\nPDF Backend\nThe PDF backend is responsible for:\n\nRetrieving all text content and their geometric coordinates on each page.\nRendering the visual representation of each page as it appears in a PDF viewer.\n\nThese capabilities are encapsulated in Docling’s PDF backend interface. Docling provides multiple backend choices, including a custom-built PDF parser based on the low-level qpdf library. This parser is open-sourced as a separate package, docling-parse, and powers Docling’s default PDF backend. Figure 2 illustrates the parsing process.\n\n\n\n\n\n\n\n\nOriginal PDF page\n\n\n\n\n\n\n\nParsed PDF page\n\n\n\n\n\n\nFigure 2: Illustration of docling parse from source.\n\n\n\nThe docling-parse package provides a Python interface to extract text content, images, and annotations from PDFs. It also supports rendering PDF pages as images. The extracted content is serialized to JSON Listing 1, which can be further processed by downstream components.\n\n\n\nListing 1: Example of docling-parse JSON output\n\n\n{'annotations': [{'/A': {'/IsMap': False,\n    '/S': '/URI',\n    '/URI': 'https://www.deloitte.com/global/en/Industries/financial-services/perspectives/pushing-through-undercurrents.html'},\n   '/BS': {'/S': '/S', '/Type': '/Border', '/W': 0},\n   '/Border': [0, 0, 0],\n   '/H': '/N',\n   '/Rect': [474.409, 580.322, 512.947, 569.083],\n   '/Subtype': '/Link',\n   '/Type': '/Annot'},\n  {'/A': {'/IsMap': False,\n    '/S': '/URI',\n    '/URI': 'https://www.deloitte.com/global/en/Industries/financial-services/perspectives/pushing-through-undercurrents.html'},\n   '/BS': {'/S': '/S', '/Type': '/Border', '/W': 0},\n   '/Border': [0, 0, 0],\n   '/H': '/N',\n   '/Rect': [67.9417, 568.322, 286.919, 557.22],\n   '/Subtype': '/Link',\n   '/Type': '/Annot'}],\n 'original': {'cells': {'data': [[36.142,\n     711.041,\n     54.862,\n     739.753,\n     36.142,\n     711.041,\n     54.862,\n     711.041,\n     54.862,\n     739.753,\n     36.142,\n     739.753,\n     'P',\n     -1,\n     8.32,\n     '/WinAnsiEncoding',\n     'WINANSI',\n     '/TT0',\n     '/FSUTKX+OpenSans-Light',\n     False,\n     True],\n    [54.542,\n     711.041,\n     73.422,\n     739.753,\n     54.542,\n     711.041,\n     73.422,\n     711.041,\n     73.422,\n     739.753,\n     54.542,\n     739.753,\n     'u',\n     -1,\n\n\n\nAI Models\nDocling integrates several AI models for layout analysis and table structure recognition (TableFormer (Nassar et al. 2022)). Pre-trained weights (hosted on Hugging Face) and a separate package for inference code (docling-ibm-models) are available.\nLayout Analysis Model\nThis model detects and classifies various elements on a page image by predicting bounding boxes. The architecture is based on RT-DETR and retrained on DocLayNet (Pfitzmann et al. 2022) and proprietary datasets. The Docling pipeline uses page images at 72 dpi resolution. Bounding box proposals are post-processed to remove overlaps based on confidence and size and then intersected with text tokens to group them into meaningful units (e.g., paragraphs, section titles, tables).\nRT-DETR (Real-Time DEtection TRansformer) is an object detection system using a hybrid encoder to process image features and IoU-aware query selection to focus on important parts of the image.\nTable Structure Recognition\n\n\n\n\n\n\nFigure 3: TableFormer architecture. Source: DS4SD/docling-ibm-models\n\n\n\nThe TableFormer model (a vision transformer) recovers table structure Figure 3. It predicts the logical row and column structure of a table based on an input image, determining which cells belong to column headers, row headers, or the table body. TableFormer handles tables with partial or no borderlines, empty cells, row or column spans, and other complexities.\nThe Docling pipeline feeds table objects detected in the layout analysis to the TableFormer model. TableFormer structure predictions are matched back to the PDF cells to avoid re-transcription of text in the table image.\nOCR (Optical Character Recognition)\nDocling optionally supports OCR for scanned PDFs or content in embedded bitmaps. Docling supports multiple OCR engines such as EasyOCR, Tesseract, RapidOCR, and OcrMac. By default, Docling feeds a high-resolution page image (216 dpi) to the OCR engine to capture small print details.\nAssembly and Post-processing\nIn the final stage, Docling assembles all prediction results into a DoclingDocument, defined in the docling-core package. This document object is then passed through a post-processing model that augments features, such as:\n\nDocument language detection\nReading order correction\nMatching figures with captions\nLabeling metadata (title, authors, references)\n\nThe final output can be serialized to JSON or transformed into Markdown.\nAdditional post-processing steps can include:\n\nClassification of figures.\nIdentification of code blocks or formulas.\nAnnotation of pictures with LLMs (example).\n\nThe DoclingDocument is a unified representation designed to encapsulate document structure and content in a standardized way. It’s a Pydantic datatype supporting text, tables, pictures, and more. It distinguishes between the main body and auxiliary elements (“furniture”). It retains layout information (bounding boxes) and provenance information. The DoclingDocument structure is organized into content items (texts, tables, pictures) and content structure (body, furniture, groups) Figure 4.\n\n\n\n\n\n\nFigure 4: DoclingDocument structure. Source: DS4SD/docling documentation"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-marker-pdf",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-marker-pdf",
    "title": "PDF Parsing for LLM Input",
    "section": "Marker-PDF: Accurate PDF Conversion",
    "text": "Marker-PDF: Accurate PDF Conversion\nMarker-pdf pipeline is another interesting option for PDF parsing. Raw extraction is done by pdftext, which is based on pypdfium2. Then, it uses Surya Surya GitHub. Surya is a document OCR toolkit that performs:\n\nOCR in 90+ languages.\nLine-level text detection in any language.\nLayout analysis (table, image, header, etc. detection).\nReading order detection.\nTable recognition (detecting rows/columns).\nLaTeX OCR.\n\nThe JSON output format from Surya is illustrated in Listing 2. This structured output includes detailed information about page dimensions, blocks of text, and individual spans with their corresponding bounding boxes, font information, and text content. Marker itself converts PDFs and images to markdown, JSON, and HTML. The key features of Marker include:\n\nSupports a range of documents in all languages.\nFormats tables, forms, equations, links, references, and code blocks.\nExtracts and saves images along with the markdown.\nRemoves headers/footers/other artifacts.\nEasily extensible with custom formatting and logic.\nOptionally boosts accuracy with an LLM.\nWorks on GPU, CPU, or MPS.\n\n\n\n\nListing 2: Example of Surya JSON output\n\n\n[\n  {\n    \"page\": 0,\n    \"bbox\": [\n      0,\n      0,\n      595.2760009765625,\n      841.8900146484375\n    ],\n    \"width\": 596,\n    \"height\": 842,\n    \"rotation\": 0,\n    \"blocks\": [\n      {\n        \"lines\": [\n          {\n            \"spans\": [\n              {\n                \"bbox\": [\n                  36.14179992675781,\n                  99.6307373046875,\n                  481.22967529296875,\n                  131.6307373046875\n                ],\n                \"text\": \"Pushing through undercurrents\",\n                \"rotation\": 0,\n                \"font\": {\n                  \"name\": \"OpenSans-Light\",\n                  \"flags\": 524320,\n                  \"size\": 1,\n                  \"weight\": 240\n                },\n                \"char_start_idx\": 0,\n                \"char_end_idx\": 28,\n                \"url\": \"\"\n              },\n              {\n                \"bbox\": [\n                  466.78369140625,\n                  125.09466552734375,\n                  466.78369140625,\n                  125.09466552734375\n                ],\n                \"text\": \"\\n\",\n                \"rotation\": 0,\n                \"font\": {\n                  \"name\": \"\",\n                  \"flags\": 0,\n                  \"size\": 1,\n                  \"weight\": -1\n                },\n                \"char_start_idx\": 29,\n                \"char_end_idx\": 30,\n                \"url\": \"\"\n              }\n            ],\n            \"bbox\": [\n              36.14179992675781,\n              99.6307373046875,\n              481.22967529296875,\n              131.6307373046875\n            ]\n          }\n        ],\n..."
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#sec-mineru",
    "href": "posts/pdf-parsing/pdf-parsing.html#sec-mineru",
    "title": "PDF Parsing for LLM Input",
    "section": "MinerU: Multi-Module Document Parsing",
    "text": "MinerU: Multi-Module Document Parsing\n\n\n\n\n\n\nFigure 5: Overview of the MinerU framework processing workflow.\n\n\n\nMinerU is a multi-module document parsing framework that uses a multi-stage approach, employing various document parsing models to process document images. The code repository is available at the MinerU GitHub Repository, and technical details can be found in the reference paper (Wang et al. 2024). MinerU provides source code, models, and documentation for parsing various document formats efficiently.\nThe MinerU framework processing workflow (Figure 5) consists of four stages:\n\nDocument Preprocessing: Uses PyMuPDF to read PDF files, filters out unprocessable files, and extracts PDF metadata (parseability, language type, page dimensions).\nDocument Content Parsing: Employs the PDF-Extract-Kit library for layout analysis (layout and formula detection). Applies different recognizers to various regions: OCR for text and titles, formula recognition for formulas, and table recognition for tables.\nDocument Content Post-Processing: Removes invalid regions, stitches content according to regional positioning, and obtains positioning, content, and sorting information for different document regions.\nFormat Conversion: Generates user-required formats, such as Markdown, for subsequent use.\n\nDocument Preprocessing\nThis stage focuses on filtering unprocessable PDFs and obtaining PDF metadata:\n\nLanguage Identification: Currently processes Chinese and English documents.\nContent Garbled Detection: Identifies text-based PDFs with garbled text.\nScanned PDF Identification: Distinguishes between text-based and scanned PDFs.\nPage Metadata Extraction: Extracts document metadata such as total page count, page dimensions, and other attributes.\n\nDocument Content Parsing\nMinerU uses the PDF-Extract-Kit model library to detect different types of regions and recognize their content:\n\nLayout Analysis: Identifies different types of elements and their regions on a page.\nFormula Detection: Detects inline and displayed formulas.\nFormula Recognition: Recognizes formula images into LaTeX source code using the UniMERNet model.\nTable Recognition: Extracts tabular data from visual table images using TableMaster and StructEqTable.\nOCR: Applies Paddle-OCR to recognize text regions.\n\nDocument Content Post-Processing\nThis stage addresses content ordering by handling the relationships between Bounding Boxes (BBoxes):\n\nContainment Relationships: Removes formulas and text blocks contained within image and table regions.\nPartial Overlap Relationships: Shrinks partially overlapping text boxes and ensures the integrity of text when overlapping with tables/images.\nSegmentation Algorithm: Divides the page into regions based on human reading order (“top to bottom, left to right”).\n\nModels Overview\n\n\n\nTable 1: MinerU models overview\n\n\n\n\n\n\n\n\n\n\nTask Type\nDescription\nModels\n\n\n\n\nLayout Detection\nLocate different elements in a document: including images, tables, text, titles, formulas\nDocLayout-YOLO_ft, YOLO-v10_ft, LayoutLMv3_ft\n\n\nFormula Detection\nLocate formulas in documents: including inline and block formulas\nYOLOv8_ft\n\n\nFormula Recognition\nRecognize formula images into LaTeX source code\nUniMERNet\n\n\nOCR\nExtract text content from images (including location and recognition)\nPaddleOCR\n\n\nTable Recognition\nRecognize table images into corresponding source code (LaTeX/HTML/Markdown)\nPaddleOCR+TableMaster, StructEqTable"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#pdf-sample-dataset",
    "href": "posts/pdf-parsing/pdf-parsing.html#pdf-sample-dataset",
    "title": "PDF Parsing for LLM Input",
    "section": "PDF Sample Dataset",
    "text": "PDF Sample Dataset\nWe test the different PDF parsing options on a small but diverse dataset of PDFs. These PDFs are available at the following link: PDF Parsing Dataset. The dataset contains different types of PDFs to cover various difficulties faced by PDF parsers:\n\nSlides\nImage-only PDFs (requiring OCR)\nReports\nTables\n\nThe PDF files located in the pdfs directory were sourced from the following locations:\n\nXC9500_CPLD_Family-1-4.pdf: Downloaded from https://media.digikey.com/pdf/Data%20Sheets/AMD/XC9500_CPLD_Family.pdf\n2023-conocophillips-aim-presentation-1-7.pdf: Downloaded from https://static.conocophillips.com/files/2023-conocophillips-aim-presentation.pdf\n\nThe following four PDF files are sourced from the RAG blog benchmark, specifically from the associated Google Drive folder:\n\ngx-iif-open-data.pdf\ndeloitte-tech-risk-sector-banking.pdf\nlife-sciences-smart-manufacturing-services-peak-matrix-assessment-2023.pdf\ndttl-tax-technology-report-2023.pdf"
  },
  {
    "objectID": "posts/pdf-parsing/pdf-parsing.html#qualitative-results",
    "href": "posts/pdf-parsing/pdf-parsing.html#qualitative-results",
    "title": "PDF Parsing for LLM Input",
    "section": "Qualitative Results",
    "text": "Qualitative Results\nNote: The following results are based on a limited set of tests and should be considered indicative rather than scientifically rigorous.\n\nYou can directly compare the output Markdown results at the Hugging Face Space demo: pdf-parsing-demo.\n\nDocling provides decent results for text extraction, layout analysis, and table recognition. The OCR support is also a valuable addition for scanned PDFs. Very recently, it provides support for image description example, which is a very promising feature.\n\n\n\n\n\n\nFigure 6: Docling parsing results\n\n\n\nLlamaParse in default mode also provides good results. It provides descriptions of charts and images and is able to extract tables. However, the quality of the chart and image descriptions is not top-tier. It is also a closed-source solution.\n\n\n\n\n\n\nFigure 7: LlamaParse parsing results\n\n\n\nMarker provides good results and a description of the images using Gemini.\n\n\n\n\n\n\nFigure 8: Marker parsing results\n\n\n\nPyMuPDF in default mode is a more raw extraction approach, parsing the raw content. The other libraries tend to build upon more fundamental extraction libraries such as PyMuPDF. The results generally require further refinement.\n\n\n\n\n\n\nFigure 9: Pymupdf parsing results\n\n\n\nGemini is very good at parsing the content and providing a description of the images. It is versatile and effective. However, it is closed source.\n\n\n\n\n\n\nFigure 10: Gemini parsing results\n\n\n\n\n\n\n\n\n\nFigure 11: Gemini parsing results (suppl.)\n\n\n\nMistral OCR is a closed-source solution that provides good results. It is particularly effective at extracting text and tables. However, it does not support image description by default.\n\n\n\n\n\n\nFigure 12: Mistral parsing results\n\n\n\nMinerU is not part of the app because it is not easy to split the output by page, and tables are under HTML format. However, the results are good, and the layout is very good. It doesn’t support image description. MinerU is a very promising library.\n\n\n\n\n\n\nFigure 13: MinerU parsing results\n\n\n\nWe tested unstructured (free version), which did not produce satisfying results.\nMarkitdown is a very new library that we did not test. It seems to rely on ad-hoc solutions and is simply an aggregator of solutions.\nWe also tested (not extensively) some cloud-based solutions for PDF parsing such as Adobe PDF Extract API, AWS Textract and Azure AI Document intelligence but we were not convinced by the first results. Apparently, they seem to be very good at extracting for large quantity of pdf following the same template.\nIn terms of table extraction capabilities, we tested Camelot, a specialized library specifically designed for extracting tables from PDFs. Camelot excels at handling both lattice and stream type tables, making it particularly effective for complex table extraction tasks. The library is well-documented and maintained, with comprehensive documentation available on their GitHub repository. The results were good for table extraction. Camelot is a dedicated library for tables.\nConclusion: The choice of PDF parsing library depends on the specific requirements of the task at hand. or general-purpose parsing with a strong emphasis on image understanding, consider Gemini. However, be aware that this is a closed-source solution. If an open-source solution is preferred, Docling, Marker and MinerU are strong contenders. For tasks requiring specialized table extraction, Camelot is a reliable choice."
  },
  {
    "objectID": "posts/mhc/mhc.html",
    "href": "posts/mhc/mhc.html",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "",
    "text": "This blog post explores the application of machine learning techniques to predict the binding affinity between peptides and Major Histocompatibility Complex (MHC) molecules. In Section 1, we introduce the biological aspects of MHC. In Section 2, we then present the deep learning approach and the results of MHC antigen presentation."
  },
  {
    "objectID": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "href": "posts/mhc/mhc.html#mhc-class-i-and-class-ii",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "MHC Class I and Class II",
    "text": "MHC Class I and Class II\nMHC molecules are divided into two main classes: MHC class I and MHC class II.\nMHC Class I\nMHC class I molecules are present on most cells (except red blood cells). They present antigens from inside the cell. When a cell is infected or becomes cancerous, proteins within the cell are broken down into smaller fragments called epitopes. These epitopes are loaded onto MHC class I molecules and displayed on the cell surface. Killer T cells (cytotoxic T lymphocytes or CTLs) recognize these epitopes and can destroy infected or cancerous cells.\n\n\n\n\n\n\nFigure 1: MHC class I pathway: Proteins in the cytosol are degraded by the proteasome, liberating peptides internalized by TAP channel in the endoplasmic reticulum, there associating with MHC-I molecules freshly synthesized. MHC-I/peptide complexes enter Golgi apparatus, are glycosylated, enter secretory vesicles, fuse with the cell membrane, and externalize on the cell membrane interacting with T lymphocytes. This figure shows the detailed steps of how MHC Class I molecules get loaded. Source\n\n\n\nIn humans, the main MHC class I molecules are HLA-A, HLA-B, and HLA-C (HLA stands for Human Leukocyte Antigen).\nMHC Class II\nMHC class II molecules are primarily found on specialized immune cells called antigen-presenting cells (APCs) like macrophages, dendritic cells, and B cells. They present antigens from outside the cell. APCs engulf foreign invaders and break them down into epitopes in a process called phagocytosis. These epitopes are loaded onto MHC class II molecules and displayed on the cell surface. Helper T cells recognize these epitopes and activate other immune cells to fight the infection. These Helper T cells have receptors that specifically bind to MHC Class II molecules. If a helper T cell recognizes a foreign epitope presented by MHC class II, it becomes activated and starts to coordinate the immune response. It releases chemical signals (cytokines) that help other immune cells, like B cells and killer T cells, to fight off the infection.\n\n\n\n\n\n\nFigure 2: This diagram shows how MHC class I molecules present antigens from inside the cell to cytotoxic T cells (CD8+), leading to the destruction of infected cells. Notice how the antigen is processed inside the cell and then presented on the cell surface. On the contrary, MHC class II molecules present antigens from outside the cell to helper T cells (CD4+), which then activate other immune cells. Notice how this differs from MHC class I, which presents antigens from inside the cell.(Antunes et al. 2018)\n\n\n\n\n\n\n\n\n\nFigure 3: Molecular structures of class I and class II MHCs. Molecular representation of a class I MHC (A, C) and a class II MHC (B, D). The upper panel shows a top view, while the bottom panel shows a cross section side-view of the binding clefts. Note that the binding cleft of a class I receptor is deeper, with “closed” extremities, while the class II cleft is shallower, with open extremities. The pockets involved in binding primary “anchor” residues are indicated. Together, structural differences in the shape of the cleft and the location of binding pockets have an impact on the overall conformation of bound ligands (e.g., peptides tend to adopt bulged conformations when bound to class I, and more linear conformations when bound to class II). (Antunes et al. 2018)\n\n\n\nIn humans, the main MHC class II molecules are HLA-DP, HLA-DQ, and HLA-DR.\nHuman Leukocyte Antigens (HLA)\nIn humans, MHC molecules are called Human Leukocyte Antigens (HLAs). The genes for HLA proteins are located within the MHC region on chromosome 6.\nHLAs are important for:\n\nOrgan transplantation: HLA matching is crucial to prevent organ rejection.\nAutoimmune diseases: Certain HLA types are associated with increased risk of autoimmune diseases.\nDrug responses: HLA variations can influence how individuals respond to certain medications.\nEvolutionary advantage: HLA diversity is important for population survival against various pathogens.\n\n\n\n\n\n\n\nFigure 4: Codominant expression of HLA genes. Each person inherits HLA genes from both parents, resulting in the expression of multiple HLA types. This increases the diversity of antigens that can be presented. Source\n\n\n\nMHC Diversity\nThe MHC is highly diverse, with many different versions (alleles) of each MHC gene. This diversity is essential because different MHC molecules bind to different peptides. A diverse population is more likely to have individuals who can present antigens from new pathogens, ensuring better survival chances for the species."
  },
  {
    "objectID": "posts/mhc/mhc.html#datasets",
    "href": "posts/mhc/mhc.html#datasets",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Datasets",
    "text": "Datasets\nThe IPD-IMGT/HLA Database is a specialized resource focusing on the sequences of the human major histocompatibility complex (MHC) or human leukocyte antigen (HLA) system. It provides comprehensive information about HLA alleles, including their sequences, nomenclature, and associated metadata. This database is crucial for researchers in immunology, transplantation, and vaccine development, as accurate HLA typing is essential for understanding immune responses and predicting transplant compatibility. The IEDB (Immune Epitope Database) is a widely used resource for curated experimental data on immune epitopes. It catalogs epitopes recognized by T cells and B cells in various diseases and conditions. The IEDB facilitates research in epitope discovery, vaccine design, and understanding immune recognition, offering tools and data for analyzing and predicting immune responses."
  },
  {
    "objectID": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "href": "posts/mhc/mhc.html#relevance-to-machine-learning",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Relevance to Machine Learning",
    "text": "Relevance to Machine Learning\nThe interaction between an MHC molecule and a peptide is highly dependent on the amino acid sequence of the peptide and the specific type of MHC molecule. This sequence-structure-function relationship makes MHC-peptide binding prediction a suitable problem for machine learning. Experimental data on MHC-peptide binding affinities, though sometimes sparse, is available for training and evaluating predictive models. The high polymorphism of MHC genes, with numerous variants (alleles) existing in the population, adds complexity and motivates the development of specific prediction models.\nKey MHC Concepts:\n\nMHC Class I and Class II: The two main classes of MHC molecules.\nEpitope: The specific part of the peptide recognized by the T cell receptor.\nPolymorphism: The existence of multiple versions (alleles) of MHC genes within a population. Different MHC alleles bind to different sets of peptides.\nBinding Affinity: The strength of the interaction between an MHC molecule and a peptide. Often measured experimentally, it serves as the target variable for many machine learning models.\n\nMachine Learning Approaches\nMHC-peptide binding prediction aims to develop models that accurately predict the binding affinity between a given peptide sequence and a specific MHC allele. This can be framed as a regression or classification task.\n\nInput: Peptide sequence, MHC allele (represented as a sequence or encoding).\nOutput: Binding affinity (e.g., IC50 value, Kd value) or a binary label (binder/non-binder).\n\nFeature engineering and model selection are crucial for building effective predictors. Common approaches include:\n\nSequence-based Features: Amino acid composition, n-grams, physicochemical properties.\nStructure-based Features: (If available) Information about the 3D structure of the MHC-peptide complex.\nMHC Allele Encoding: Techniques such as one-hot encoding, amino acid embeddings, or other methods to represent the MHC allele sequence.\nMachine Learning Algorithms: Linear regression, Support Vector Machines (SVMs), Random Forests, Neural Networks (including Convolutional Neural Networks and Transformers)."
  },
  {
    "objectID": "posts/mhc/mhc.html#project-overview",
    "href": "posts/mhc/mhc.html#project-overview",
    "title": "Predicting MHC-Peptide Binding with Machine Learning",
    "section": "Project Overview",
    "text": "Project Overview\n\n\n\n\n\n\nNote\n\n\n\nThe code is available at https://github.com/nbrosse/mhcpred.\n\n\nThe goal of this project is to build a machine learning classifier that predicts whether a given peptide will be presented by a specific MHC class I protein, identified by its allele name. The data used for this project is derived from the training and evaluation data of NetMHCPan4.1 (Reynisson et al. 2020), a well-established framework for MHC binding prediction. The data is split into training and testing sets, with the training data further divided into five folds for cross-validation.\nThe dataset contains a binary target variable, “hit” (1 if the peptide is presented by the MHC, 0 otherwise), and two features:\n\n“peptide”: The amino acid sequence of the peptide. These short chains of amino acids are potential antigens that could be presented to the immune system.\n“allele”: The name of the MHC class I allele. MHC molecules are highly polymorphic, meaning there are many different versions (alleles) within the human population. Each allele has a slightly different binding groove, affecting which peptides it can bind and present. You can find details on the naming convention here (nomenclature).\n\n\n\n\n\n\n\nNote\n\n\n\nPredicting MHC antigen presentation is a complex field. This project provides a simplified introduction to the problem. For a more in-depth understanding, we recommend exploring NetMHCPan (Reynisson et al. 2020) and MHCflurry (O’Donnell, Rubinsteyn, and Laserson 2020) and the references cited within those publications. Note that the specific data used in this project is derived from NetMHCPan4.1 but must remain private.\n\n\nWe begin with Exploratory Data Analysis (EDA) to understand the characteristics of our data.\n\nEDA\n\n# Import data loading functions\nfrom mhcpred.data import get_train_data, get_test_data\n\n# Load training and test data\ndf_train = get_train_data()\ndf_test = get_test_data()\n\n\n# View first few rows of training data\ndf_train.head()\n\n\n\n\n\n\n\n\npeptide\nallele\nhit\nfold\n\n\n\n\n0\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n0\n\n\n1\nKESKINQVF\nHLA-B*44:02\nTrue\n0\n\n\n2\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n0\n\n\n3\nRTIADSLINSF\nHLA-B*57:03\nTrue\n0\n\n\n4\nEEKTIIKKL\nHLA-B*44:03\nTrue\n0\n\n\n\n\n\n\n\n\n# Get allele counts in training data\ndf_train[[\"allele\"]].value_counts()\n\nallele     \nHLA-A*02:01    265252\nHLA-B*07:02    201038\nHLA-B*57:01    184773\nHLA-A*29:02    181136\nHLA-B*40:02    145817\n                ...  \nHLA-A*69:01        12\nHLA-A*02:06         6\nHLA-A*26:02         6\nHLA-A*26:03         6\nHLA-A*25:01         6\nName: count, Length: 130, dtype: int64\n\n\n\ndf_test[\"allele\"].value_counts()\n\nallele\nHLA-A*02:02    77053\nHLA-A*02:06    54510\nHLA-A*02:11    48445\nHLA-B*53:01    46991\nHLA-B*15:17    45917\nHLA-A*02:05    45136\nHLA-B*15:03    44968\nHLA-A*33:01    43333\nHLA-A*66:01    41538\nHLA-C*12:03    36448\nHLA-C*03:03    35568\nHLA-A*11:01    33424\nHLA-A*30:02    33180\nHLA-C*08:02    32416\nHLA-A*23:01    30467\nHLA-A*32:01    28036\nHLA-B*40:02    23768\nHLA-B*14:02    21601\nHLA-B*37:01    20048\nHLA-B*40:01    18908\nHLA-B*45:01    18750\nHLA-B*18:01    18284\nHLA-B*58:01    17946\nHLA-B*15:02    16702\nHLA-B*15:01    16624\nHLA-A*30:01    15837\nHLA-C*07:02    15293\nHLA-B*46:01    14015\nHLA-B*38:01     9509\nHLA-B*35:03     8275\nHLA-A*26:01     7730\nHLA-C*05:01     7033\nHLA-A*25:01     6906\nHLA-A*68:01     5648\nHLA-B*08:01     3365\nHLA-B*07:02     2469\nName: count, dtype: int64\n\n\n\n# Get positive samples per allele in training\ndf_train.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*01:01     7156\nHLA-A*01:03        7\nHLA-A*02:01    13025\nHLA-A*02:03     1873\nHLA-A*02:04     3155\n               ...  \nHLA-C*12:04        3\nHLA-C*14:02     2441\nHLA-C*15:02     1873\nHLA-C*16:01     2970\nHLA-C*17:01      602\nName: hit, Length: 130, dtype: int64\n\n\n\ndf_train.hit.sum()\n\n197547\n\n\n\nlen(df_train)\n\n3679405\n\n\n\n# ~5.37% positive rate\ndf_train.hit.sum() / len(df_train)\n\n0.05368993084479692\n\n\n\ndf_test.groupby(\"allele\").hit.sum()\n\nallele\nHLA-A*02:02    3063\nHLA-A*02:05    2016\nHLA-A*02:06    1975\nHLA-A*02:11    2035\nHLA-A*11:01    2309\nHLA-A*23:01    1697\nHLA-A*25:01     396\nHLA-A*26:01     555\nHLA-A*30:01     892\nHLA-A*30:02    2415\nHLA-A*32:01    1436\nHLA-A*33:01    2138\nHLA-A*66:01    1988\nHLA-A*68:01     433\nHLA-B*07:02     159\nHLA-B*08:01     180\nHLA-B*14:02    1056\nHLA-B*15:01     769\nHLA-B*15:02     637\nHLA-B*15:03    1953\nHLA-B*15:17    1712\nHLA-B*18:01     784\nHLA-B*35:03     330\nHLA-B*37:01    1253\nHLA-B*38:01     619\nHLA-B*40:01    1268\nHLA-B*40:02    1333\nHLA-B*45:01     760\nHLA-B*46:01     575\nHLA-B*53:01    2016\nHLA-B*58:01     866\nHLA-C*03:03    2003\nHLA-C*05:01     383\nHLA-C*07:02     593\nHLA-C*08:02    1546\nHLA-C*12:03    1273\nName: hit, dtype: int64\n\n\n\ndf_test.hit.sum() / len(df_test)\n\n0.04800130213150049\n\n\n\n# Find alleles only in test set\nset(df_test.allele.unique()) - set(df_train.allele.unique())\n\n{'HLA-A*02:02', 'HLA-A*02:11', 'HLA-A*33:01', 'HLA-B*53:01'}\n\n\n\nDataset Class Imbalance\n\nTraining Set:\n\nTotal samples: 3,679,405\nPositive rate: 5.37%\n\nTest Set:\n\nTotal samples: 453,934\nPositive rate: 4.8%\n\n\nAllele Distribution\n\nMost frequent: HLA-A*02:01 (265,252 samples)\nLeast frequent: Multiple alleles with only 6 samples\nDistribution: Highly imbalanced across alleles\n\nTest-Only Alleles\n\nHLA-A*02:02\nHLA-A*02:11\nHLA-A*33:01\nHLA-B*53:01\n\n\nSource: EDA\n\nUsing MHCflurry Pretrained Models for Prediction\nWe leverage the mhcflurry package to build our classifier. MHCflurry is a tool specifically designed for MHC binding affinity prediction. See also the associated paper (O’Donnell, Rubinsteyn, and Laserson 2020). MHCflurry is a software package focused on predicting how strongly peptides bind to MHC class I molecules. It’s based on machine learning models trained on a large dataset of experimentally measured peptide-MHC binding affinities. The current version uses neural networks trained with a mix of binding affinity and mass spectrometry data (ligand presentation).\nWe use the Binding Affinity pretrained model from mhcflurry to predict the binding affinity of peptides to MHC class I molecules using Class1AffinityPredictor. The following code assumes you have installed mhcflurry and downloaded the required pretrained models.\nmhcflurry-downloads fetch models_class1_presentation\npython scripts/mhcflurry_benchmark.py\ndef predict_with_mhcflurry() -&gt; pd.DataFrame:\n    predictor = Class1AffinityPredictor.load()\n    df_test = get_test_data()\n    mhcflurry_predictions = predictor.predict_to_dataframe(\n        peptides=df_test.peptide.values,\n        alleles=df_test.allele.values,\n        allele=None,\n    )\n    df = pd.merge(df_test, mhcflurry_predictions, on=[\"allele\", \"peptide\"], how=\"left\")\n    df.to_csv(str(output_path / \"mhcflurry_predictions.csv\"), index=False)\n    return df\nThe output is of the form:\n\n\n\nTable 1: MHCflurry pretrained model predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npeptide\nhit\nallele\nprediction\nprediction_low\nprediction_high\nprediction_percentile\n\n\n\n\nAAPATRAAL\nTrue\nHLA-B*35:03\n94.297\n59.902\n144.624\n0.205\n\n\nAAPSAAREL\nTrue\nHLA-B*35:03\n116.19\n79.847\n169.241\n0.262\n\n\nAEISQIHQSVTD\nTrue\nHLA-B*35:03\n26103.26\n22695.389\n28415\n15.739\n\n\nALEEQLQQIRAE\nTrue\nHLA-B*35:03\n24797.131\n19988.967\n28062.65\n13.571\n\n\nAQDPLLLQM\nTrue\nHLA-B*35:03\n2164.336\n745.888\n5390.727\n1.413\n\n\nASAPPGPPA\nTrue\nHLA-B*35:03\n1398.729\n387.675\n3293.692\n1.157\n\n\nDAHKGVAL\nTrue\nHLA-B*35:03\n84.315\n54.736\n133.899\n0.175\n\n\nDNPIQTVSL\nTrue\nHLA-B*35:03\n1386.767\n565.122\n3667.21\n1.151\n\n\nDPEAFLVQI\nTrue\nHLA-B*35:03\n245.485\n133.986\n394.752\n0.484\n\n\n\n\n\n\nThe first 3 columns come from the test dataset.\n\npeptide: The amino acid sequence of the peptide being evaluated.\nhit: The ground truth, a boolean value indicating whether the peptide is known to be presented by the given MHC allele (True) or not (False).\nallele: The name of the MHC class I allele being considered.\n\nThe following columns are added by the binding affinity predictions:\n\nprediction: The raw prediction score from the MHCflurry model. Higher values generally indicate a stronger predicted binding affinity. These values are not directly interpretable in isolation.\nprediction_low/prediction_high: These represent the lower and upper bounds of a 95% confidence interval around the prediction value. They provide an estimate of the uncertainty associated with the prediction.\nprediction_percentile: This is the most useful column for interpreting the results. It represents the percentile rank of the prediction score compared to a background distribution of scores for random peptides. A lower percentile indicates a stronger predicted binding affinity. For example, a percentile of 1.0 means that the predicted score is in the top 1% of all possible scores.\n\nThe uncertainty estimation comes from the ensemble of neural networks used for the prediction. A percentile threshold (e.g., 2%) is commonly used to determine whether a peptide is likely to bind (lower is better).\n\n\nPrediction: Fitting a Class1BinaryNeuralNetwork\nWe now fit a Class1BinaryNeuralNetwork on the training dataset. The code is available at https://github.com/nbrosse/mhcpred.\nHere’s a glimpse of the training data structure:\n\n\n\nTable 2: Training data.\n\n\n\n\n\npeptide\nallele\nhit\n\n\n\n\nYFPLAPFNQL\nHLA-C*14:02\nTrue\n\n\nKESKINQVF\nHLA-B*44:02\nTrue\n\n\nQPHDPLVPLSA\nHLA-B*54:01\nTrue\n\n\nRTIADSLINSF\nHLA-B*57:03\nTrue\n\n\n\n\n\n\nChallenges arise in encoding the peptide and allele sequences for use in a neural network. Peptides have variable lengths, and alleles are represented by their names.\nThe MHCFlurry package provides a mapping between alleles and their corresponding MHC molecule sequences within the allele_sequences.csv file. This mapping is crucial for encoding the alleles.\n\n\n\nTable 3: Allele sequences.\n\n\n\n\n\nAllele\nSequence\n\n\n\n\nHLA-A*01:01\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:02\nYSAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYA\n\n\nHLA-A*01:03\nYFAMYQENMAHTDANTLYGIMYDRDYTWVARVYRGYA\n\n\nHLA-A*01:04\nYFAMYQENMAHTDANTLYGIIYDRDYTWVARVYRGYX\n\n\nHLA-A*01:06\nYFAMYQENMAHTDANTLYGIIYDRDYTWVALAYRGYA\n\n\n\n\n\n\nFirst, we import necessary libraries. We also import components from our own mhcpred library, which contains the neural network architecture and data loading functions.\nimport pickle\nfrom pathlib import Path\nfrom typing import Iterator\n\nimport numpy as np\nimport pandas as pd\nfrom mhcflurry.allele_encoding import AlleleEncoding\nfrom mhcflurry.encodable_sequences import EncodableSequences\nfrom sklearn.model_selection import train_test_split\n\nfrom mhcpred.class1_binary_nn import Class1BinaryNeuralNetwork\nfrom mhcpred.config import settings\nfrom mhcpred.data import get_test_data, get_train_data\nfrom mhcpred.hyperparameters import base_hyperparameters\nWe load the allele sequences, training data, and test data using helper functions. The allele sequences are crucial for encoding the MHC alleles.\nallele_sequences = pd.read_csv(\n    str(data_path / \"allele_sequences.csv\"), index_col=0\n).iloc[:, 0]\n\ndf_total_train = get_train_data()\ndf_test = get_test_data()\nWe determine the alleles present in our data and filter the loaded allele sequences to only include those we’ll be using.\nalleles_in_use = set(df_total_train.allele).union(set(df_test.allele))\nallele_sequences_in_use = allele_sequences[allele_sequences.index.isin(alleles_in_use)]\nThe AlleleEncoding class is designed to cache encodings for a sequence of alleles. It maps allele names to integer indices and sequences, allowing consistent use of these mappings, especially as inputs to neural networks. The EncodableSequences class is used to encode variable-length peptides into fixed-size numerical matrices. It caches various encodings of a list of sequences and provides methods to encode these sequences into fixed-length categorical or vector representations.\nWe also split the training data into training and validation sets using train_test_split from sklearn. Stratified splitting ensures the class balance is maintained across the training and validation sets. The validation data is also preprocessed by encoding the peptides and alleles.\nallele_encoding = AlleleEncoding(\n    alleles=allele_sequences_in_use.index.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\ndf_train, df_val = train_test_split(\n    df_total_train, test_size=0.1, shuffle=True, stratify=df_total_train.hit.values\n)\n\nval_peptides = EncodableSequences(df_val.peptide.values)\nval_alleles = AlleleEncoding(\n    alleles=df_val.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\nAlleleEncoding provides a robust and efficient way to manage and encode allele sequences. It handles the complexities of mapping allele names to indices, storing and padding sequences, and providing different encoding options. The AlleleEncoding class manages allele sequences efficiently:\n\nAllele Universe vs. Used Alleles: The class distinguishes between two sets of alleles:\n\n\nAllele Universe: The complete set of alleles the system knows about. This is defined by the allele_to_sequence dictionary, mapping allele names to their amino acid sequences.\nUsed Alleles: The specific set of alleles used in a particular analysis or task. This is provided as a list when creating an AlleleEncoding instance.\n\n\nallele_to_index Mapping: A dictionary (self.allele_to_index) is created to map each allele in the universe to a unique integer index. This includes a special index for None values, often used as padding. This mapping ensures consistency: the same allele always gets the same index.\nSequence Storage (self.sequences): The amino acid sequences for all alleles in the universe are stored in a Pandas Series (self.sequences). Critically, these sequences are padded to the same length using “X” characters. This padding is essential for creating fixed-length numerical representations, which many machine learning models require.\nBorrowing (borrow_from): The borrow_from parameter allows you to create a new AlleleEncoding instance that inherits the allele universe and mappings from an existing instance. This is a powerful way to ensure consistency across different parts of your code. You don’t have to redefine the allele_to_sequence mapping every time.\nEncoding: The class provides methods to encode the allele sequences into numerical matrices, suitable for machine learning.\n\n\nallele_representations(encoding_name): Encodes the entire allele universe. This is useful for pre-calculating encodings for all known alleles.\nfixed_length_vector_encoded_sequences(encoding_name): Encodes the used alleles (the subset provided when the object was initialized). This uses the pre-calculated encodings from allele_representations and selects only the encodings for the alleles in self.alleles, in the correct order. This gives you a matrix where each row represents an allele sequence.\n\n\nEncoding Methods (encoding_name): The type of encoding can be specified using the encoding_name parameter. Common options include “BLOSUM62” (a substitution matrix) and “one-hot” encoding.\n\nBLOSUM62 (Blocks Substitution Matrix) is a widely used substitution matrix in bioinformatics. It represents the likelihood of one amino acid being substituted for another during evolution. The matrix assigns a score to each pair of amino acids, reflecting their similarity. Higher scores indicate a higher probability of substitution (or that the two amino acids are more similar). Negative scores indicate substitutions that are less likely or even unfavorable.\nThe AlleleEncoding class uses BLOSUM62 to convert amino acid sequences into numerical representations. Each amino acid in the sequence is replaced by a vector of 21 numbers (20 amino acids + the “X” character). Each of these 21 numbers is the BLOSUM62 score between the amino acid in the sequence and the amino acid represented by the position in the 21-element vector.\n\nAmino Acid Indexing: First, each amino acid is converted to an index. There’s a mapping from amino acid letter to index.\nBLOSUM62 Lookup: For each amino acid in the sequence, the code looks up its corresponding row in the BLOSUM62 matrix. This row represents the similarity scores between that amino acid and all other amino acids (and ‘X’).\nVector Representation: The row from the BLOSUM62 matrix becomes the vector representation of that amino acid. So, “M” would be represented by a vector of 21 numbers (the scores of M with every other amino acid and X), and “A” would also be represented by its own 21-number vector.\nSequence Encoding: The encoded sequence becomes a matrix. If the original sequence was of length n, the encoded sequence is now an n x 21 matrix.\n\nThe train_data_iterator function is a generator that yields batches of training data. This function also handles filtering of alleles that might be present in the training data but not in the allele_sequences data to handle potential data inconsistencies.\ndef train_data_iterator(\n    df_train: pd.DataFrame,\n    train_allele_encoding: AlleleEncoding,\n    batch_size: int = 1024,\n) -&gt; Iterator[tuple[AlleleEncoding, EncodableSequences, np.ndarray]]:\n    \"\"\"\n    This function creates a data generator for training the neural network.\n    It iterates over the training data in batches and yields tuples of \n    (allele_encoding, peptide_sequences, labels).  It also handles filtering\n    of alleles not found in the initial allele encoding.\n    \"\"\"\n    # Get unique alleles in the training set.\n    alleles = df_train.allele.unique()\n    # Filter alleles to keep only those for which sequences are available.\n    usable_alleles = [\n        c for c in alleles if c in train_allele_encoding.allele_to_sequence\n    ]\n    print(\"Using %d / %d alleles\" % (len(usable_alleles), len(alleles)))\n    print(\n        \"Skipped alleles: \",\n        [c for c in alleles if c not in train_allele_encoding.allele_to_sequence],\n    )\n    df_train = df_train.query(\"allele in @usable_alleles\")\n\n    # Calculate the number of batches.\n    n_splits = np.ceil(len(df_train) / batch_size)\n\n    # Infinite loop to allow for multiple epochs.\n    while True:\n        # Split the training data into batches.\n        epoch_dfs = np.array_split(df_train.copy(), n_splits)\n        for k, df in enumerate(epoch_dfs):\n            if len(df) == 0:\n                continue\n            # Encode peptides and alleles for the current batch.\n            encodable_peptides = EncodableSequences(df.peptide.values)\n            allele_encoding = AlleleEncoding(\n                alleles=df.allele.values,\n                borrow_from=train_allele_encoding,  # Reuse encoding from main allele_encoding\n            )\n            # Yield the encoded data and labels (hit column).\n            yield (allele_encoding, encodable_peptides, df.hit.values)\nThe neural network model is initialized using the Class1BinaryNeuralNetwork class. Base hyperparameters are used for initialization. The model is then trained using the fit_generator method. This method takes the training data generator, validation data, and other parameters like the number of epochs and steps per epoch. The steps_per_epoch is calculated based on the training data size and batch size.\nmodel = Class1BinaryNeuralNetwork(**base_hyperparameters)\nsteps_per_epoch = np.ceil(len(df_train) / batch_size)\nbatch_size = 1024  # Define batch_size here\n\ntrain_generator = train_data_iterator(df_train, allele_encoding, batch_size) #create the generator\n\nmodel.fit_generator(\n    generator=train_generator,\n    validation_peptide_encoding=val_peptides,\n    validation_affinities=df_val.hit.values,\n    validation_allele_encoding=val_alleles,\n    validation_inequalities=None,\n    validation_output_indices=None,\n    steps_per_epoch=steps_per_epoch,\n    epochs=2,\n)\nThe Class1BinaryNeuralNetwork neural network takes two inputs:\n\nAllele: A single input representing the MHC allele.\nPeptide: A sequence of 45 amino acids represented as a 21-dimensional vector for each amino acid (likely using BLOSUM62 encoding or a similar technique).\n\nThe network then processes these inputs through several layers:\n\nEmbedding Layer: The allele input is passed through an embedding layer. This layer learns a 777-dimensional vector representation for each allele, capturing its key characteristics relevant to peptide binding.\nFlatten Layers: These layers reshape the input data. The peptide input, which is initially a 45x21 matrix, is flattened into a 945-element vector. Similarly, the 1x777 allele embedding is flattened into a 777-element vector. This prepares the data for the subsequent dense layers.\nConcatenate Layer: The flattened representations of the peptide and allele are combined into a single 1722-element vector. This crucial step merges the information from both inputs, allowing the network to learn the combined effect of allele and peptide on binding affinity.\nDense Layers: These are fully connected layers. The first dense layer transforms the 1722-element vector into a 1024-element vector, and the second further reduces it to 512 elements. These layers learn complex non-linear relationships between the combined allele and peptide representation, extracting features crucial for predicting binding affinity.\nDropout Layers: Dropout is a regularization technique. During training, these layers randomly “drop out” (ignore) a fraction of neurons. This prevents the network from overfitting to the training data and improves its ability to generalize to unseen data.\nOutput Layer: The final dense layer has a single output neuron. This neuron outputs a single value, representing the predicted binding affinity between the given peptide and MHC allele. Since we’re predicting a binary “hit” variable, a sigmoid activation function is used in this layer to output a probability between 0 and 1.\n\nThis architecture is designed to effectively learn the complex patterns governing MHC-peptide binding.\nFinally, the trained model is used to make predictions on the test data. The test data is preprocessed in the same way as the training data, and the predict method of the model is used to generate predictions. These predictions are then added to the test dataframe and saved to a CSV file.\ntest_peptides = df_test.peptide.values\ntest_allele_encoding = AlleleEncoding(\n    alleles=df_test.allele.values,\n    allele_to_sequence=allele_sequences_in_use.to_dict(),\n)\n\npredictions = model.predict(\n    peptides=test_peptides,\n    allele_encoding=test_allele_encoding,\n)\n\ndf_test[\"predictions\"] = predictions\ndf_test.to_csv(str(output_path / \"mhcpred_predictions.csv\"), index=False)\nWe evaluate the predictions of the two methods (mhcflurry and mhcpred) using standard binary classification metrics.\n\n\nMetrics\nThis notebook contains training metrics history and classification metrics computed on the predictions by - mhcflurry (benchmark) - mhcpred\n\nfrom pathlib import Path\nimport pickle\n\nfrom mhcpred.config import settings\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\nmodels_path = Path(settings.models_path)\noutput_path = Path(settings.output_path)\n\nInformation on the training history\nI prefer to use tensorboard, but it is not implemented in the mhcflurry package. The information is quite scarce, but when you execute the code, you have the loss for each step and not only for the whole epoch. Of course, it is a very basic version of logging and should be improved.\n\nwith open(str(models_path / \"model.pickle\"), \"rb\") as f:\n    model = pickle.load(f)\n\n\nmodel.fit_info\n\n[{'learning_rate': 0.0010000000474974513,\n  'loss': [0.09700655937194824, 0.06465369462966919],\n  'val_loss': [0.06880103051662445, 0.05075661838054657],\n  'time': 524.7155420780182,\n  'num_points': 6628048}]\n\n\nBinary classification metrics\nWe compute the usual binary classification metrics on the unbalanced test dataset: accuracy, balanced accuracy, confusion matrix and classification report by scikit-learn.\nWe report the unbalanced accuracy because the dataset is very unbalanced so the accuracy only is not a good measure of accuracy (the model can predict always False and it works quite well).\nmhcflurry metrics\n\nmhcflurry_rank_percentile_threshold = 2  # rank threshold for positive hits\n# It comes from the mhcflurry article.\n\n\ndf = pd.read_csv(str(output_path / \"mhcflurry_predictions.csv\"))\ny_pred = df.prediction_percentile.values &lt;= mhcflurry_rank_percentile_threshold\ny_true = df.hit.values\nacc = accuracy_score(y_true=y_true, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=y_true, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=y_true, y_pred=y_pred)\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.99    900996\n        True       0.67      0.86      0.76     45423\n\n    accuracy                           0.97    946419\n   macro avg       0.83      0.92      0.87    946419\nweighted avg       0.98      0.97      0.97    946419\n\n\n\n\nacc, balanced_acc\n\n(0.9731936911663861, 0.9217833819652606)\n\n\nThe metrics are quite good. We note that we do not have a good precision on the True class (0.67), the model has a tendency to predict True too often, so we have too many False Positives. We see it on the confusion matrix, 19234 False Positives.\nmhcpred metrics\n\nmhcpred_proba_threshold = 0.5  # by default, but we try to tune it later\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9731657332258088, 0.7775326900487307)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.98      0.99      0.99    900725\n        True       0.82      0.56      0.67     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.90      0.78      0.83    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nmhcpred has worse performances compared to mhcflurry, see the balanced accuracy. On the True class, in that case, the recall is not good (0.56), the model has a tendency to predict False too often, on the confusion matrix we have 20000 True Negatives. It indicates that if we lower the threshold, we may improve the model.\nThreshold tuning\nWe plot the precision recall curve to try to identify a better threshold.\n\nfrom sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n\nprecision, recall, thresholds = precision_recall_curve(y_true=y_true, probas_pred=df.predictions.values)\ndisp = PrecisionRecallDisplay(precision=precision, recall=recall)\ndisp.plot()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprecision_recall_thresholds = pd.DataFrame({\n    \"precision\": precision[:-1],\n    \"recall\": recall[:-1],\n    \"thresholds\": thresholds,\n})\n\n\nprecision_recall_thresholds\n\n\n\n\n\n\n\n\nprecision\nrecall\nthresholds\n\n\n\n\n0\n0.048001\n1.000000\n0.000114\n\n\n1\n0.048001\n1.000000\n0.000116\n\n\n2\n0.048001\n1.000000\n0.000117\n\n\n3\n0.048001\n1.000000\n0.000125\n\n\n4\n0.048002\n1.000000\n0.000125\n\n\n...\n...\n...\n...\n\n\n889313\n1.000000\n0.000110\n0.992152\n\n\n889314\n1.000000\n0.000088\n0.992280\n\n\n889315\n1.000000\n0.000066\n0.992347\n\n\n889316\n1.000000\n0.000044\n0.992431\n\n\n889317\n1.000000\n0.000022\n0.992971\n\n\n\n\n889318 rows × 3 columns\n\n\n\nA threshold of approx. 0.2 seems to be a good compromise for precision/recall.\n\nmhcpred_proba_threshold = 0.2\n\n\ndf = pd.read_csv(str(output_path / \"mhcpred_predictions.csv\"))\ny_true = df.hit.values\ny_pred = df.predictions.values &gt;= mhcpred_proba_threshold\nacc = accuracy_score(y_true=df.hit.values, y_pred=y_pred)\nconfusion_mat = confusion_matrix(y_true=df.hit.values, y_pred=y_pred)\nbalanced_acc = balanced_accuracy_score(y_true=df.hit.values, y_pred=y_pred)\n\nclass_report = classification_report(y_true=y_true, y_pred=y_pred, output_dict=False)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_mat)\ndisp.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\nacc, balanced_acc\n\n(0.9697360118629252, 0.8462451280426622)\n\n\n\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n       False       0.99      0.98      0.98    900725\n        True       0.68      0.71      0.69     45416\n\n    accuracy                           0.97    946141\n   macro avg       0.83      0.85      0.84    946141\nweighted avg       0.97      0.97      0.97    946141\n\n\n\nWe see that we have improved the balanced accuracy. We have a deterioration of the precision but a better recall.\nSource: Metrics"
  },
  {
    "objectID": "posts/encoding-distances/unimol-gbf.html",
    "href": "posts/encoding-distances/unimol-gbf.html",
    "title": "Encoding Distances in Molecules and Pockets: A Comparison of GBFPT and DCEPT",
    "section": "",
    "text": "This blog post compares two different methods for encoding distances in 3D molecules and protein pockets: Gaussian kernel with pair type (GBFPT) and Discretization categorical embedding with Pair Type (DCEPT). We analyze their performance within the Uni-Mol framework, a universal 3D molecular representation learning model. We observed unstable gradients with GBFPT and hypothesized that DCEPT, inspired by AlphaFold’s distance representation, might offer a more stable alternative. We found that while DCEPT exhibits more stable training behavior, GBFPT ultimately yields superior pocket embeddings for retrieval tasks.\nBefore diving into the details, let’s define the acronyms used in this article:\n\nGBFPT: Gaussian Basis Function with Pair Type\nDCEPT: Discretization Categorical Embedding with Pair Type\n\n\nIntroduction to Uni-Mol and Distance Encoding\nCode for Uni-Mol is available at https://github.com/dptech-corp/Uni-Mol, and the related article is (Zhou et al. 2023). In brief, Uni-Mol is a 3D foundation model for molecules and pockets based on a SE(3) Transformer architecture. It comprises two pretrained models: one for molecular conformations and another for protein pocket data. Uni-Mol is pretrained on large-scale unlabeled data and is able to directly take 3D positions as both inputs and outputs. Uni-Mol backbone is a Transformer based model that can capture the input 3D information and predict 3D positions directly. Uni-Mol pretraining is done on two large-scale datasets: a 209M molecular conformation dataset and a 3M candidate protein pocket dataset, for pretraining 2 models on molecules and protein pockets, respectively. In the pretraining phase, Uni-Mol has to predict masked atoms, as well as masked noisy atoms coordinates and distances for effectively learning 3D spatial representation. The overall pretraining architecture is illustrated in Figure 2 and the framework is given in Figure 1 (taken from (Zhou et al. 2023)).\n\n\n\n\n\n\nFigure 1: Schematic illustration of the Uni-Mol framework\n\n\n\n\n\n\n\n\n\nFigure 2: Left: the overall pretraining architecture. Middle: the model inputs, including atom representation and pair representation. Right: details in the model block.\n\n\n\nBackground: 3D Spatial Encoding in Uni-Mol\nWe focus here on the encoding of the coordinates in distances (pair representation in Figure 2 middle part) and the decoding part, prediction of distances (pair-dist head in Figure 2 left part). In (Zhou et al. 2023) Section D.1, 3D spatial positional encodings benchmark, they investigate the performance of different 3D spatial positional encoding on the 3D molecular pretraining. In particular, they benchmarked:\n\nGaussian kernel (GK), a simply Gaussian density function.\nGaussian kernel with pair type (GKPT) (Shuaibi et al. 2021). Based on GK, an affine transformation according to the pair type is applied on pair distances, before applying the Gaussian kernel.\nRadial Bessel basis (RBB) (Gasteiger, Yeshwanth, and Günnemann 2021). A Bessel based radial function.\nDiscretization categorical embedding (DCE). They convert the continued distances to the discrete bins, by Discretization. With binned distances, embedding-based positional encoding is directly used.\nDelta coordinate (DC) (Zhao et al. 2021). Following Point Transformer, the deltas of coordinates are directly used as pair-wise spatial relative positional encoding.\nGaussian kernel with pair type and local graph (GKPTLG). Based on GKPT, they set up a model with locally connected graphs. In particular, the cutoff radius is set to 6 Å.\n\nThe validation loss during pretraining for each encoding is summarized in Figure 3 (taken from (Zhou et al. 2023)). From the results, they drew the following conclusions:\n\nThe performance of DCE and GK are almost the same, and outperform RBB and DC. And they choose GK as the basic encoding.\nCompared with GK, GKPT converges faster. This indicates the pair type is critical in the 3D spatial positional encoding.\nCompared with GKPT, GKPTLG converges slower. This indicates the locally cutoff graph is not effective for self-supervised learning, and the default fully connected graph structure inherent in the Transformer architecture is more effective.\nAs GKPT outperforms all other encoding, they use it in the backbone model of Uni-Mol.\n\n\n\n\n\n\n\nFigure 3: Validation loss in pretraining for different 3D spatial encodings\n\n\n\nThe code for the GKPT encoding is given by:\nimport torch\nimport torch.nn as nn\n\n@torch.jit.script\ndef gaussian(x, mean, std):\n    pi = 3.14159\n    a = (2 * pi) ** 0.5\n    return torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std)\n\n\nclass GaussianLayer(nn.Module):\n    def __init__(self, K=128, edge_types=1024):\n        super().__init__()\n        self.K = K\n        self.means = nn.Embedding(1, K)\n        self.stds = nn.Embedding(1, K)\n        self.mul = nn.Embedding(edge_types, 1)\n        self.bias = nn.Embedding(edge_types, 1)\n        nn.init.uniform_(self.means.weight, 0, 3)\n        nn.init.uniform_(self.stds.weight, 0, 3)\n        nn.init.constant_(self.bias.weight, 0)\n        nn.init.constant_(self.mul.weight, 1)\n\n    def forward(self, x, edge_type):\n        mul = self.mul(edge_type).type_as(x)\n        bias = self.bias(edge_type).type_as(x)\n        x = mul * x.unsqueeze(-1) + bias\n        x = x.expand(-1, -1, -1, self.K)\n        mean = self.means.weight.float().view(-1)\n        std = self.stds.weight.float().view(-1).abs() + 1e-5\n        return gaussian(x.float(), mean, std).type_as(self.means.weight)\nK represents the number of Gaussian basis functions, edge_types the number of possible edge types, x the distance matrix (for an initial 3D molecule or pocket) and edge_type the corresponding edge type matrix. Edge types represent the different types of atom pairs (e.g., C-C, C-O, C-N, etc.)\n\n\nExperimental Setup\n\n\n\n\n\n\nNote\n\n\n\nAll the Uni-Mol experiments run for this article are based on a small pockets dataset inspired from the PDBbind database (http://www.pdbbind.org.cn/), a collection of protein-ligand complexes and their binding affinities. The dataset was split into training and validation sets based on pocket similarity. The wandb project is available https://wandb.ai/nicolasb/unimol_analysis/ as well as a summary report https://api.wandb.ai/links/nicolasb/kdz59bry.\n\n\nMotivation for DCEPT and Implementation\nWhen we train Uni-Mol on a small dataset of pockets inspired from the PDBbind database, we remark that the gradients related to GaussianLayer parameters are not stable and can take very large values. To address the gradient instability observed with GBFPT, and drawing inspiration from AlphaFold’s use of discretization, we explored using a Discretization Categorical Embedding with Pair Type (DCEPT) as an alternative. In Figure 4, some gradients are of the order of one thousand. Uni-Mol relies on Uni-Core which implements gradient clipping and these high values do not affect the stability of the training.\n\n\n\n\n\n\nFigure 4: Gradients of GaussianLayer parameters (GBFPT encoding)\n\n\n\nNevertheless, we wanted to try another encoding that would be naturally stable without exploding gradients. Furthermore, DCE is the encoding used in AlphaFold (Jumper et al. 2021). We implemented Discretization categorical embedding with Pair Type encoding (DCEPT) that takes into account the edge type. A distogram is a discrete representation of the distance matrix, where distances are binned into predefined intervals. The binning process transforms continuous distances into discrete categories.\nimport torch\nimport torch.nn as nn\n\n# Constants\nPAD_DIST = 0\n\nclass NonLinearModule(nn.Module):\n    def __init__(self, input_dim, out_dim, activation_fn):\n        super().__init__()\n        self.linear = nn.Linear(input_dim, out_dim)\n        self.activation = getattr(nn, activation_fn)()\n\n    def forward(self, x):\n        return self.activation(self.linear(x))\n\nclass DistEncoding(nn.Module):\n    def __init__(\n        self,\n        distogram_nb_bins: int,\n        nb_edge_types: int,\n        embedding_dim: int,\n        edge_type_padding_idx: int,\n        encoder_attention_heads: int,\n        activation_fn: str,\n    ):\n        \"\"\"\n        Initializes the DistEncoding module for encoding distances and edge types.\n\n        Args:\n            distogram_nb_bins: Number of bins for the distogram (distance discretization).\n            nb_edge_types: Number of possible edge types (e.g., different bond types).\n            embedding_dim: Dimension of the embeddings for distances and edge types.\n            edge_type_padding_idx: Padding index for edge type embeddings.\n            encoder_attention_heads: Number of attention heads in the Transformer encoder.\n            activation_fn: Activation function to use in the projection layer.\n        \"\"\"\n        super(DistEncoding, self).__init__()\n\n        # Embedding layer for the distogram (discretized distances)\n        self.dist_embedding = nn.Embedding(\n            num_embeddings=distogram_nb_bins,\n            embedding_dim=embedding_dim,\n            padding_idx=PAD_DIST,  # Use PAD_DIST for padding\n        )\n\n        # Embedding layer for edge types\n        self.edge_type_embedding = nn.Embedding(\n            num_embeddings=nb_edge_types,\n            embedding_dim=embedding_dim,\n            padding_idx=edge_type_padding_idx,\n        )\n\n        # Projection layer to combine distance and edge type embeddings and project\n        # to the correct dimension for attention bias.\n        self.projection = NonLinearModule(\n            input_dim=2 * embedding_dim,  # Concatenate dist and edge embeddings\n            out_dim=encoder_attention_heads,  # Output dimension matches attention heads\n            activation_fn=activation_fn,\n        )\n\n    def forward(\n        self, distogram: torch.Tensor, edge_types: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass of the DistEncoding module.\n\n        Args:\n            distogram: Tensor of discretized distances (batch_size, seq_len, seq_len).\n            edge_types: Tensor of edge types (batch_size, seq_len, seq_len).\n\n        Returns:\n            attn_bias: Tensor of attention biases (batch_size, num_heads, seq_len, seq_len).\n        \"\"\"\n        n_node = distogram.size(-1)  # Sequence length (number of nodes/atoms)\n\n        # Embed the discretized distances\n        dist_embeddings = self.dist_embedding(distogram)  # (B, L, L, D)\n\n        # Embed the edge types\n        edge_types_embeddings = self.edge_type_embedding(edge_types)  # (B, L, L, D)\n\n        # Concatenate distance and edge type embeddings\n        embeddings = torch.cat((dist_embeddings, edge_types_embeddings), dim=-1)  # (B, L, L, 2D)\n\n        # Project the combined embeddings to generate attention bias\n        attn_bias = self.projection(embeddings)  # (B, L, L, H) where H = num_heads\n\n        # Reshape the attention bias to the correct format for the Transformer\n        attn_bias = attn_bias.permute(0, 3, 1, 2).contiguous()  # (B, H, L, L)\n        attn_bias = attn_bias.view(-1, n_node, n_node) # (B*H, L, L) - Correct if attention is applied per head.\n        return attn_bias\ndistogram_nb_bins is the number of bins (128 by default), nb_edge_types the number of edge types, embedding_dim the dimension of the embedding (128 by default), encoder_attention_heads the number of attention heads in the transformer because the distance encoding is directly injected in the attention matrix. distogram is the distogram (discretization of the distance matrix) and edge_types the edge types. We concatenate the two embeddings creating de facto the DCEPT and then project to feed into the attention matrix.\nTraining Dynamics\nDuring the training, the gradients related to DistEncoding parameters do not take large values and are naturally stable without clipping gradients. This is illustrated in Figure 5.\n\n\n\n\n\n\nFigure 5: Gradients of DistEncoding parameters (DCEPT encoding)\n\n\n\nFollowing (Jumper et al. 2021) distogram prediction task, we also replace the distance prediction task (mean squared error loss) implemented in Uni-Mol by a distogram prediction task (cross entropy loss). We remark that this loss replacement does not change the characteristics of Uni-Mol.\nOn the small pockets inspired from the PDBbind database, we notice that the training and validation loss curves are better with DCEPT encoding compared to GBFPT encoding (on average), see Figure 6. More precisely, the masked_token_loss and the masked_acc metrics related to the recovery of masked atoms seem to stagnate a little at first with DCEPT encoding compared to GBFPT encoding. It may be due to the fact that DCEPT are at first completely random embeddings and less intuitive for the neural network. However, the masked_coord_loss is better with DCEPT encoding both in the training and validation sets. Note that the masked_distogram_loss corresponds to the distogram loss (cross entropy loss) used in AlphaFold (Jumper et al. 2021) and is implemented only for DCEPT encoding. For DCEPT encoding, we also add a distance prediction head with the corresponding MSE loss taken from Uni-Mol and a small multiplication factor (0.01). The rationale for including this head was to maintain some of the original Uni-Mol distance prediction capabilities alongside the distogram prediction. That explains why the DCEPT masked_dist_loss decreases slightly slower than GBFPT. Several additional experiments (not shown here) demonstrate that using a distogram or distance loss does not change the behavior of Uni-Mol.\nIn conclusion, according to the loss curves and the stability of the gradients, DCEPT seems to be a better encoding than GBFPT (or at least as good as) during pre-training.\n\n\n\n\n\n\nFigure 6: Uni-Mol training and validation losses with GBFPT and DCEPT encoding\n\n\n\n\n\nDownstream Performance: Pocket Retrieval\nHowever, Uni-Mol stands as a foundational model pre-trained through unsupervised methods. The pre-training metrics do not reflect the expected capabilities of the model. Notably, we expect that the pockets embeddings obtained with Uni-Mol should be good proxies of the pockets themselves: if two pockets are close to each other, their embeddings should be close in cos similarity or euclidean distance.\nWe have collected a dataset of 5 pockets (taken from 2oax, 3oxc, 5kxi, 5zk3 and 6v7a proteins) and for each pocket, a group of similar and dissimilar pockets. We compute the cos similarities between each reference pocket and the similar/dissimilar pockets and we sort the pockets by their cos similarity. Better embeddings translate into more similar pockets in the top retrieved pockets. More precisely, we sort the pockets by their cos similarities, we select the top 100 pockets, we count the number of similar pockets in the top 100 and we get a number between 0 and 1, the higher the better. We test two different embeddings: either, the vector corresponding to the [CLS] token (see (Zhou et al. 2023) Section 2.2) (indicated by _cls) or the mean of the pocket atoms vectors (indicated by _mean). Table 1 summarizes the results for each encoding, embedding and reference pocket and we remark that\n\nGBFPT is superior to DCEPT for pockets retrieval,\nThe mean of the pocket atoms vectors is better or near as good as the [CLS] embedding.\n\n\n\n\nTable 1: Uni-Mol pockets retrieval with GBFPT or DCEPT encoding (higher is better)\n\n\n\n\n\n\n6v7a\n2oax\n5kxi\n5zk3\n3oxc\n\n\n\n\nunimol_gbfpt_cls\n0.46\n1.0\n0.32\n0.3\n1.0\n\n\nunimol_gbfpt_mean\n0.79\n1.0\n0.38\n0.29\n1.0\n\n\nunimol_dcept_cls\n0.3\n1.0\n0.26\n0.29\n1.0\n\n\nunimol_dcept_mean\n0.51\n1.0\n0.27\n0.28\n1.0\n\n\n\n\n\n\nInvestigating Noise Sensitivity\nIn conclusion, despite better pre-training behavior and metrics, DCEPT encoding is disappointing when it comes to embeddings comparison. We suppose that this defect comes from a higher sensitivity of the discretization procedure. Two distance matrices from two close pockets may be more distinctly differentiated with DCEPT encoding compared to GBFPT encoding. This could be due to the information loss inherent in discretizing continuous distances. GBFPT might be capturing subtle long-range interactions that DCEPT, with its discrete representation, misses. To test this hypothesis, we take the 6v7a pocket and we noise its coordinates with a uniform noise between 0 and 1A. Since we have a batch size of 16, we fill up a batch with the reference pocket 6v7a and 15 noisy pockets. For each pocket, the distance matrix is encoded by GBFPT or DCEPT and we get an encoding of size 128 for each distance in the distance matrix. We compute the cos similarities between each encoding and the reference encoding in the reference matrix distance of 6v7a and we obtain the overall statistics of these cos similarities for GBFPT and DCEPT. In Table 2, we have the absolute errors statistics between 1 and the cos similarities of the noisy pockets from 6v7a, the lower the better because the vectors are similar. We remark that as presumed DCEPT encoding is less robust to noise compared to GBFPT encoding.\n\n\n\nTable 2: Statistics of cos similarities errors for noisy pockets from 6v7a\n\n\n\n\n\n\n\n\n\n\n\nAbsolute errors cos similarities GBFPT encoding (1 - cos similarities)\nAbsolute errors cos similarities DCEPT encoding (1 - cos similarities)\n\n\n\n\nmean\n0.002\n0.020\n\n\nmedian\n0.000\n0.007\n\n\n\n\n\n\n\n\nConclusion\nThese results suggest that while DCEPT offers advantages during pre-training in terms of gradient stability, the discretization process might lead to a loss of information and reduced robustness, ultimately hindering the quality of the generated pocket embeddings for downstream tasks like pocket retrieval. Further research is needed to investigate the optimal bin size for distograms and to explore alternative distance encoding techniques that balance stability and information retention. This analysis was conducted on a small, custom-built pocket dataset, and future work should evaluate these encoding methods on larger, more diverse datasets to ensure the generalizability of our findings.\n\n\n\n\n\nReferences\n\nGasteiger, Johannes, Chandan Yeshwanth, and Stephan Günnemann. 2021. “Directional Message Passing on Molecular Graphs via Synthetic Coordinates.” In Advances in Neural Information Processing Systems, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:15421–33. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2021/file/82489c9737cc245530c7a6ebef3753ec-Paper.pdf.\n\n\nJumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” Nature 596 (7873): 583–89. https://doi.org/10.1038/s41586-021-03819-2.\n\n\nShuaibi, Muhammed, Adeesh Kolluru, Abhishek Das, Aditya Grover, Anuroop Sriram, Zachary Ulissi, and C. Lawrence Zitnick. 2021. “Rotation Invariant Graph Neural Networks Using Spin Convolutions.” https://arxiv.org/abs/2106.09575.\n\n\nZhao, Hengshuang, Li Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun. 2021. “Point Transformer.” In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 16239–48. https://doi.org/10.1109/ICCV48922.2021.01595.\n\n\nZhou, Gengmo, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. 2023. “Uni-Mol: A Universal 3D Molecular Representation Learning Framework.” In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=6K2RM6wVqKu."
  }
]